{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/adibayaseen/PPI-Inhibitors/blob/main/Comprehensive_GNN_PPI_Inhibitor_Prediction_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Comprehensive GNN-Based Pipeline for Predicting Small-Molecule Inhibition of Protein Complexes\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **complete, step-by-step implementation** of the Graph Neural Network (GNN) based model for predicting whether a small molecule can act as an inhibitor of a specific protein complex.\n",
    "\n",
    "### Research Paper Summary\n",
    "\n",
    "**Title:** Predicting small-molecule inhibition of protein complexes\n",
    "\n",
    "**Problem Statement:**\n",
    "- Protein-Protein Interactions (PPIs) are crucial in biological processes and disease mechanisms\n",
    "- Traditional methods for discovering PPI inhibitors are expensive and time-consuming\n",
    "- **No existing method** takes both a protein complex AND a compound as inputs to predict targeted inhibition\n",
    "\n",
    "**Solution:**\n",
    "- First **targeted** machine learning predictor of small molecule inhibition of protein complexes\n",
    "- Integrates:\n",
    "  - 3D structure of protein complex (via Graph Neural Network)\n",
    "  - Protein-protein binding interface features\n",
    "  - Compound SMILES representation (via molecular fingerprints)\n",
    "\n",
    "**Results:**\n",
    "- Cross-validation AUC-ROC: **0.86** (86% accuracy)\n",
    "- External test set 1 (recent publications): AUC-ROC **0.82**\n",
    "- External test set 2 (SARS-CoV-2): AUC-ROC **0.78**\n",
    "- Outperforms baseline SVM and GearNet embeddings\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1. **Environment Setup**: Install required packages and dependencies\n",
    "2. **Data Loading**: Load datasets (2P2I database, negative examples)\n",
    "3. **Feature Extraction**: Extract features from protein complexes and compounds\n",
    "4. **Data Loaders**: Create balanced data loaders for training\n",
    "5. **Model Architecture**: Implement GNN and MLP models\n",
    "6. **Training**: Leave-one-complex-out cross-validation\n",
    "7. **Validation**: Evaluate model performance\n",
    "8. **External Testing**: Test on independent datasets\n",
    "9. **Baseline Comparison**: Compare with SVM and GearNet\n",
    "10. **Results Visualization**: Generate ROC and PR curves\n",
    "\n",
    "---\n",
    "\n",
    "**IMPORTANT NOTE:**\n",
    "- Set Runtime → Change Runtime Type to **GPU** for optimal performance\n",
    "- This notebook uses only the functions and approaches from the original research\n",
    "- All code is heavily documented with explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Setup and Package Installation\n",
    "\n",
    "### Purpose\n",
    "Install all required Python packages and dependencies for:\n",
    "- **Structural biology**: BioPython (PDB file handling)\n",
    "- **Cheminformatics**: RDKit (SMILES processing, molecular fingerprints)\n",
    "- **Deep Learning**: PyTorch (neural network implementation)\n",
    "- **Data Science**: NumPy, Pandas, Scikit-learn\n",
    "\n",
    "### Key Packages\n",
    "- `biopython`: Parse protein structures from PDB files\n",
    "- `rdkit`: Generate molecular fingerprints from SMILES strings\n",
    "- `torch`: Deep learning framework for GNN implementation\n",
    "- `scikit-learn`: Data preprocessing and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access pre-computed protein features\n",
    "# These features are too large to recompute in this notebook\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Expected structure:\n",
    "# /content/drive/MyDrive/GNN-PPI-Inhibitor/\n",
    "#   ├── ProteinData_dict.pickle          # 2P2I protein GNN features\n",
    "#   └── DBD5_ProteinData_dict.pickle    # DBD5 protein GNN features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the GitHub repository containing data and utility functions\n",
    "# This will be removed and re-cloned if it already exists\n",
    "!rm -rf PPI-Inhibitors\n",
    "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
    "\n",
    "# Repository contains:\n",
    "# - Data/: Datasets (complexes, inhibitors, SMILES)\n",
    "# - Features/: Pre-computed features (interface, sequence, compound fingerprints)\n",
    "# - code/: Original notebook implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install BioPython for protein structure parsing\n",
    "# BioPython provides:\n",
    "# - PDBParser: Parse PDB files into structured objects\n",
    "# - NeighborSearch: Find neighboring atoms in 3D space\n",
    "# - Structure navigation: Access chains, residues, atoms\n",
    "!pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RDKit for cheminformatics operations\n",
    "# RDKit provides:\n",
    "# - SMILES parsing: Convert string representation to molecular objects\n",
    "# - Fingerprint generation: Extended-Connectivity Fingerprints (ECFP)\n",
    "# - Molecular descriptors: Properties and features\n",
    "!pip install rdkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Import Required Libraries\n",
    "\n",
    "### Purpose\n",
    "Import all necessary Python libraries and modules for the complete pipeline.\n",
    "\n",
    "### Library Categories\n",
    "\n",
    "1. **Deep Learning (PyTorch)**\n",
    "   - `torch`: Core PyTorch functionality\n",
    "   - `torch.nn`: Neural network layers and loss functions\n",
    "   - `torch.optim`: Optimizers (Adam)\n",
    "   - `torch.utils.data`: Dataset and DataLoader classes\n",
    "\n",
    "2. **Structural Biology (BioPython)**\n",
    "   - `Bio.PDB.PDBParser`: Parse PDB structure files\n",
    "   - `Bio.PDB.NeighborSearch`: Find neighboring atoms within distance threshold\n",
    "\n",
    "3. **Cheminformatics (RDKit)**\n",
    "   - `rdkit.Chem`: SMILES parsing\n",
    "   - Morgan Fingerprints: Molecular structure encoding\n",
    "\n",
    "4. **Machine Learning (Scikit-learn)**\n",
    "   - Preprocessing: StandardScaler, OneHotEncoder\n",
    "   - Cross-validation: GroupKFold\n",
    "   - Metrics: ROC-AUC, PR-AUC, precision, recall\n",
    "\n",
    "5. **Data Manipulation**\n",
    "   - NumPy: Numerical computations\n",
    "   - Pandas: Data structures and analysis\n",
    "   - Pickle: Serialization of Python objects\n",
    "\n",
    "6. **Visualization**\n",
    "   - Matplotlib: Plotting ROC and PR curves\n",
    "\n",
    "7. **Utilities**\n",
    "   - tqdm: Progress bars\n",
    "   - warnings: Suppress BioPython warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Comprehensive GNN-Based PPI Inhibitor Prediction Pipeline\n",
    "\n",
    "This module implements the complete pipeline for predicting small-molecule\n",
    "inhibition of protein complexes using Graph Neural Networks.\n",
    "\n",
    "Author: Based on research by Yaseen et al. (2024)\n",
    "Paper: Predicting small-molecule inhibition of protein complexes\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# DEEP LEARNING LIBRARIES (PyTorch)\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# ============================================================================\n",
    "# STRUCTURAL BIOLOGY LIBRARIES (BioPython)\n",
    "# ============================================================================\n",
    "from Bio.PDB import *\n",
    "from Bio.PDB import PDBParser, NeighborSearch\n",
    "import warnings  # Suppress PDB parsing warnings\n",
    "\n",
    "# ============================================================================\n",
    "# CHEMINFORMATICS LIBRARIES (RDKit)\n",
    "# ============================================================================\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# ============================================================================\n",
    "# MACHINE LEARNING LIBRARIES (Scikit-learn)\n",
    "# ============================================================================\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, normalize\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    precision_score, recall_score, auc\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# DATA MANIPULATION LIBRARIES\n",
    "# ============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION LIBRARIES\n",
    "# ============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY LIBRARIES\n",
    "# ============================================================================\n",
    "from tqdm import tqdm  # Progress bars\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# CUDA CONFIGURATION\n",
    "# ============================================================================\n",
    "# Check if GPU is available and configure device\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "def cuda(v):\n",
    "    \"\"\"\n",
    "    Move tensor to GPU if CUDA is available.\n",
    "    \n",
    "    Args:\n",
    "        v: Tensor or variable to move to GPU\n",
    "    \n",
    "    Returns:\n",
    "        Tensor on GPU if available, otherwise on CPU\n",
    "    \"\"\"\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n",
    "\n",
    "def toTensor(v, dtype=torch.float, requires_grad=False):\n",
    "    \"\"\"\n",
    "    Convert numpy array or list to PyTorch tensor and move to GPU.\n",
    "    \n",
    "    Args:\n",
    "        v: Input data (numpy array, list, etc.)\n",
    "        dtype: PyTorch data type (default: torch.float)\n",
    "        requires_grad: Whether to track gradients (default: False)\n",
    "    \n",
    "    Returns:\n",
    "        PyTorch tensor on appropriate device\n",
    "    \"\"\"\n",
    "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
    "\n",
    "def toNumpy(v):\n",
    "    \"\"\"\n",
    "    Convert PyTorch tensor to NumPy array.\n",
    "    Handles both CPU and GPU tensors.\n",
    "    \n",
    "    Args:\n",
    "        v: PyTorch tensor\n",
    "    \n",
    "    Returns:\n",
    "        NumPy array\n",
    "    \"\"\"\n",
    "    if USE_CUDA:\n",
    "        return v.detach().cpu().numpy()\n",
    "    return v.detach().numpy()\n",
    "\n",
    "# Print GPU information\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA is available. Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"✗ CUDA is not available. Using CPU.\")\n",
    "    print(\"  Warning: Training will be significantly slower without GPU.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Custom Data Loaders and Samplers\n",
    "\n",
    "### Purpose\n",
    "Implement custom Dataset and Sampler classes to handle **class imbalance**.\n",
    "\n",
    "### The Class Imbalance Problem\n",
    "\n",
    "In PPI inhibitor prediction:\n",
    "- **Positive examples** (inhibitors): ~714 examples\n",
    "- **Negative examples** (non-inhibitors): ~10,413 examples\n",
    "- **Ratio**: ~1:15 (highly imbalanced)\n",
    "\n",
    "**Why this is a problem:**\n",
    "- Model can achieve high accuracy by always predicting \"negative\"\n",
    "- Gradient updates dominated by negative examples\n",
    "- Poor recall for positive (inhibitor) class\n",
    "\n",
    "### Solutions Implemented\n",
    "\n",
    "**1. BalancedDataset with WeightedRandomSampler**\n",
    "- Assigns higher sampling probability to minority class\n",
    "- Uses weighted random sampling with replacement\n",
    "- Creates approximately balanced batches over time\n",
    "\n",
    "**2. BinaryBalancedSampler (Stratified Sampling)**\n",
    "- Oversamples minority class to match majority class size\n",
    "- Creates batches with exactly 50% positive and 50% negative\n",
    "- Uses StratifiedKFold to ensure balance in every batch\n",
    "- Guarantees equal representation in each iteration\n",
    "\n",
    "### When to Use Which\n",
    "- **BalancedDataset**: When you want soft balancing with randomness\n",
    "- **BinaryBalancedSampler**: When you need strict 50/50 splits (used in this work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loaders",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM DATASET AND SAMPLER CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "class BalancedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class that creates a balanced dataset from imbalanced data\n",
    "    using weighted random sampling.\n",
    "    \n",
    "    This dataset calculates sample weights inversely proportional to class frequencies,\n",
    "    which can be used with WeightedRandomSampler to achieve balanced batches.\n",
    "    \n",
    "    Mathematical Formulation:\n",
    "    -------------------------\n",
    "    For class c with n_c examples:\n",
    "        weight_c = 1 / n_c\n",
    "    \n",
    "    For each sample i of class c:\n",
    "        sample_weight_i = weight_c\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    If we have 100 positive and 1000 negative examples:\n",
    "        - Positive weight = 1/100 = 0.01\n",
    "        - Negative weight = 1/1000 = 0.001\n",
    "    \n",
    "    Positive examples are 10x more likely to be sampled.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Input data (can be list, NumPy array, or PyTorch tensor)\n",
    "    labels : array-like\n",
    "        Labels corresponding to the data (1D array)\n",
    "    sample_weights : torch.Tensor\n",
    "        Weights for each sample (inversely proportional to class frequency)\n",
    "    \n",
    "    NOTE: This involves stochastic sampling, so some training examples \n",
    "    may never be selected in a given epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Initialize the balanced dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Input features (examples)\n",
    "            labels: Class labels (0 or 1 for binary classification)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        \n",
    "        # Count the number of examples in each class\n",
    "        # np.bincount([0, 1, 1, 0, 1]) -> [2, 3]\n",
    "        # Index 0 = count of class 0, Index 1 = count of class 1\n",
    "        class_counts = np.bincount(self.labels)\n",
    "        \n",
    "        # Assign weight inversely proportional to class frequency\n",
    "        # Minority class gets higher weight\n",
    "        weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "        \n",
    "        # Create a weight list for each sample based on its class\n",
    "        # If sample i has label j, assign weight_j to sample i\n",
    "        self.sample_weights = weights[labels]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample and its label.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (data, label) at index idx\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "def create_balanced_loader(data, labels, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader with balanced batches for imbalanced datasets.\n",
    "    \n",
    "    This function wraps BalancedDataset and WeightedRandomSampler to create\n",
    "    a DataLoader that yields approximately balanced batches.\n",
    "    \n",
    "    How it works:\n",
    "    -------------\n",
    "    1. Create BalancedDataset (computes sample weights)\n",
    "    2. Create WeightedRandomSampler with these weights\n",
    "    3. Sampler draws samples with probability proportional to weights\n",
    "    4. Minority class samples drawn more frequently\n",
    "    5. Over many batches, classes become approximately balanced\n",
    "    \n",
    "    Args:\n",
    "        data: Input features\n",
    "        labels: Class labels (1D array)\n",
    "        batch_size: Size of each batch (default: 32)\n",
    "    \n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader yielding balanced batches\n",
    "    \n",
    "    Usage Example:\n",
    "    --------------\n",
    "    >>> data = [features1, features2, ...]  \n",
    "    >>> labels = [0, 1, 1, 0, 1, ...]\n",
    "    >>> balanced_loader = create_balanced_loader(data, labels, batch_size=32)\n",
    "    >>> for batch_data, batch_labels in balanced_loader:\n",
    "    >>>     # Train your model with approximately balanced batches\n",
    "    >>>     pass\n",
    "    \"\"\"\n",
    "    # Create the balanced dataset\n",
    "    dataset = BalancedDataset(data, labels)\n",
    "    \n",
    "    # WeightedRandomSampler handles the actual balancing\n",
    "    # - weights: probability of selecting each sample\n",
    "    # - num_samples: how many samples to draw (typically len(dataset))\n",
    "    # - replacement: True allows same sample multiple times (necessary for balancing)\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=dataset.sample_weights,\n",
    "        num_samples=len(dataset.sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create DataLoader with the balanced sampler\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return loader\n",
    "\n",
    "\n",
    "class BinaryBalancedSampler(Sampler):\n",
    "    \"\"\"\n",
    "    A PyTorch Sampler that returns batches with EXACTLY 50% positive and 50% negative.\n",
    "    \n",
    "    This sampler oversamples the minority class to match the majority class size,\n",
    "    ensuring each batch contains equal numbers of positive and negative examples.\n",
    "    \n",
    "    Algorithm:\n",
    "    ----------\n",
    "    1. Identify majority class M and minority class m\n",
    "    2. Count: |M| majority examples, |m| minority examples\n",
    "    3. Oversample minority class: randomly select |M| examples from m (with replacement)\n",
    "    4. Combine: [M examples] + [|M| oversampled m examples] = 2|M| total\n",
    "    5. Use StratifiedKFold to create balanced batches from this balanced pool\n",
    "    6. Each batch will have 50% from M and 50% from oversampled m\n",
    "    \n",
    "    Mathematical Properties:\n",
    "    ------------------------\n",
    "    - Original dataset size: N\n",
    "    - After balancing: 2 * |M| examples per iteration\n",
    "    - Equivalent epochs = (2 * |M|) / N\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    Original: 100 positive, 900 negative (1000 total)\n",
    "    After balancing: 900 positive (800 oversampled), 900 negative (1800 total)\n",
    "    Equivalent epochs = 1800 / 1000 = 1.8 epochs per iteration\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    class_vector : array-like\n",
    "        Class labels for all samples\n",
    "    batch_size : int\n",
    "        Size of each batch\n",
    "    n_splits : int\n",
    "        Number of batches in one iteration\n",
    "    equivalent_epochs : float\n",
    "        How many times the minority class is seen per iteration\n",
    "    \n",
    "    NOTE: This leads to more examples per epoch than the original dataset size.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_vector, batch_size=10):\n",
    "        \"\"\"\n",
    "        Initialize the binary balanced sampler.\n",
    "        \n",
    "        Args:\n",
    "            class_vector: Array of class labels (0 or 1)\n",
    "            batch_size: Number of examples per batch\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.class_vector = class_vector\n",
    "        \n",
    "        # Convert to numpy array for easier manipulation\n",
    "        YY = np.array(self.class_vector)\n",
    "        \n",
    "        # Find unique classes and their counts\n",
    "        # U: unique classes [0, 1]\n",
    "        # C: counts [n_negative, n_positive]\n",
    "        U, C = np.unique(YY, return_counts=True)\n",
    "        \n",
    "        # Find majority class (class with most examples)\n",
    "        M = U[np.argmax(C)]  # Majority class label\n",
    "        \n",
    "        # Get indices of majority and minority classes\n",
    "        Midx = np.nonzero(YY == M)[0]  # Indices where class == M\n",
    "        midx = np.nonzero(YY != M)[0]  # Indices where class != M\n",
    "        \n",
    "        # Oversample minority class to match majority class size\n",
    "        # np.random.choice samples WITH replacement by default\n",
    "        midx_ = np.random.choice(midx, size=len(Midx))\n",
    "        \n",
    "        # Create balanced dataset:\n",
    "        # - All majority class examples\n",
    "        # - Oversampled minority class examples (same count as majority)\n",
    "        self.YY = np.array(list(YY[Midx]) + list(YY[midx_]))\n",
    "        self.idx = np.array(list(Midx) + list(midx_))\n",
    "        \n",
    "        # Calculate number of batches\n",
    "        self.n_splits = int(np.ceil(len(self.idx) / self.batch_size))\n",
    "        \n",
    "        # Calculate equivalent epochs\n",
    "        # This tells us how many times we see the original dataset\n",
    "        self.equivalent_epochs = len(self.idx) / len(self.class_vector)\n",
    "        \n",
    "        print(f'Equivalent epochs in one iteration of data loader: {self.equivalent_epochs:.2f}')\n",
    "        print(f'Original dataset size: {len(self.class_vector)}')\n",
    "        print(f'Balanced dataset size: {len(self.idx)}')\n",
    "        print(f'Number of batches: {self.n_splits}')\n",
    "    \n",
    "    def gen_sample_array(self):\n",
    "        \"\"\"\n",
    "        Generate balanced batches using StratifiedKFold.\n",
    "        \n",
    "        StratifiedKFold ensures each fold (batch) has the same class distribution\n",
    "        as the overall dataset. Since our balanced dataset is 50/50, each batch\n",
    "        will also be approximately 50/50.\n",
    "        \n",
    "        Yields:\n",
    "            numpy array: Indices for each balanced batch\n",
    "        \"\"\"\n",
    "        # Use StratifiedKFold to maintain class balance in each split\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True)\n",
    "        \n",
    "        # For each split, yield the test indices\n",
    "        # tridx: training indices (unused)\n",
    "        # ttidx: test indices (used as batch)\n",
    "        for tridx, ttidx in skf.split(self.idx, self.YY):\n",
    "            yield np.array(self.idx[ttidx])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Return an iterator over batch indices.\"\"\"\n",
    "        return iter(self.gen_sample_array())\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of batches.\"\"\"\n",
    "        return self.n_splits\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple custom dataset wrapper for PyTorch DataLoader.\n",
    "    \n",
    "    This is a minimal Dataset implementation that stores data and labels\n",
    "    and returns them when indexed.\n",
    "    \n",
    "    Attributes:\n",
    "        data: Input features or example identifiers\n",
    "        labels: Corresponding class labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Input data\n",
    "            labels: Class labels\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of the sample\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (data, label) at index idx\n",
    "        \"\"\"\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING THE SAMPLERS (Example Usage)\n",
    "# ============================================================================\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Testing BinaryBalancedSampler with Example Data\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create example data: 100 samples with 70% class 0 and 30% class 1 (imbalanced)\n",
    "    E = [(str(p_i), str(-1 * c_i)) for p_i, c_i in zip(range(100), range(100))]\n",
    "    Y = np.random.randint(0, 2, size=100, p=[0.7, 0.3])  # Imbalanced: 70% class 0\n",
    "    batch_size = 10\n",
    "    \n",
    "    print(f\"\\nOriginal class distribution:\")\n",
    "    print(f\"  Class 0: {np.sum(Y == 0)} examples ({np.sum(Y == 0)/len(Y)*100:.1f}%)\")\n",
    "    print(f\"  Class 1: {np.sum(Y == 1)} examples ({np.sum(Y == 1)/len(Y)*100:.1f}%)\")\n",
    "    \n",
    "    # Test BinaryBalancedSampler\n",
    "    print(\"\\nCreating BinaryBalancedSampler...\")\n",
    "    dataset = CustomDataset(E, Y)\n",
    "    batch_sampler = BinaryBalancedSampler(Y, batch_size)\n",
    "    data_loader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "    \n",
    "    print(\"\\nFirst 3 batches from BinaryBalancedSampler:\")\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        if i >= 3:  # Only show first 3 batches\n",
    "            break\n",
    "        batch_data, batch_labels = batch\n",
    "        n_class0 = torch.sum(batch_labels == 0).item()\n",
    "        n_class1 = torch.sum(batch_labels == 1).item()\n",
    "        print(f\"  Batch {i+1}: Class 0: {n_class0}, Class 1: {n_class1} \"\n",
    "              f\"(Balance: {n_class0/(n_class0+n_class1)*100:.1f}% / \"\n",
    "              f\"{n_class1/(n_class0+n_class1)*100:.1f}%)\")\n",
    "    \n",
    "    # Test create_balanced_loader\n",
    "    print(\"\\nTesting create_balanced_loader...\")\n",
    "    balanced_loader = create_balanced_loader(E, Y, batch_size)\n",
    "    \n",
    "    print(\"\\nFirst 3 batches from WeightedRandomSampler:\")\n",
    "    for i, (batch_data, batch_labels) in enumerate(balanced_loader):\n",
    "        if i >= 3:  # Only show first 3 batches\n",
    "            break\n",
    "        n_class0 = torch.sum(batch_labels == 0).item()\n",
    "        n_class1 = torch.sum(batch_labels == 1).item()\n",
    "        print(f\"  Batch {i+1}: Class 0: {n_class0}, Class 1: {n_class1} \"\n",
    "              f\"(Balance: {n_class0/(n_class0+n_class1)*100:.1f}% / \"\n",
    "              f\"{n_class1/(n_class0+n_class1)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Custom Data Loaders and Samplers Defined Successfully!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Protein Feature Extraction Functions\n",
    "\n",
    "### Purpose\n",
    "Extract structural and chemical features from protein complexes in PDB format.\n",
    "\n",
    "### Overview of Protein Structure\n",
    "\n",
    "A protein structure consists of a hierarchy:\n",
    "```\n",
    "Structure\n",
    "  └── Model (usually just 1)\n",
    "       └── Chain (e.g., A, B)\n",
    "            └── Residue (amino acid)\n",
    "                 └── Atom (C, N, O, etc.)\n",
    "```\n",
    "\n",
    "### Features Extracted\n",
    "\n",
    "**1. Atom Features (`atom1`)**\n",
    "- One-hot encoding of atom types\n",
    "- 13 atom categories: C, CA, CB, CG, CH2, N, NH2, OG, OH, O1, O2, SE, Unknown\n",
    "- Output: N × 13 matrix (N = number of atoms)\n",
    "- Purpose: Represent chemical identity of each atom for GNN\n",
    "\n",
    "**2. Residue Features (`res1`)**\n",
    "- One-hot encoding of amino acid types\n",
    "- 21 categories: 20 standard amino acids + Unknown\n",
    "- Output: N × 21 matrix (N = number of atoms)\n",
    "- Each atom tagged with its parent residue type\n",
    "- Purpose: Capture biochemical properties of amino acids\n",
    "\n",
    "**3. Neighborhood Graph (`neigh1`)**\n",
    "- Finds 10 nearest neighbors for each atom\n",
    "- Separates neighbors by residue:\n",
    "  - **Same residue**: Atoms within same amino acid (local structure)\n",
    "  - **Different residue**: Atoms from other amino acids (inter-residue contacts)\n",
    "- Distance threshold: 6 Angstroms\n",
    "- Output: Two N × 10 adjacency matrices\n",
    "- Purpose: Define graph structure for message passing in GNN\n",
    "\n",
    "### Why These Features?\n",
    "\n",
    "The GNN learns from **local atomic environment**:\n",
    "- Atom type: What is this atom?\n",
    "- Residue type: What amino acid does it belong to?\n",
    "- Neighbors: What atoms are nearby?\n",
    "\n",
    "This captures the 3D structure and chemistry of the protein complex."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
