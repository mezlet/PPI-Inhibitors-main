{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adibayaseen/PPI-Inhibitors/blob/main/code/seqfeaturesand_gnn_generate_prediction_gnn_with_binders_and_random_both_as_negative_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zy8pv8_EQa"
      },
      "source": [
        "**Set the Runtime->Change Runtime Type to GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLKfwNHlwnSE"
      },
      "source": [
        "# Protein 3d structure assessment with graph neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uOhzY0qAj-0",
        "outputId": "55c10558-fb7e-4afb-c2e3-d6d17a8d0ae2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_3U6PueCCgv",
        "outputId": "51518c19-fd79-4c88-c053-3cc9b1ef2079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'PPI-Inhibitors': No such file or directory\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.81-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.23.5)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.81\n",
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 533, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n"
          ]
        }
      ],
      "source": [
        "#!rm -r Data\n",
        "!rm -r PPI-Inhibitors\n",
        "!pip install biopython\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "#!pip install py3Dmol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLRQVvJaNsg5"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Using Sklearn One hot encoder to encode the atoms\n",
        "Output is of size N*M where N is the total number of atoms and M is the total number of encoded features\n",
        "'''\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "def atom1(structure):\n",
        "    atomslist=np.array(sorted(np.array(['C', 'CA', 'CB', 'CG', 'CH2', 'N','NH2',  'OG','OH', 'O1', 'O2', 'SE','1']))).reshape(-1,1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(atomslist)\n",
        "    atom_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_name() in atomslist:\n",
        "            atom_list.append(atom.get_name())\n",
        "        else:\n",
        "            atom_list.append(\"1\")\n",
        "    atoms_onehot=enc.transform(np.array(atom_list).reshape(-1,1)).toarray()\n",
        "    return atoms_onehot\n",
        "##############\n",
        "'''\n",
        "One hot encoded residue infomration using SKlearn Library\n",
        "\n",
        "Output is N*M where N is the total number of atoms and M is the encoded features of the residues.\n",
        "Any unknown  residue is mapped to 1\n",
        "'''\n",
        "\n",
        "\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import glob\n",
        "from Bio.PDB import *\n",
        "\n",
        "\n",
        "def res1(structure):\n",
        "    residuelist=np.array(sorted(np.array(['ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS','1']))).reshape(-1,1)\n",
        "    encr = OneHotEncoder(handle_unknown='ignore')\n",
        "    encr.fit(residuelist)\n",
        "\n",
        "\n",
        "\n",
        "    residue_list=[]\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_parent().get_resname() in residuelist:\n",
        "            residue_list.append((atom.get_parent()).get_resname())\n",
        "        else:\n",
        "            residue_list.append(\"1\")\n",
        "\n",
        "    res_onehot=encr.transform(np.array(residue_list).reshape(-1,1)).toarray()\n",
        "\n",
        "    return res_onehot\n",
        "###########\n",
        "\n",
        "'''\n",
        "It calculates the neighbours of each atom i.e. 10 distinct neighbours\n",
        "Output is  in the form of a ditionary representing an  adjacency list where each source atom and neighbouring atom is represented bby its sequence index .\n",
        "'''\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "import numpy as np\n",
        "from Bio.PDB.NeighborSearch import NeighborSearch\n",
        "\n",
        "def neigh1(structure):\n",
        "    #atom_list is a numpy array  that   contains all the atoms of the pdb file in atom object\n",
        "    atom_list=np.array([atom for atom in structure.get_atoms()])\n",
        "\n",
        "    #for atom in structure.get_atoms():\n",
        "    #    atom_list.append(atom)\n",
        "    #neighbour_list contains all the  neighbour atomic pairs  i.e. like if N has neighbours O and C then it is stored as [[N,C],[N,O]] i.e. has dimension N*2 where N is the total number of possible neighbours all the atoms have in an unsorted manner and it stores in the form of  atom object\n",
        "\n",
        "\n",
        "    p4=NeighborSearch(atom_list)\n",
        "    neighbour_list=p4.search_all(6,level=\"A\")\n",
        "    neighbour_list=np.array(neighbour_list)\n",
        "\n",
        "    #dist is the distance between the neighbour and the source atom  i.e. dimension is N*1\n",
        "    dist=np.array(neighbour_list[:,0]-neighbour_list[:,1])\n",
        "    #sorting in ascending order\n",
        "    place=np.argsort(dist)\n",
        "    sorted_neighbour_list=neighbour_list[place]\n",
        "\n",
        "    #old_atom_number is used for  storing atom id of the original protein before sorting\n",
        "    #old_residue_number is used for storing residue number of the original protein before sorting\n",
        "    source_vertex_list_atom_object=np.array(sorted_neighbour_list[:,0])\n",
        "    len_source_vertex=len(source_vertex_list_atom_object)\n",
        "    neighbour_vertex_with_respect_each_source_atom_object=np.array(sorted_neighbour_list[:,1])\n",
        "    old_atom_number=[]\n",
        "    old_residue_number=[]\n",
        "    for i in atom_list:\n",
        "        old_atom_number.append(i.get_serial_number())\n",
        "        old_residue_number.append(i.get_parent().get_id()[1])\n",
        "    old_atom_number=np.array(old_atom_number)\n",
        "    old_residue_number=np.array(old_residue_number)\n",
        "    req_no=len(neighbour_list)\n",
        "    total_atoms=len(atom_list)\n",
        "    #neigh_same_res is the 2D numpy array to store the indices of the  neighbours of  same residue and is of the shape N*10 where N is the total number of atoms\n",
        "    #neigh_diff_res is 2D numpy array to store  the indices of the  neighbours of different residue\n",
        "    #same_flag is used to restrict the neighbours belonging to same residue  to 10\n",
        "    #diff_flag is used to restrict the neighbours belonging to different residue to 10\n",
        "    neigh_same_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    neigh_diff_res=np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    same_flag=[0]*total_atoms\n",
        "    diff_flag=[0]*total_atoms\n",
        "    for i in range(len_source_vertex):\n",
        "        source_atom_id=source_vertex_list_atom_object[i].get_serial_number()\n",
        "        neigh_atom_id=neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
        "        source_atom_res=source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
        "        neigh_atom_res=neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
        "        #finding out index of the source and neighbouring atoms from the original atom array with respect to their residue id and atom id\n",
        "        temp_index1=np.where(source_atom_id==old_atom_number)[0]\n",
        "\n",
        "        temp_index2=np.where(neigh_atom_id==old_atom_number)[0]\n",
        "        for i1 in temp_index1:\n",
        "            if old_residue_number[i1]==source_atom_res:\n",
        "                source_index=i1\n",
        "                break\n",
        "        for i1 in temp_index2:\n",
        "            if old_residue_number[i1]==neigh_atom_res:\n",
        "                neigh_index=i1\n",
        "                break\n",
        "        #if both the residues are same\n",
        "\n",
        "        if source_atom_res==neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of same residue to 10\n",
        "\n",
        "            if int(same_flag[source_index])< 10:\n",
        "                neigh_same_res[source_index][same_flag[source_index]]=neigh_index\n",
        "                same_flag[source_index]+=1\n",
        "\n",
        "            if int(same_flag[neigh_index])< 10:\n",
        "                neigh_same_res[neigh_index][same_flag[neigh_index]]=source_index\n",
        "                same_flag[neigh_index]+=1\n",
        "\n",
        "        # if both the residues are different\n",
        "        elif source_atom_res!=neigh_atom_res :\n",
        "\n",
        "            #limiting the number of neighbours of different residues to 10\n",
        "\n",
        "            if int(diff_flag[source_index])< 10:\n",
        "                neigh_diff_res[source_index][diff_flag[source_index]]=neigh_index\n",
        "                diff_flag[source_index]+=1\n",
        "\n",
        "\n",
        "            if int(diff_flag[neigh_index])< 10:\n",
        "\n",
        "                neigh_diff_res[neigh_index][diff_flag[neigh_index]]=source_index\n",
        "                diff_flag[neigh_index]+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return neigh_same_res,neigh_diff_res\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "#from QA.data_import import get_dataloader,data1\n",
        "#from QA.temp_network import GNN\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "        #print(\"Wsv shape\",self.Wsv.shape)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        #print(\"input\",x)\n",
        "        #print(Z.shape)\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "        #pdb.set_trace()\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        #print(\"Here i am in froward of first layer\")\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        #print(\"input atoms\",atoms)\n",
        "        #print(atoms.shape)\n",
        "        #print(\"Wv shape\",self.Wv.shape)\n",
        "        node_signals = atoms@self.Wv\n",
        "        ####\n",
        "        #print(\"input residues\",residues)\n",
        "        #print(residues.shape)\n",
        "        #print(\"Wr shape\",self.Wr.shape)\n",
        "        ####\n",
        "        residue_signals = residues@self.Wr\n",
        "        #print(\"Wsr shape\",self.Wsr.shape)\n",
        "        #print(\"Wdr shape\",self.Wdr.shape)\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        #print(\"neigh_signals_same shape\",neigh_signals_same.shape)\n",
        "        ####\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        \"\"\"\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)\n",
        "        \"\"\"\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        #print(\"same norm\",same_neigh > -1, 1)\n",
        "        #1/0\n",
        "        #Orignal\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        same_norm = torch.sum(same_neigh > -1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1).type(torch.float)\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "        #pdb.set_trace()\n",
        "        #print(\" in First layer final_res.shape\",final_res.shape,\"same_neigh.shape\",same_neigh.shape,\"diff_neigh.shape\",diff_neigh.shape)\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "        #pdb.set_trace()\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        #pdb.set_trace()\n",
        "        #Actual\n",
        "        \"\"\"\n",
        "        self.conv1 = GNN_First_Layer(filters=128)\n",
        "        self.conv2 = GNN_Layer(v_feats=128, filters=256)\n",
        "        self.conv3 = GNN_Layer(v_feats=256, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "        \"\"\"\n",
        "        #modified by me\n",
        "        self.conv1 = GNN_First_Layer(filters=512)\n",
        "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
        "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "    def forward(self, x):\n",
        "        #pdb.set_trace()\n",
        "        x1=self.conv1(x)\n",
        "        #pdb.set_trace()\n",
        "        x2=self.conv2(x1)\n",
        "        #pdb.set_trace()\n",
        "        x3=self.conv3(x2)\n",
        "        #pdb.set_trace()\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        #pdb.set_trace()\n",
        "        x = F.normalize(x)\n",
        "        \"\"\"\n",
        "        #pdb.set_trace()\n",
        "        x5=self.dense(x)\n",
        "        #pdb.set_trace()\n",
        "        x6=torch.squeeze(x5,1)\n",
        "        \"\"\"\n",
        "        #print(\"GNN\")\n",
        "\n",
        "        return x#x6,\n",
        "\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "        #PdBloc='/content/PPI-Inhibitors/Data/Pdb/'\n",
        "        #data=glob.glob(PdBloc+'/*')\n",
        "        #data=glob.glob(PdBloc)\n",
        "        #data=data[0:2]\n",
        "        #assert (len(UniqueProtein) == len(data)) , \"The two lists must be the same length!\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_atom.shape)\n",
        "            #\n",
        "            one_hot_res=(res1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            #print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            #import pdb; pdb.set_trace()\n",
        "            #GNNData.__setitem__('Total atoms', len(one_hot_atom))\n",
        "            #data_list.append(GNNData)\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "def readFile(filename):\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  Name=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      #if len(d)==6:\n",
        "      name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "      Name.append(name);PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);labels.append(float (y));\n",
        "  return  PdbId,Ligandnames,SMILES,labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzwASCsDn1O2"
      },
      "outputs": [],
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\"\"\"\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gglFcYgfcvGE"
      },
      "outputs": [],
      "source": [
        "#Compound part\n",
        "!pip install kora\n",
        "import kora.install.rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRjE8tIydDv0"
      },
      "outputs": [],
      "source": [
        "#from torch_geometric.data import InMemoryDataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE6yIRsQ7C5Y"
      },
      "outputs": [],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etHfJlkb6uhF"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_mtVh7bdQrc"
      },
      "outputs": [],
      "source": [
        "#from torch_geometric.nn.conv import gcn_conv\n",
        "##\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json,pickle\n",
        "from collections import OrderedDict\n",
        "from rdkit import Chem\n",
        "#from rdkit.Chem import MolFromSmiles\n",
        "import networkx as nx\n",
        "#from utils import *\n",
        "# training function at each epoch\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    print('Training on {} samples...'.format(len(GCN_data)))\n",
        "\n",
        "    model.train()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        #data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output,Sfeatures= model(data)#,Sfeatures\n",
        "        loss = loss_fn(output, data.target.view(-1, 1).float().to(device))\n",
        "        #loss = loss_fn(output, data.Target.view(-1, 1).float().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,\n",
        "                                                                           batch_idx * len(data.x),\n",
        "                                                                           len(train_loader.dataset),\n",
        "                                                                           100. * batch_idx / len(train_loader),\n",
        "                                                                           loss.item()))\n",
        "\n",
        "def predicting(model, device,test_loader,labels):\n",
        "    model.eval()\n",
        "    total_preds = torch.Tensor()\n",
        "    total_labels = torch.Tensor()\n",
        "    print('Make prediction for {} samples...'.format(len(test_loader)))\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            #data = data.to(device)\n",
        "            try:\n",
        "              output,Sfeatures = model(data)#,Sfeatures\n",
        "            except Exception as trr:\n",
        "              print (trr)\n",
        "              continue\n",
        "            #output = model(data)\n",
        "            #print (\"output\",output)\n",
        "            #pdb.set_trace()\n",
        "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "            total_labels = torch.cat((total_labels, data.target.view(-1, 1).cpu()), 0)\n",
        "    return total_labels.numpy().flatten(),total_preds.numpy().flatten(),Sfeatures\n",
        "##\n",
        "def atom_features(atom):\n",
        "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb','Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr','Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
        "                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6,7,8,9,10]) +\n",
        "                    [atom.GetIsAromatic()])\n",
        "\n",
        "def one_of_k_encoding(x, allowable_set):\n",
        "    if x not in allowable_set:\n",
        "        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def one_of_k_encoding_unk(x, allowable_set):\n",
        "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
        "    if x not in allowable_set:\n",
        "        x = allowable_set[-1]\n",
        "    return list(map(lambda s: x == s, allowable_set))\n",
        "\n",
        "def smile_to_graph(smile):\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol is not None:\n",
        "      c_size = mol.GetNumAtoms()\n",
        "      features = []\n",
        "      for atom in mol.GetAtoms() :\n",
        "          feature = atom_features(atom)\n",
        "          features.append( feature / sum(feature) )\n",
        "\n",
        "      edges = []\n",
        "      for bond in mol.GetBonds():\n",
        "          edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
        "      g = nx.Graph(edges).to_directed()\n",
        "      edge_index = []\n",
        "      for e1, e2 in g.edges:\n",
        "          edge_index.append([e1, e2])\n",
        "\n",
        "      return c_size, features, edge_index\n",
        "import os\n",
        "import numpy as np\n",
        "from math import sqrt\n",
        "from scipy import stats\n",
        "#from torch_geometric.data import InMemoryDataset, DataLoader\n",
        "#from torch_geometric import data as DATA\n",
        "import torch\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "class PrepairDataset():\n",
        "    \"\"\"\n",
        "    def __init__(self, root='/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt', dataset='PPI-Inhibitor',\n",
        "                 xd=None, xt=None, y=None, transform=None,\n",
        "                 pre_transform=None,smile_graph=None,PdBloc='/content/PPI-Inhibitors/Data/Pdb/*'):\n",
        "\n",
        "        #root is required for save preprocessed data, default is '/tmp'\n",
        "        super(PrepairDataset, self).__init__(root, transform, pre_transform)\n",
        "        # benchmark dataset, default = 'davis'\n",
        "        self.dataset = dataset\n",
        "        if os.path.isfile(self.processed_paths[0]):\n",
        "            print('Pre-processed data found: {}, loading ...'.format(self.processed_paths[0]))\n",
        "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "        else:\n",
        "            print('Pre-processed data {} not found, doing pre-processing...'.format(self.processed_paths[0]))\n",
        "            with open(root) as f:\n",
        "              D = f.readlines()\n",
        "            Name=[];PdbId=[];LigandId=[];SMILES=[];labels=[];\n",
        "            from tqdm import tqdm as tqdm\n",
        "            #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "            for d in tqdm(D):\n",
        "                name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "                Name.append(name);PdbId.append(Pdbid);LigandId.append(Ligandid);SMILES.append(smiles);labels.append(y);\n",
        "            PdBloc='/content/PPI-Inhibitors/Data/Pdb'\n",
        "            self.process(SMILES, PdbId, labels,smile_graph,PdBloc)\n",
        "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "    \"\"\"\n",
        "    def ProcessfromFile(self,filename,smile_graph):\n",
        "        PdBloc='/content/PPI-Inhibitors/Data/Pdb'\n",
        "        with open(filename) as f:\n",
        "          D = f.readlines()\n",
        "        Name=[];PdbId=[];LigandId=[];SMILES=[];labels=[];\n",
        "        All_data_list=[]\n",
        "        from tqdm import tqdm as tqdm\n",
        "        #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "        for d in tqdm(D):\n",
        "            if len(d)==6:\n",
        "              name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "              Name.append(name);PdbId.append(Pdbid);LigandId.append(Ligandid);SMILES.append(smiles);labels.append(y);\n",
        "            elif len(d)==2:\n",
        "              Pdbid,smiles= d.split()\n",
        "              PdbId.append(Pdbid);SMILES.append(smiles);labels.append(float(-1.0));\n",
        "         ########\n",
        "        UniqueProtein=list (set (PdbId))\n",
        "        UniqueSMILES=list (set (SMILES))\n",
        "        ProteinData_dict=PrepairDataset.processProtein(UniqueProtein,PdBloc)\n",
        "        SmilesData_dict=PrepairDataset.processSMILES(UniqueSMILES, labels_list,smile_graph)\n",
        "        for e in range(len(SMILES)):\n",
        "          Pdata,Sdata,label=PdbId[e],SMILES[e],labels[e]\n",
        "          #Preprocessed data\n",
        "          if Sdata in SmilesData_dict and Pdata in ProteinData_dict:\n",
        "            Pdata,Sdata=ProteinData_dict[Pdata],SmilesData_dict[Sdata]\n",
        "            All_data=DATA.data(Pdata,Sdata,label)\n",
        "            All_data_list.append(All_data)\n",
        "        return All_data_list\n",
        "    # Customize the process method to fit the task of drug-target affinity prediction\n",
        "    # Inputs:\n",
        "    # XD - list of SMILES\n",
        "    #XT: target Protein PDB id\n",
        "    # Y: list of labels (i.e. affinity)\n",
        "    # Return: PyTorch-Geometric format processed data\n",
        "    def process(xd, xt, y,smile_graph):\n",
        "        assert (len(xd) == len(xt) and len(xt) == len(y)), \"The three lists must be the same length!\"\n",
        "        data_list = []\n",
        "        data_len = len(xd)\n",
        "        print(\"data len\",data_len)\n",
        "        smiles_list=[];targetP_list=[];labels_list=[];\n",
        "        for i in range(data_len):\n",
        "            #print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n",
        "            smiles = xd[i]\n",
        "            targetP = xt[i]\n",
        "            labels = y[i]\n",
        "            smiles_list.append(smiles);targetP_list.append(targetP);labels_list.append(labels);\n",
        "        ########\n",
        "        UniqueProtein=list (set (targetP_list))\n",
        "        UniqueSMILES=list (set (smiles_list))\n",
        "        ProteinData_dict=PrepairDataset.processProtein(UniqueProtein[0:2])#,PdBloc)\n",
        "        print( \"total pdbs\",len(ProteinData_dict))\n",
        "        SmilesData_dict=PrepairDataset.processSMILES(UniqueSMILES, labels_list,smile_graph)\n",
        "        All_data_list=[]\n",
        "        \"\"\"\n",
        "        with open(filename) as f:\n",
        "          D = f.readlines()\n",
        "        Name=[];PdbId=[];LigandId=[];SMILES=[];labels=[];\n",
        "        All_data_list=[]\n",
        "        from tqdm import tqdm as tqdm\n",
        "        #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "        for d in tqdm(D):\n",
        "            if len(d)==6:\n",
        "              name,inhibtedc,Pdbid,Ligandid,smiles,y = d.split()\n",
        "              Name.append(name);PdbId.append(Pdbid);LigandId.append(Ligandid);SMILES.append(smiles);labels.append(y);\n",
        "              #Preprocessed data\n",
        "              Pdata,Sdata,label=ProteinData_dict[Pdbid],SmilesData_dict[smiles],y\n",
        "              All_data=DATA.data(Pdata,Sdata,label)\n",
        "              All_data_list.append(All_data)\n",
        "            elif len(d)==2:\n",
        "              Pdbid,smiles= d.split()\n",
        "              PdbId.append(Pdbid);SMILES.append(smiles);labels.append(float(-1.0));\n",
        "              #Preprocessed data\n",
        "              Pdata,Sdata,label=ProteinData_dict[Pdbid],SmilesData_dict[smiles],float(-1.0)\n",
        "              All_data=DATA.data(Pdata,Sdata,label)\n",
        "              All_data_list.append(All_data)\n",
        "        return All_data_list\n",
        "        \"\"\"\n",
        "        for e in range(len(smiles_list)):\n",
        "          Pdata,Sdata,label=targetP_list[e],smiles_list[e],labels_list[e]\n",
        "          #Preprocessed data\n",
        "          if Sdata in SmilesData_dict and Pdata in ProteinData_dict:\n",
        "            print(\"Sdata,Pdata,label\",Sdata,Pdata,label)\n",
        "            GNN_data,GCN_data=ProteinData_dict[Pdata],SmilesData_dict[Sdata]\n",
        "            All_data=(GNN_data,GCN_data,label)\n",
        "            All_data_list.append(All_data)\n",
        "        return All_data_list\n",
        "    def processProtein(UniqueProtein, PdBloc):#, PdBloc):\n",
        "        data_list = []\n",
        "        #PdBloc='/content/PPI-Inhibitors/Data/Pdb/'\n",
        "        #data=glob.glob(PdBloc+'/*')\n",
        "        #data=glob.glob(PdBloc)\n",
        "        #data=data[0:2]\n",
        "        #assert (len(UniqueProtein) == len(data)) , \"The two lists must be the same length!\"\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        PData_dict={}\n",
        "        for i in range(len(UniqueProtein)):\n",
        "            #print('Converting PDB to Graph: {}/{}'.format(i+1, len(UniqueProtein)))\n",
        "            UniqueProtein[i]=UniqueProtein[i].split('.pdb')[0]\n",
        "            P1=PdBloc+UniqueProtein[i]+'.pdb'\n",
        "            #if P1 in UniqueProtein:\n",
        "            parser = PDBParser()\n",
        "            with warnings.catch_warnings(record=True) as w:\n",
        "              structure = parser.get_structure(\"\", P1)\n",
        "            one_hot_atom=(atom1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_atom.shape)\n",
        "            #\n",
        "            one_hot_res=(res1(structure))\n",
        "            #print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "            neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "            #print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "            # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "            one_hot_atom=torch.tensor(one_hot_atom,dtype=torch.float32).to(device)\n",
        "            one_hot_res=torch.tensor(one_hot_res,dtype=torch.float32).to(device)\n",
        "            neigh_same_res=torch.tensor(neigh_same_res).to(device).long()\n",
        "            neigh_diff_res=torch.tensor(neigh_diff_res).to(device).long()\n",
        "            GNNData = [one_hot_atom,one_hot_res,neigh_same_res,neigh_diff_res]\n",
        "            #import pdb; pdb.set_trace()\n",
        "            #GNNData.__setitem__('Total atoms', len(one_hot_atom))\n",
        "            #data_list.append(GNNData)\n",
        "            PData_dict[UniqueProtein[i]]= GNNData\n",
        "        return PData_dict\n",
        "    def processSMILES(xd, targets,smile_graph):\n",
        "        #assert (len(xd) == len(smile_graph)), \"The two lists must be the same length!\"\n",
        "        data_list = []\n",
        "        data_len = len(xd)\n",
        "        Sdata_dict={}\n",
        "        for i in range(data_len):\n",
        "            print('Converting SMILES to graph: {}/{}'.format(i+1, data_len))\n",
        "            smiles = xd[i]\n",
        "            target=targets[i]\n",
        "            print(\"target\",target)\n",
        "            mol = Chem.MolFromSmiles(smile)\n",
        "            if smile in smile_graph and mol is not None and smile_graph[smiles] is not None:\n",
        "              #import pdb; pdb.set_trace()\n",
        "              # convert SMILES to molecular representation using rdkit\n",
        "              c_size, features, edge_index = smile_graph[smiles]\n",
        "              if c_size>1:\n",
        "                # make the graph ready for PyTorch Geometrics GCN algorithms:\n",
        "                GCNData = DATA.Data(x=torch.Tensor(features),\n",
        "                                    edge_index=torch.LongTensor(edge_index).transpose(1, 0),target=torch.LongTensor([target]))\n",
        "                #import pdb; pdb.set_trace()\n",
        "              # GCNData.target = torch.LongTensor([target])\n",
        "                #GCNData.__setitem__('c_size', torch.LongTensor([c_size]))\n",
        "                # append graph, label and target sequence to data list\n",
        "                #data_list.append(GCNData)\n",
        "                Sdata_dict[smiles]=GCNData\n",
        "        #Sdata_dict=dict(zip(xd,data_list))\n",
        "        return Sdata_dict\n",
        "        if self.pre_filter is not None:\n",
        "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data_list = [self.pre_transform(data) for data in data_list]\n",
        "        print('Graph construction done. Saving to file.')\n",
        "        data, slices = self.collate(data_list)\n",
        "        # save preprocessed data:\n",
        "        torch.save((data, slices), self.processed_paths[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_uTP5QndCu5"
      },
      "outputs": [],
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import DataStructs\n",
        "def getFP(s,r = 3,nBits =2048):\n",
        "    compound = Chem.MolFromSmiles(s.strip())\n",
        "    if compound is not None:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(compound, r, nBits = nBits)\n",
        "        #fp = pat.GetAvalonCountFP(compound,nBits=nBits)\n",
        "        m = np.zeros((0, ), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, m)\n",
        "        return m\n",
        "def twomerFromSeq(s):\n",
        "    k=2\n",
        "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
        "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
        "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
        "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
        "    for i in range (0,len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
        "    ind=[]\n",
        "    for i in range (0,len(crossproduct)): ind.append(i)\n",
        "    combinations=dict(zip(crossproduct,ind))\n",
        "\n",
        "    V=np.zeros(int((math.pow(7,k))))      #defines a vector of 343 length with zero entries\n",
        "    try:\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                c+=groups[kmer[l]]\n",
        "                V[combinations[int(c)]]+=1\n",
        "    except:\n",
        "        count={'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0}\n",
        "        for q in range(0,len(s)):\n",
        "            if s[q]=='A' or s[q]=='V' or s[q]=='G':\n",
        "                count['1']+=1\n",
        "            if s[q]=='I' or s[q]=='L'or s[q]=='F' or s[q]=='P':\n",
        "                count['2']+=1\n",
        "            if s[q]=='Y' or s[q]=='M'or s[q]=='T' or s[q]=='S':\n",
        "                count['3']+=1\n",
        "            if s[q]=='H' or s[q]=='N'or s[q]=='Q' or s[q]=='W':\n",
        "                count['4']+=1\n",
        "            if s[q]=='R' or s[q]=='K':\n",
        "                count['5']+=1\n",
        "            if s[q]=='D' or s[q]=='E':\n",
        "                count['6']+=1\n",
        "            if s[q]=='C':\n",
        "                count['7']+=1\n",
        "        val=list(count.values()  )           #[ 0,0,0,0,0,0,0]\n",
        "        key=list(count.keys()     )           #['1', '2', '3', '4', '5', '6', '7']\n",
        "        m=0\n",
        "        ind=0\n",
        "        for t in range(0,len(val)):     #find maximum value from val\n",
        "            if m<val[t]:\n",
        "                m=val[t]\n",
        "                ind=t\n",
        "        m=key [ind]                     # m=group number of maximum occuring group alphabets in protein\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                if kmer[l] not in groups:\n",
        "                    c+=m\n",
        "                else:\n",
        "                    c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "\n",
        "    V=V/(len(s)-1)\n",
        "    return np.array(V)\n",
        "from scipy import spatial\n",
        "def chainLabel(Cname_T,xl_T,Cname,xl):\n",
        "    \"\"\"\n",
        "    Cname_T: Target chain Name\n",
        "    xl_T: Target chain co-ordinates\n",
        "    Cname: Off Target chain Name\n",
        "    xl: Off Target chain co-ordinates\n",
        "    \"\"\"\n",
        "    tc = getCoords(xl_T)\n",
        "    nc = getCoords(xl)\n",
        "    D = getDist(tc, nc, thr = 8.0)\n",
        "    feats=extract_feats(generate_pair_features(D,xl_T,xl))\n",
        "    return feats\n",
        "def generate_pair_features(dist_info,xl,xr):\n",
        "    prot_dic=make_dic()\n",
        "#    pdb.set_trace()\n",
        "    for rec in dist_info:\n",
        "\n",
        "        try:\n",
        "            l_letter= three_to_one(xl[rec[0]].get_resname())\n",
        "            r_letter= three_to_one(xr[rec[1]].get_resname())\n",
        "#            print(l_letter,l_letter)\n",
        "            if (l_letter,r_letter) in prot_dic.keys():\n",
        "                prot_dic[(l_letter,r_letter)]+=1\n",
        "            elif (r_letter,l_letter) in prot_dic.keys():\n",
        "                prot_dic[(r_letter,l_letter)]+=1\n",
        "        except:\n",
        "            prot_dic[('_','_')]+=1\n",
        "    return prot_dic\n",
        "def getCoords(R):\n",
        "    \"\"\"\n",
        "    Get atom coordinates given a list of biopython residues\n",
        "    \"\"\"\n",
        "    Coords = []\n",
        "    for (idx, r) in enumerate(R):\n",
        "        v = [ak.get_coord() for ak in r.get_list()]\n",
        "        Coords.append(v)\n",
        "    return Coords\n",
        "def InterfaceFeatures(Complexs,pdbloc):\n",
        "    Found =  listdir(pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    comp_id=list(set(Complexs))\n",
        "    for ids in range(len(comp_id)):\n",
        "        if comp_id[ids]+'.pdb' in Found:\n",
        "            stx=pdbloc+'/'+comp_id[ids]+'.pdb'#'/2XA0.pdb'\n",
        "            chains=Struct2chain(stx)\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=comp_id[ids]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "    #pickle.dump(InterfaceFeatures, open(path+Filename+\"_InterfaceFeatures.npy\", \"wb\"))\n",
        "    return InterfaceFeatures\n",
        "def getDist(C0, C1, thr=np.inf):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    N0 = []\n",
        "    N1 = []\n",
        "    for i in range(len(C0)):\n",
        "        for j in range(len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            # dji=spatial.distance.cdist(C1[j], C0[i]).min()\n",
        "            #d=min(dij,dji)\n",
        "            #print d\n",
        "            if (d < thr):  # and not np.isnan(self.Phi[i]) and not np.isnan(self.Phi[j])\n",
        "                N0.append((i, j, d))\n",
        "                N1.append((j, i, d))\n",
        "    return (N0, N1)\n",
        "from Bio import SeqIO\n",
        "from Bio.SeqIO import FastaIO\n",
        "from itertools import product\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.preprocessing import normalize\n",
        "import math\n",
        "import numpy as np\n",
        "from Bio.Data import IUPACData\n",
        "from Bio.PDB.Polypeptide import *\n",
        "def prot_feats_seq(seq):\n",
        "    #Interfacedict=pickle.load(open(path+\"InhibitorNewModel2022/InterfaceFeatures2chainsSVM.npy\",\"rb\"))\n",
        "    #InterfaceF=Interfacedict[complexname]\n",
        "    aa=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    f=[]\n",
        "    X = ProteinAnalysis(str(seq))\n",
        "    X.molecular_weight() #throws an error if 'X' in sequence. we skip such sequences\n",
        "    p=X.get_amino_acids_percent()\n",
        "    dp=[]\n",
        "    for a in aa:\n",
        "        dp.append(p[a])\n",
        "    dp=np.array(dp)\n",
        "    dp=normalize(np.atleast_2d(dp), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "    f.extend(dp[0])\n",
        "\n",
        "    tm=np.array(twomerFromSeq(str(seq)))\n",
        "    tm=normalize(np.atleast_2d(tm), norm='l2', copy=True, axis=1,return_norm=False)\n",
        "    f.extend(tm[0])\n",
        "    return np.array(f)\n",
        "def Struct2chain(stx):\n",
        "    \"\"\"\n",
        "    Seq: sequence of the chain\n",
        "    seq_L:sequence Length\n",
        "    \"\"\"\n",
        "    p = PDBParser()\n",
        "    L=[]\n",
        "    stx=p.get_structure('X',stx)\n",
        "    for model in stx:\n",
        "        for C in model:\n",
        "            RL=[]\n",
        "            for R in C:\n",
        "                RL.append(R)\n",
        "            pp=PPBuilder().build_peptides(C)\n",
        "            if len(pp)==0:\n",
        "                pp=CaPPBuilder().build_peptides(C)\n",
        "            seq=''.join([str(p.get_sequence()) for p in pp])\n",
        "            #seq=''.join([p.get_sequence().tostring() for p in pp])\n",
        "            seq_L=len(seq)\n",
        "            L.append((C.full_id[2],seq,seq_L,RL))\n",
        "    return L\n",
        "def extract_feats(dic):\n",
        "    feats=[]\n",
        "    key_list=np.load('/content/PPI-Inhibitors/Features/'+'prote_letter_pair_keys.npy')#to keep features order same\n",
        "    for key in key_list:\n",
        "#        pdb.set_trace()\n",
        "        feats.append(dic[(key[0].decode('utf-8'),key[1].decode('utf-8'))])\n",
        "\n",
        "    return feats\n",
        "def make_dic():\n",
        "    prot_dic={}\n",
        "    letters=IUPACData.protein_letters\n",
        "    for i in range(len(letters)):\n",
        "        for j in range(i,len(letters)):\n",
        "            prot_dic[(letters[i],letters[j])]=0.0\n",
        "    prot_dic[('_','_')]=0.0# for Amino acids other than 20 natural\n",
        "    return prot_dic\n",
        "def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "    pdbname=listdir(Pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "    AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "    for  b in range(len(UniqueProtein)):\n",
        "        if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "            stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'#\n",
        "            chains=Struct2chain(stx)\n",
        "            #########Interface Features\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    seq_TF=prot_feats_seq(seq_T)\n",
        "                    seq_NTF=prot_feats_seq(seq)\n",
        "                    SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "                        SequenceFeatures[name]=SeQFeatures\n",
        "                        AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    return InterfaceFeatures,SequenceFeatures,AllFeatures\n",
        "def External_GenerateRandomNegative(posexamples):\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank###Names\n",
        "    SuperdrugNames=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    #path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open('/content/PPI-Inhibitors/Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];complex_ligand_dict={};\n",
        "    for key,val in  posexamples:\n",
        "      #print(key,val,posexamples[key,val][1])\n",
        "      if key not in complex_ligand_dict:\n",
        "        complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "      else:\n",
        "        #print(\"else\",key,val)\n",
        "        complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    totalcomp=list (set (complex_ligand_dict.keys()))\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        #print(origanlL)\n",
        "        #print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        #print(pos)\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    #print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg),SuperDrug_dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "def PredictScorefromFile(filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN,LOCOcomplexname):\n",
        "  githubpath='/content/PPI-Inhibitors/'\n",
        "#filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN=githubpath+'Data/External data/2dyh_all.txt',githubpath+'Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];targets=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "      if getFP(smiles) is not None:\n",
        "        PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "  ################\n",
        "  \"\"\"\n",
        "  Result_dict={}\n",
        "  pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "  Pos,Negs,SuperDrug_dict=External_GenerateRandomNegative(pos)\n",
        "  poslabel=1.0*np.ones(len(Pos));neglabel=-1.0*np.ones(len(Negs));targets=np.append(poslabel,neglabel )\n",
        "  All_examples=[];All_examples.extend(Pos);All_examples.extend(Negs)\n",
        "  #Write File for External\n",
        "  External_All_Examples=open('/content/drive/MyDrive/GNN-PPI-Inhibitor/_'+LOCOcomplexname+'_'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt',\"w\")\n",
        "  \"\"\"\n",
        "  complexnames=[];SMILES=[];targets=[];\n",
        "  #/content/PPI-Inhibitors/Data/External data/2dyh_all_External_All_Examples.txt\n",
        "  with open('/content/PPI-Inhibitors/Data/External data/'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt') as f:\n",
        "    D = f.readlines()\n",
        "  for d in tqdm(D):\n",
        "      complexname,smiles,target= d.split()\n",
        "      complexnames.append(complexname);SMILES.append(smiles);targets.append(target);#All_examples.append()\n",
        "  pdbname=listdir(Pdbloc);mypdb=[]\n",
        "  for p in pdbname:\n",
        "    if p.split('.pdb')[0] in Pdbid:\n",
        "      mypdb.append(p)\n",
        "  UniqueProtein=list (set (mypdb))\n",
        "  External_Protein_GNN_Data_dict=PrepairDataset.processProtein(UniqueProtein,Pdbloc)\n",
        "  ##########for Seq+interface features\n",
        "  #pdbname=listdir(Pdbloc)\n",
        "  s,i,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features(UniqueProtein,Pdbloc)\n",
        "  ##############3 for sequence fedatures of DBD5 pdb's\n",
        "  Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "  All_External_ProteinSeqandInterfaceData_dict=dict( list (External_ProteinSeqandInterfaceData_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "  #Testing\n",
        "  DBD5_Protein_GNN_Data_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "  #Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "  All_Protein_GNN_Data_dict=dict( list (External_Protein_GNN_Data_dict.items())+list (DBD5_Protein_GNN_Data_dict.items()))\n",
        "  for d in All_Protein_GNN_Data_dict:\n",
        "    data=All_Protein_GNN_Data_dict[d]\n",
        "    All_Protein_GNN_Data_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "  #####################\n",
        "  Cttname=[];Ctt=[];Pttname=[];Ptt=[];\n",
        "  for (complexname,ligandsmile) in zip(complexnames,SMILES):#All_examples:#\n",
        "    Cttname.append(ligandsmile);Ctt.append(getFP(ligandsmile));\n",
        "    Pttname.append(complexname);Ptt.append(All_External_ProteinSeqandInterfaceData_dict[complexname][0:69]);\n",
        "  #standarization\n",
        "\n",
        "  Ctt = Cscaler.transform(Ctt)\n",
        "  Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "  Ptt = Pscaler.transform(Ptt)\n",
        "  Pttdict=dict (zip (Pttname,torch.FloatTensor( Ptt).cuda()))\n",
        "  #########\n",
        "  Y_t,Z,Targets=[],[],[]\n",
        "  for target,(complexname,ligandsmile) in zip(targets,zip(complexnames,SMILES)):#zip(targets,All_examples):#\n",
        "    #target=targets[nt]\n",
        "    #IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "    #External_All_Examples.write(complexname+' '+ligandsmile+' '+str(target)+'\\n')\n",
        "    test_score=trainedModel_IPPI(All_Protein_GNN_Data_dict[complexname],Cttdict[ligandsmile],Pttdict[complexname],train_GNN)\n",
        "    #print (test_score)\n",
        "    test_score=test_score.cpu().data.numpy()[0]\n",
        "    Z.append(test_score);Targets.append(float (target))\n",
        "    #Result_dict[(complexname,Ligandname)]=test_score\n",
        "  return Z,Targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0NjGHV8ewJC"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZfmspC7D6D"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "class IPPI_MLP_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IPPI_MLP_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2560, 512)#1024)\n",
        "        #self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 100)\n",
        "        self.fc6 = nn.Linear(100, 1)\n",
        "    def forward(self, proteinF,LigandFeatures,ProteinInterfaceF,GNN_model):\n",
        "          #call GNN for protein\n",
        "          #result,PFeatures=GNN_model(proteinF)\n",
        "          PFeatures=GNN_model(proteinF)\n",
        "          #print(PFeatures[0])\n",
        "          Cfeatures=LigandFeatures#torch.FloatTensor()#.cuda()#Compound_Net(LigandFeatures)\n",
        "          P_all_Features=PFeatures[0]#torch.hstack((PFeatures[0])#,ProteinInterfaceF))\n",
        "          PC_Features=torch.hstack((P_all_Features,Cfeatures))\n",
        "          x = torch.tanh(self.fc1(PC_Features))#.to('cuda:0')\n",
        "         # x = torch.tanh(self.fc2(x))#.to('cuda:1')\n",
        "          x = torch.tanh(self.fc3(x))#.to('cuda:2')\n",
        "          #x = self.fc5(x)\n",
        "          x = self.fc6(x)\n",
        "          return x\n",
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "#For End to End LEarning\n",
        "import pickle\n",
        "path,githubpath='/content/drive/MyDrive/GNN-PPI-Inhibitor/','/content/PPI-Inhibitors/'\n",
        "Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "Pos_seqandInterfaceF_dict=pickle.load(open(githubpath+'Features/Pos_seqandInterfaceF_dict.npy',\"rb\"))\n",
        "Complex_AllFeatures_dict=dict( list (Pos_seqandInterfaceF_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "##############\n",
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]#.cuda()\n",
        "#################################\n",
        "CompoundFingerprintFeaturesDict=pickle.load(open(githubpath+'Features/Compound_Fingerprint_Features_Dict.npy',\"rb\"))\n",
        "#Load Protein data for GNN\n",
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "DBD5_ProteinDataGNN_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "All_ProteinData_dict=dict( list (ProteinDataGNN_dict.items())+list (DBD5_ProteinDataGNN_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "#########\n",
        "from tqdm import tqdm as tqdm\n",
        "import pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "###########\n",
        "#with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt') as f:\n",
        "with open(githubpath+'Data/WriteAllexamplesRandomBindersIdsAll_24JAN.txt') as f:\n",
        "    D = f.readlines()\n",
        "Labels=[];Ligandnames=[];Complexs=[];TestPoscomplexes=[];#SMILESlist=[];\n",
        "for d in tqdm(D):\n",
        "  if len(d.split())==4:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()\n",
        "  else:\n",
        "      TestPoscomp,Complexname,Ligandname,label = d.split()[0],d.split()[1],(' ').join(d.split()[2:-1]),d.split()[-1]\n",
        "  TestPoscomplexes.append(TestPoscomp),Ligandnames.append(Ligandname);Complexs.append(Complexname);Labels.append(float (label))\n",
        "##\n",
        "#########Make dictionary, Rootcomplexname=(complexname,compoundname),label\n",
        "Allexamples=dict (zip(zip(TestPoscomplexes,zip(Complexs,Ligandnames)),Labels))\n",
        "#Group kfold\n",
        "Alldata=list (Allexamples.keys())\n",
        "KK=[k[0].split('_')[0] for k in Alldata]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "from torchmetrics.classification import BinaryHingeLoss\n",
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "AlltestExamples=[];Externallabels=[];ExternalscoresLOCO=[];covid19_Externallabels=[];covid19_ExternalscoresLOCO=[];Y_score=[];Y_t=[];classratio_dict={};\n",
        "AUC_ROC_final=[];Avg_P_final=[];\n",
        "Complexs,Ligandnames, Labels=np.array(Complexs),np.array(Ligandnames),np.array(Labels)\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    Alldata=np.array(Alldata)\n",
        "    train,test=Alldata[trainindex],Alldata[testindex]\n",
        "    Ctr=[];Ptr=[];y_train=[];Ctrname=[];Ptrname=[];Xtr=[];G=[];Cttname=[];Ctt=[];y_test=[];Ptt=[];Pttname=[];\n",
        "    #Split train and test\n",
        "    for t in train:\n",
        "        Ctrname.append(t[1][1]);Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        #change this only for GNN Complex_AllFeatures_dict with All_ProteinData_dic and t\n",
        "        #####\n",
        "        GNNcomp=t[1][0].split('_')[0]#t[1][0].split('_')[0]\n",
        "        SeqonlyF=ComplexInterfaceFeatures[GNNcomp][0:69]\n",
        "        Ptrname.append(GNNcomp);Ptr.append(SeqonlyF);\n",
        "        #print (ComplexInterfaceFeatures[GNNcomp])\n",
        "        #s,i,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features([GNNcomp],githubpath+'Data/Pdb/')#UniqueProtein,Pdbloc)\n",
        "        #1/0\n",
        "        y_train.append(Allexamples[t[0],t[1]])\n",
        "    #Split train and test\n",
        "    for t in test:\n",
        "        GNNcomp=t[1][0].split('_')[0]\n",
        "        Cttname.append(t[1][1]);Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]]);\n",
        "        SeqonlyF=ComplexInterfaceFeatures[GNNcomp][0:69]\n",
        "        Pttname.append(GNNcomp);Ptt.append(SeqonlyF);#ComplexInterfaceFeatures[GNNcomp]);\n",
        "        y_test.append(Allexamples[t[0],t[1]])\n",
        "    #standarization\n",
        "    Pscaler = StandardScaler().fit(Ptr)\n",
        "    Cscaler = StandardScaler().fit(Ctr)\n",
        "    Ctr = Cscaler.transform(Ctr)\n",
        "    Ptr=Pscaler.transform(Ptr)\n",
        "    Ptt=Pscaler.transform(Ptt)\n",
        "    Ptrdict=dict (zip(Ptrname,torch.FloatTensor(Ptr).cuda()))\n",
        "    Ctrdict=dict (zip (Ctrname,torch.FloatTensor( Ctr).cuda()))\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "    Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "    Pttdict=dict (zip(Pttname,torch.FloatTensor(Ptt).cuda()))\n",
        "    ######################################3\n",
        "    #CompoundNet=Compound_Net().cuda()7\n",
        "    ################7\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #############\n",
        "    GNN_model=GNN().cuda()\n",
        "    ############\n",
        "    \"\"\"\n",
        "    Mcomplexname,Mepoch='3TDU','10'\n",
        "    IPPI_Net.load_state_dict(torch.load(path+'IPPI_Net_'+ Mcomplexname+'_Epoch'+str (Mepoch)))\n",
        "    GNN_model.load_state_dict(torch.load(path+'GNN_model_'+ Mcomplexname+'_Epoch'+str (Mepoch)))\n",
        "    #############\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    #criterion = BinaryHingeLoss().cuda()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.0001,weight_decay=0.0)#0001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \",test[0][0].split('_')[0] )\n",
        "    #Done=['3UVW','2B4J','2RNY','4AJY','1YCR','4QC3','1NW9','2E3K','4GQ6','4YY6']\n",
        "    Done=['4GQ6','1BXL','4ESG','2FLU','1YCQ','2XA0','3D9T','2B4J','2RNY','4AJY', '1YCR','4QC3','1BKD','1NW9','2E3K','4YY6','4GQ6','3UVW','3DAB','3WN7','1F47','3TDU']#,'1Z92'\n",
        "    Batchlosslist=[]\n",
        "    ################load classratio_dict based on rrot complex\n",
        "    classratio_dict=pickle.load(open(githubpath+'Features/Classratio_GNNdict.npy','rb'))\n",
        "    Mepoch=0\n",
        "    if test[0][0].split('_')[0] in Done:\n",
        "      continue\n",
        "    \"\"\"\n",
        "    if test[0][0].split('_')[0]!=Mcomplexname:\n",
        "      continue\n",
        "    \"\"\"\n",
        "    for epoch in range(30):\n",
        "      total_preds = torch.Tensor()\n",
        "      total_labels = torch.Tensor()\n",
        "      Batchloss=0\n",
        "      epoch=epoch+int (Mepoch)\n",
        "      #output=IPPI_Net(train)\n",
        "      for n in range(len(train)):\n",
        "        #print (train[n])\n",
        "        complexname,Ligandname =train[n][1]\n",
        "        GNNcomplex=complexname.split('_')[0]\n",
        "        #output=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],complexname,CompoundNet,CompoundFingerprintFeaturesDict[Ligandname])\n",
        "        #standrized\n",
        "        output=IPPI_Net(All_ProteinData_dict[GNNcomplex],Ctrdict[Ligandname],Ptrdict[GNNcomplex],GNN_model)\n",
        "        #loss = criterion(output, y_train[n].view(-1))AQQAQ\n",
        "        loss=criterion(output, y_train[n].reshape(1))\n",
        "        #print(\"loss\",loss.cpu().data.numpy())\n",
        "        if y_train[n]==1.0:\n",
        "              #print(train[n][0],classratio_dict[train[n][0]])\n",
        "              loss=classratio_dict[GNNcomplex]*loss\n",
        "        Batchloss=Batchloss+loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #params = list (IPPI_Net.parameters()) + list( GNN_model.parameters()) + list (CompoundNet.parameters())\n",
        "        #print(train[n][1],\"loss\",loss.cpu().data.numpy())\n",
        "      #############\n",
        "      Batchlosslist.append(Batchloss.cpu().data.numpy()/len(train))\n",
        "      print(\"Epoch\",epoch,\"Batch loss\",Batchloss.cpu().data.numpy()/len(train))\n",
        "      Y_t,Y_score=[],[]\n",
        "      if epoch%5==0 or Batchloss.cpu().data.numpy()/len(train)<0.1:\n",
        "        #Testing\n",
        "        Y_score,Y_t=[],[]\n",
        "        for nt in range(len(test)):\n",
        "          complexname,Ligandname =test[nt][1]\n",
        "          GNNcomplex=complexname.split('_')[0]\n",
        "          test_score=IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "          Y_score.extend(test_score.cpu().data.numpy())\n",
        "          Y_t.append(y_test[nt])\n",
        "        auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "        average_PR=average_precision_score(Y_t,Y_score)\n",
        "        LOCOcomplexname=test[0][0].split('_')[0]\n",
        "        print(\"LOCOcomplexname\",LOCOcomplexname,\"Epoch\",epoch,\"auc\",\"PR\",auc_roc.round(3),average_PR.round(3),\"loss\",loss.cpu().data.numpy().round(3))\n",
        "        file_model = path+'/IPPI_Net_'+ LOCOcomplexname+'_Epoch'+str (epoch)\n",
        "        torch.save(IPPI_Net.state_dict(), file_model)\n",
        "        file_model = path+'/GNN_model_'+ LOCOcomplexname+'_Epoch'+str (epoch)\n",
        "        torch.save(GNN_model.state_dict(), file_model)\n",
        "        #print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "        #(filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN,LOCOcomplexname)\n",
        "        External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)\n",
        "        ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "        External_Auc= roc_auc_score(External_labels, External_score)\n",
        "        External_AP=average_precision_score(External_labels, External_score)\n",
        "        print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "        #########\\\n",
        "\n",
        "        #covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "        Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "        covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "        Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "        ###\n",
        "        Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "        print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))\n",
        "        if Batchloss.cpu().data.numpy()/len(train)<0.1:\\\n",
        "          break;\n",
        "      ##########\n",
        "    #Testing\n",
        "    Y_score,Y_t=[],[]\n",
        "    for nt in range(len(test)):\n",
        "      complexname,Ligandname =test[nt][1]\n",
        "      GNNcomplex=complexname.split('_')[0]\n",
        "      test_score=IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.append(y_test[nt])\n",
        "    TestComplex=test[0][0]\n",
        "    Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "    Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "    average_P_score=average_precision_score(Y_t,Y_score)\n",
        "    np.save(path+TestComplex+'_Loss.npy''_Epoch'+str (epoch),Batchlosslist)\n",
        "    #np.save (path+TestComplex+'_AUC_list.npy''_Epoch'+str (epoch),AUC_list)\n",
        "    #np.save (path+ TestComplex+'_PR_list.npy''_Epoch'+str (epoch),PR_list)\n",
        "    ####\n",
        "    file_model = path+'/IPPI_Net_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(IPPI_Net.state_dict(), file_model)\n",
        "    file_model = path+'/GNN_model_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(GNN_model.state_dict(), file_model)\n",
        "    Y_t=np.array(Y_t)\n",
        "    #print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "    print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "    #####################External\n",
        "    Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)\n",
        "    #Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "    #print (\"RFPP\",RFPP_all)\n",
        "    ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "    External_Auc= roc_auc_score(External_labels, External_score)\n",
        "    External_AP=average_precision_score(External_labels, External_score)\n",
        "    print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "    #########\\\n",
        "    #covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "    Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "    covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "    Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "    ###\n",
        "    Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "    print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")\n",
        "##########\n",
        "External_fpr, External_tpr, External_thresholds = roc_curve(Externallabels,ExternalscoresLOCO)\n",
        "External_Auc = roc_auc_score(Externallabels,ExternalscoresLOCO)\n",
        "External_Auc=(External_Auc).round(2)\n",
        "fig = plt.figure()\n",
        "plt.plot(External_fpr, External_tpr,color='k',marker='d',label='External_Auc:{: .2f}'.format(External_Auc))\n",
        "plt.title('AUCROC External');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC External SVM PPI Inhibitors Random and 2 Times Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "#######\n",
        "covid19_External_fpr, covid19_External_tpr, covid19_External_thresholds = roc_curve(covid19_Externallabels,covid19_ExternalscoresLOCO)\n",
        "covid19_External_Auc = roc_auc_score(covid19_Externallabels,covid19_ExternalscoresLOCO)\n",
        "covid19_External_Auc=(covid19_External_Auc).round(2)\n",
        "fig = plt.figure()\n",
        "plt.plot(covid19_External_fpr, covid19_External_tpr,color='k',marker='d',label='covid19_External_Auc:{: .2f}'.format(covid19_External_Auc))\n",
        "plt.title('covid19 AUCROC External');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC covid19 External SVM PPI Inhibitors Random and 2 Times Binders combine Negative.pdf\", bbox_inches='tight')\n",
        "########\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "print(\"averge score of test\",np.average(Tscores_list))\n",
        "\"\"\"\n",
        "np.save(path+'All_loss.npy',All_loss)\n",
        "np.save (path+ 'AUC_list.npy',AUC_list)\n",
        "np.save (path+ 'PR_list.npy',PR_list)\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss over all complexes 30 epochs.pdf\", bbox_inches='tight')\n",
        "############\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.plot( AUC_list,color='b',marker=',',label='AUC')\n",
        "plt.plot( PR_list,color='m',marker=',',label='PR')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss-PR over all complexes.pdf\", bbox_inches='tight')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFCNvL4yv-Ao"
      },
      "outputs": [],
      "source": [
        "External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model,LOCOcomplexname)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model,LOCOcomplexname)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86CYhVz7KSNb"
      },
      "outputs": [],
      "source": [
        "seqF,Interface,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features(GNNcomp,githubpath+'Data/Pdb/')#UniqueProtein,Pdbloc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5MqZyHbKata"
      },
      "outputs": [],
      "source": [
        "LoadProtein_SVM_Features(GNNcomp,githubpath+'Data/Pdb/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EllhLn9m303h"
      },
      "outputs": [],
      "source": [
        "#def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "UniqueProtein,Pdbloc=[GNNcomp],githubpath+'Data/Pdb/'\n",
        "pdbname=listdir(Pdbloc)\n",
        "InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "for  b in range(len(UniqueProtein)):\n",
        "    if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "        stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'#\n",
        "        chains=Struct2chain(stx)\n",
        "        #########Interface Features\n",
        "        for j in range(len(chains)):\n",
        "            Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "            for k in range(j,len(chains)):\n",
        "                Cname,seq,L,xl=chains[k]\n",
        "                #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                seq_TF=prot_feats_seq(seq_T)\n",
        "                seq_NTF=prot_feats_seq(seq)\n",
        "                SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                InterfaceF=np.array(Interface)\n",
        "                InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                if name not in InterfaceFeatures.keys():\n",
        "                    InterfaceFeatures[name]=Interface\n",
        "                    SequenceFeatures[name]=SeQFeatures\n",
        "                    AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    #return InterfaceFeatures,SequenceFeatures,AllFeatures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_PTn1pX1rzg"
      },
      "outputs": [],
      "source": [
        "len(SequenceFeatures['3UVW'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT5FHagA2KYv"
      },
      "outputs": [],
      "source": [
        "t.split('_')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alZXhEtIsCSZ"
      },
      "outputs": [],
      "source": [
        "ComplexInterfaceFeatures={}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "  if len(key.split('_'))>1:\n",
        "    compname=key.split('_')[0]\n",
        "    ComplexInterfaceFeatures[compname]=Complex_AllFeatures_dict[key]\n",
        "  else:\n",
        "    ComplexInterfaceFeatures[key]=Complex_AllFeatures_dict[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0hKsuZquY-F"
      },
      "outputs": [],
      "source": [
        "Ptrdict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3JfpKx7rDTR"
      },
      "outputs": [],
      "source": [
        "#####Testing\n",
        "Y_score,Y_t=[],[]\n",
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],GNN_model)\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "TestComplex=test[0][0]\n",
        "Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_P_score=average_precision_score(Y_t,Y_score)\n",
        "TestComplex=test[0][0].split('_')[0]\n",
        "file_model = path+'/IPPI_Net_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(IPPI_Net.state_dict(), file_model)\n",
        "file_model = path+'/GNN_model_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(GNN_model.state_dict(), file_model)\n",
        "Y_t=np.array(Y_t)\n",
        "#print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Nvo-7PebeZ"
      },
      "outputs": [],
      "source": [
        "file_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCu_Kaud8j1N"
      },
      "outputs": [],
      "source": [
        "#####################External\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqGRygHxEOy8"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname])\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "TestComplex=test[0][0]\n",
        "Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_P_score=average_precision_score(Y_t,Y_score)\n",
        "np.save(path+TestComplex+'_Loss.npy''_Epoch'+str (epoch),Batchlosslist)\n",
        "#np.save (path+TestComplex+'_AUC_list.npy''_Epoch'+str (epoch),AUC_list)\n",
        "#np.save (path+ TestComplex+'_PR_list.npy''_Epoch'+str (epoch),PR_list)\n",
        "####\n",
        "file_model = path+'/IPPI_Net_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(IPPI_Net.state_dict(), file_model)\n",
        "file_model = path+'/GNN_model_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "torch.save(GNN_model.state_dict(), file_model)\n",
        "Y_t=np.array(Y_t)\n",
        "#print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####################External\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")\n",
        "##########"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xT79QXu2MhLX"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZQ9Ua7UQB4i"
      },
      "outputs": [],
      "source": [
        "classratio_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CQSUp-SKNwp"
      },
      "outputs": [],
      "source": [
        "classratio_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1HTPrWNvT-n"
      },
      "outputs": [],
      "source": [
        "plt.plot(Batchlosslist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SbBr9HHfpek"
      },
      "outputs": [],
      "source": [
        "AUC_ROC_final=[];Avg_P_final=[];Z=[];Yo=[];Y_t=[];Y_score=[];\n",
        "from os import listdir\n",
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],complexname,CompoundNet,Cttdict[Ligandname])\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "TestComplex=test[0][0]\n",
        "Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_P_score=average_precision_score(Y_t,Y_score)\n",
        "Y_t=np.array(Y_t)\n",
        "#print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####################External\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,CompoundNet,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,CompoundNet,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7T1TKsxTpx3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiIWaAftSZfc"
      },
      "outputs": [],
      "source": [
        "loss = criterion(output, y_train[n].reshape(1)[:,]>0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7acFDtz-J8V"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkqv80gKSITV"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cSPgJA_RJAh"
      },
      "outputs": [],
      "source": [
        "y_train[n].reshape(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlXAJSuSTNLB"
      },
      "outputs": [],
      "source": [
        "train[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mC-I6xNQXua"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phtDWlTwP8E3"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-CSPe-EP-wd"
      },
      "outputs": [],
      "source": [
        "y_train[n].view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SEGfRHfa6ErJ"
      },
      "outputs": [],
      "source": [
        "y_train[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1w3BvZK6PMs"
      },
      "outputs": [],
      "source": [
        "y_train[n-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqCGBnCpMfTz"
      },
      "outputs": [],
      "source": [
        "Batchloss=Batchloss+loss.cpu().data.numpy()\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "#params = list (IPPI_Net.parameters()) + list( GNN_model.parameters()) + list (CompoundNet.parameters())\n",
        "print(\"Epoch\",epoch,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzcKx9mJC0KR"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvptbH8vAMPp"
      },
      "outputs": [],
      "source": [
        "loss.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlThYaMgC806"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCDE8O4BAP3O"
      },
      "outputs": [],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7h6lFd4B6gj"
      },
      "outputs": [],
      "source": [
        "target=y_train[n].reshape(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3StYc19dvjO"
      },
      "outputs": [],
      "source": [
        "loss = criterion(total_preds, total_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ij9ganBBLmCE"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkKLFRSMd18j"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.classification import BinaryHingeLoss\n",
        "bhl = BinaryHingeLoss()\n",
        "loss=bhl(total_preds,total_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVX4BOTIf6-t"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-T5WjSOdWaH"
      },
      "outputs": [],
      "source": [
        "total_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmOoli43dZ9t"
      },
      "outputs": [],
      "source": [
        "total_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtSxXjU2UPU7"
      },
      "outputs": [],
      "source": [
        "total_labels[:,0]>0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv0je1kzEiud"
      },
      "outputs": [],
      "source": [
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)\n",
        "#Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "#print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR\",round (External_Auc,3), round (External_AP,3))#,RFPP_all,\"\\n\")\n",
        "#########\\\n",
        "#covid19dict,Covid19_scores,Covid19_labels,Covid19_RFPP_all=PredictRFPPfromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3))#,Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlvBSu7wFPG4"
      },
      "outputs": [],
      "source": [
        "Resultdict_External,Covid19_External_score,Covid19_External_labels=PredictScorefromFile(githubpath+'/Data/External data/HansonACE2hits.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvS2DIUjDezY"
      },
      "outputs": [],
      "source": [
        "Resultdict,External_score,External_labels=PredictScorefromFile(githubpath+'/Data/External data/2dyh_all.txt',githubpath+'/Data/External data/pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsWhD0OBk0d-"
      },
      "outputs": [],
      "source": [
        "ProteinDataGNN_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYHhMYlua_Re"
      },
      "outputs": [],
      "source": [
        "CompoundFingerprintFeaturesDict[Ligandname]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BBzoOFEcHzO"
      },
      "outputs": [],
      "source": [
        "Ligandname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d91rJeHlYBsg"
      },
      "outputs": [],
      "source": [
        "All_ProteinData_dict[GNNcomplex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO4TS0sPUGQ2"
      },
      "outputs": [],
      "source": [
        "CompoundFingerprintFeaturesDict[Ligandname]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23VmclR6cbyI"
      },
      "outputs": [],
      "source": [
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt][1]\n",
        "  GNNcomplex=complexname.split('_')[0]\n",
        "  test_score=IPPI_Net(GNN_model,All_ProteinData_dict[GNNcomplex],complexname,CompoundFingerprintFeaturesDict[Ligandname])\n",
        "  Y_score.append(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "average_PR=average_precision_score(Y_t,Y_score)\n",
        "print(\"Epoch\",epoch,\"auc\",\"PR\",auc_roc.round(3),average_PR.round(3),\"loss\",loss.cpu().data.numpy().round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RH--Lt4bCln"
      },
      "outputs": [],
      "source": [
        "Covid19_RFPP_all=PredictRFPPfromFile(path+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)#path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Covid19_External_score,Covid19_External_labels=PredictScorefromFile(path+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)#,path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "###\n",
        "Covid19_External_AP=average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "print(\"Covid19_External_Auc,PR,RFPP\",round (Covid19_External_Auc,3), round (Covid19_External_AP,3),Covid19_RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyMmWZ7rcZa7"
      },
      "outputs": [],
      "source": [
        "PredictScorefromFile(path+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcJvvm7bRUkK"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "#Result=PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "Pdbloc=Externalpath+'pdb/'\n",
        "#trainedModel_IPPI,train_GNN=IPPI_Net,GNN_model\n",
        "with open(Externalpath+'2dyh_all.txt') as f:\n",
        "  D = f.readlines()\n",
        "InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];labels=[];\n",
        "All_data_list=[]\n",
        "from tqdm import tqdm as tqdm\n",
        "#2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "for d in tqdm(D):\n",
        "    #if len(d)==6:\n",
        "    Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "    PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "################\n",
        "pdbname=listdir(Pdbloc)\n",
        "mypdb=[]\n",
        "for p in pdbname:\n",
        "  if p.split('.pdb')[0] in Pdbid:\n",
        "    mypdb.append(p)\n",
        "UniqueProtein=list (set (mypdb))\n",
        "pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "Pos,Negs,labels=External_GenerateRandomNegative(pos)\n",
        "1/0\n",
        "#Negs=External_GenerateRandomNegative(pos)\n",
        "#omplexnames=list (complex_ligand_dict.keys())\n",
        "AllNeg=[];AllPos=[];\n",
        "totalcomp=list(set (np.array([([t][0]) for t in pos])))\n",
        "External_ProteinData_dict=PrepairDataset.processProtein(UniqueProtein,Pdbloc)\n",
        "Result_dict={}\n",
        "#Testing\n",
        "Y_t,Z=[],[]\n",
        "for nt in range(len(InhibitedComp)):\n",
        "  complexname,Ligandname, ligandsmile,inhibitC =PdbId[nt], Ligandnames[nt], SMILES[nt],InhibitedComp[nt]\n",
        "  test_score=trainedModel_IPPI(train_GNN,External_ProteinData_dict[complexname],complexname,ligandsmile)\n",
        "  if test_score!=0.0:\n",
        "      test_score=test_score.cpu().data.numpy()[0]\n",
        "      Z.append(test_score)\n",
        "      Result_dict[(complexname,Ligandname)]=(inhibitC,test_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoKBzkYCilus"
      },
      "outputs": [],
      "source": [
        "    posexamples=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank\n",
        "    ###Names\n",
        "    SuperdrugNames=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############\n",
        "    ###SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];\n",
        "    complex_ligand_dict={}\n",
        "    for key,val in  posexamples:\n",
        "      print(key,val,posexamples[key,val][1])\n",
        "      if key not in complex_ligand_dict:\n",
        "        complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "      else:\n",
        "        #print(\"else\",key,val)\n",
        "        complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    #totalligands_train=list (set (totalligands_train))\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "    print(\"N=\",len(AllNeg),\"P\",len(AllPos))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXEY-sbVluVB"
      },
      "outputs": [],
      "source": [
        "Covid19_RFPP_all=PredictRFPPfromFile(path,path+'/HansonACE2hits.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ctr,Ptr)\n",
        "Covid19_External_score,Covid19_External_labels=PredictScorefromFileSVM(path,path+'/HansonACE2hits.txt',path+'/External data for Inhibitors/protein pdb/',clf,Ptr,Ctr,Pscaler, Cscaler)#(filename,Pdbloc,trainedModel_SVM,Ptr,Ctr)\n",
        "covid19_Externallabels.extend(Covid19_External_labels);covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "Covid19_External_Auc= roc_auc_score(Covid19_External_labels, Covid19_External_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1Vwwg-9jpfr"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "Resultdict,External_score,External_labels=PredictScorefromFile(Externalpath+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'HansonACE2hits.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "print (\"RFPP\",RFPP_all)\n",
        "ExternalscoresLOCO.extend(External_score);Externallabels.extend(External_labels)\n",
        "External_Auc= roc_auc_score(External_labels, External_score)\n",
        "External_AP=average_precision_score(External_labels, External_score)\n",
        "print(\"External_Auc,PR,RFPP\",round (External_Auc,3), round (External_AP,3),RFPP_all,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLfpMbtJe41B"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "Resultdict,Tscores,Tlabels=PredictScorefromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "Resultdict,Tscores,Tlabels,RFPP_all=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "print (\"RFPP\",RFPP_all)\n",
        "T_Auc=roc_auc_score(np.array(Tlabels), np.array(Tscores))#Tscores,Tlabels\n",
        "T_PR=average_precision_score(np.array(Tlabels), np.array(Tscores))\n",
        "print (T_Auc,T_PR)\n",
        "Tscores_list.append(Tscores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrLma57AbAVI"
      },
      "outputs": [],
      "source": [
        "PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80_vkg5NbjDa"
      },
      "outputs": [],
      "source": [
        "GNN_model(All_ProteinData_dict[complexname])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8wjHfblXoJ4"
      },
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkc3FiVMJv-Q"
      },
      "outputs": [],
      "source": [
        "len(Tscores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-kK_-O0sl56"
      },
      "outputs": [],
      "source": [
        "plt.plot(Tscores,color='r')\n",
        "plt.plot(Tlabels,color='b')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYnw56aPAckO"
      },
      "outputs": [],
      "source": [
        "complexname,Ligandname =test[nt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRqKHtk4XeIt"
      },
      "outputs": [],
      "source": [
        "complexname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icgyydgs_4Oe"
      },
      "outputs": [],
      "source": [
        "All_examples[complexname,ligandname][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsKQceM2F_U5"
      },
      "outputs": [],
      "source": [
        "All_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yirvEdXkEOfe"
      },
      "outputs": [],
      "source": [
        "Superdrug_SMILES#Features all unique Compounds of Superdrugbank and truepositive examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKGyWzt7EHp3"
      },
      "outputs": [],
      "source": [
        "Actual_Compound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-g-RTn-D5aO"
      },
      "outputs": [],
      "source": [
        "Pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Rd5miiCedR"
      },
      "outputs": [],
      "source": [
        "Actual_Compound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4kgn1MjCfIU"
      },
      "outputs": [],
      "source": [
        "\n",
        "##########loop\n",
        "RFPP_all=[]\n",
        "perntile_values = [0,1,5,10,20,40,50,60,70,80,90,95,99,100]#99\n",
        "for Pi_index in range(len(P2C_dict)):\n",
        "  ####For one example Pi\n",
        "  Pi=P2C_dict[Pi_index ]#pick one protein P index\n",
        "  Pi_Feature=External_ProteinData_dict[Pi]#P features of that index\n",
        "  Actual_Compound=P2C_dict[Pi]#Actul_Compound paired with Pi\n",
        "  #Actual_Compound_Features=np.array([U[c] for c in Actual_Compound])\n",
        "  Ctt=np.vstack((Actual_Compound,Superdrug_SMILES))#Features all unique Compounds of Superdrugbank and truepositive examples\n",
        "  Ptt=np.array([Pi for i in range(len(Ctt))])#Copy same feature of protein equal to number of unique compounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuTEKRM1CTWq"
      },
      "outputs": [],
      "source": [
        "P2C_dict[Pi_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBUsudDUviNZ"
      },
      "outputs": [],
      "source": [
        "Resultdict,Tscores,Tlabels,RFPP_list=PredictRFPPfromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRFllVhddGFE"
      },
      "outputs": [],
      "source": [
        "T_Auc=roc_auc_score(np.array(Tlabels), np.array(Tscores))#Tscores,Tlabels\n",
        "T_PR=average_precision_score(np.array(Tlabels), np.array(Tscores))\n",
        "print (T_Auc,T_PR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx9mRKr8dfXr"
      },
      "outputs": [],
      "source": [
        "Tscores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoKQCOHdbAvm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score,roc_curve,precision_score,recall_score,average_precision_score\n",
        "#For End to End LEarning\n",
        "\"\"\"\n",
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "#########\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "\"\"\"\n",
        "pos_Pid,Ligandnames,pos_smiles,pos_label=readFile('/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt')\n",
        "###\n",
        "\"\"\"\n",
        "UniqueProtein=list (set (pos_Pid))\n",
        "ProteinData_dict=PrepairDataset.processProtein(UniqueProtein)##PdBloc)\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "# save dictionary to pickle file\n",
        "with open(path+'ProteinData_dict.pickle', 'wb') as filename:\n",
        "    pickle.dump(ProteinData_dict, filename, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\"\"\"\n",
        "#pickle.dump(path+'ProteinData_dict.npy',ProteinData_dict)\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "import pickle\n",
        "ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "import pandas as pd\n",
        "fields=['(Complexname)','Binders SMILES']\n",
        "df=pd.read_csv('/content/PPI-Inhibitors/Data/BindersWithComplexname.csv', skipinitialspace=True, usecols=fields)\n",
        "# See the keys\n",
        "#print(df.keys())\n",
        "neg_Pidname,neg_smiles=df[df.keys()[0]].values,df[df.keys()[1]].values\n",
        "\n",
        "neg_Pid=[n.split('_')[0] for n in neg_Pidname]\\\n",
        "\n",
        "New_binders_dict=AppendlistinDict(neg_Pid,neg_smiles)\n",
        "####\n",
        "neg_label=-1.0*(np.ones(len(neg_smiles)))\n",
        "#####\n",
        "Z = []; Yo = []; A = [];Yp=[]\n",
        "AUC_ROC_final=[];Precision_final=[];Recall_final=[];Avg_P_final=[];\n",
        "####\n",
        "comp=dict(zip(zip (pos_Pid,Ligandnames),zip(pos_Pid,pos_smiles)))\n",
        "#####\n",
        "complex_ligand_dict=AppendlistinDict(pos_Pid,Ligandnames)\n",
        "Ldict=dict (zip(Ligandnames,pos_smiles))\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[]; PR_list=[];All_loss=[]\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    trainSMILES=[Ldict[t[1]] for t in trainPos]\n",
        "    traincomp=[t[0] for t in trainPos]\n",
        "    trainPos=list (zip(traincomp,trainSMILES))\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    poslabel=1.0*np.ones(len(trainPos));neglabel=-1.0*np.ones(len(trainNeg))\n",
        "    y_train=np.append(poslabel,neglabel )\n",
        "    ###########\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    testSMILES=[Ldict[t[1]] for t in testPos]\n",
        "    testcomp=[t[0] for t in testPos]\n",
        "    testPos=list (zip(testcomp,testSMILES))\n",
        "    poslabel=1.0*np.ones(len(testPos));neglabel=-1.0*np.ones(len(testNeg))\n",
        "    y_test=np.append(poslabel,neglabel )\n",
        "    test=np.vstack((testPos,testNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "    ######################################3\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #############\n",
        "    GNN_model=GNN().cuda()\n",
        "    #####\n",
        "    epoch='29'\n",
        "    #Done=['1BKD','3D9T','1YCQ','2FLU','4QC3','2B4J','3UVW']\n",
        "    Done=['3UVW','2B4J','2RNY','4AJY','1YCR','4QC3','1NW9','2E3K','4GQ6','4YY6','1BXL','4ESG','3DAB','2FLU','1YCQ','2XA0','1Z92', '1F47','3TDU','3D9T','1BKD','3WN7']#\n",
        "    complexnameT=test[0][0]\n",
        "    #print (\"test complex \",complexnameT)\n",
        "    for d in Done:\n",
        "      if complexnameT==d:\n",
        "        IPPI_Net.load_state_dict(torch.load(path+'IPPI_Net_'+ complexnameT+'_Epoch'+str (epoch)))\n",
        "        GNN_model.load_state_dict(torch.load(path+'GNN_model_'+ complexnameT+'_Epoch'+str (epoch)))\n",
        "        #Testing\n",
        "        Y_t,Y_score=[],[]\n",
        "        for nt in range(len(test)):\n",
        "          complexname,Ligandname =test[nt]\n",
        "          #print(\"n\",n)\n",
        "          test_score=IPPI_Net(GNN_model,ProteinData_dict[complexname],complexname,Ligandname)\n",
        "          if test_score!=0.0:\n",
        "              Y_score.extend(test_score.cpu().data.numpy())\n",
        "              Y_t.append(y_test[nt])\n",
        "        Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "        Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "        average_P_score=average_precision_score(Y_t,Y_score)\n",
        "        LP=np.load(path+complexnameT+'_Loss.npy''_Epoch'+str (epoch)+'.npy')\n",
        "        AUC_epoch=np.load(path+ complexnameT+'_AUC_list.npy''_Epoch'+str (epoch)+'.npy')\n",
        "        PR_epoch=np.load (path+ complexnameT+'_PR_list.npy''_Epoch'+str (epoch)+'.npy')\n",
        "        All_loss.extend(LP);AUC_list.extend(AUC_epoch); PR_list.extend(PR_epoch)\n",
        "        ######\n",
        "        fig=plt.figure()\n",
        "        plt.plot(LP,color='k',marker=',',label='Loss_'+complexnameT)\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        fig .savefig(path+complexnameT+\"_Loss.pdf\", bbox_inches='tight')\n",
        "        ########\n",
        "        Y_t=np.array(Y_t)\n",
        "        #print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "        print(complexnameT,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1))#,\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "        AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "        #####External\n",
        "        Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "        #Resultdict,Tscores=PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "        Resultdict,Tscores=PredictScorefromFile(Externalpath+'2dyh_all.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "        print(\"Tscores\",Tscores)\n",
        "        1/0\n",
        "        #import pdb;pdb.set_trace()\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr Over all complexes \\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])))#,\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUCROC PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "\n",
        "np.save(path+'All_loss.npy',All_loss)\n",
        "np.save (path+ 'AUC_list.npy',AUC_list)\n",
        "np.save (path+ 'PR_list.npy',PR_list)\n",
        "####\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss over all complexes 30 epochs.pdf\", bbox_inches='tight')\n",
        "############\n",
        "fig=plt.figure()\n",
        "plt.plot(All_loss,color='k',marker=',',label='Loss')\n",
        "plt.plot( AUC_list,color='b',marker=',',label='AUC')\n",
        "plt.plot( PR_list,color='m',marker=',',label='PR')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "fig .savefig(path+\"PPI Inhibitors loss-PR over all complexes.pdf\", bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aUb8qPAKwnH"
      },
      "outputs": [],
      "source": [
        "Externalpath='/content/PPI-Inhibitors/Data/External data/'\n",
        "Result=PredictScorefromFile(Externalpath+'External1.txt',Externalpath+'pdb/',IPPI_Net,GNN_model)\n",
        "print(Result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FEIviWOTlUN"
      },
      "outputs": [],
      "source": [
        "#For End to End LEarning\n",
        "\"\"\"\n",
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "#########\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "\"\"\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "#DBD5_ProteinData_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "\"\"\"\n",
        "All_ProteinData_dict=dict( list (Pos_ProteinData_dict.items())+list (DBD5_ProteinData_dict.items()))\n",
        "for d in All_ProteinData_dict:\n",
        "  data=All_ProteinData_dict[d]\n",
        "  All_ProteinData_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "\"\"\"\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[]; PR_list=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateRandomNegative(trainindex,CC,complex_ligand_dict,Ldict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    train_SMILES=[Ldict[t[1]] for t in train]\n",
        "    traincomp=[t[0] for t in train]\n",
        "    train=list (zip(traincomp,train_SMILES))\n",
        "    poslabel=1.0*np.ones(len(trainPos));neglabel=-1.0*np.ones(len(trainNeg))\n",
        "    y_train=np.append(poslabel,neglabel )\n",
        "    ###########\n",
        "    testPos,testNeg=GenerateRandomNegative(testindex,CC,complex_ligand_dict,Ldict)\n",
        "    test=np.vstack((testPos,testNeg))\n",
        "    testSMILES=[Ldict[t[1]] for t in test]\n",
        "    testcomp=[t[0] for t in test]\n",
        "    test=list (zip(testcomp,testSMILES))\n",
        "    poslabel=1.0*np.ones(len(testPos));neglabel=-1.0*np.ones(len(testNeg))\n",
        "    y_test=np.append(poslabel,neglabel )\n",
        "    #All examples passed for negative examples generation\n",
        "    ######################################3\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #############\n",
        "    GNN_model=GNN().cuda()\n",
        "    #####\n",
        "    \"\"\"\n",
        "    complexname,epoch='3UVW','0'\n",
        "    IPPI_Net.load_state_dict(torch.load(path+'IPPI_Net_'+ complexname+'_Epoch'+str (epoch)))\n",
        "    GNN_model.load_state_dict(torch.load(path+'GNN_model_'+ complexname+'_Epoch'+str (epoch)))\n",
        "    \"\"\"\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(list (IPPI_Net.parameters()) + list( GNN_model.parameters()),lr=0.01,weight_decay=0.0001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    ####\n",
        "    print (\"test complex \",testPos[0][0],\"test complex[-1]\",testPos[-1][0])\n",
        "    #Done=['3UVW','2B4J','2RNY','4AJY','1YCR','4QC3','1NW9','2E3K','4GQ6','4YY6']\n",
        "    #3UVW 2B4J 2RNY 4QC3 1YCR 4AJY 1NW9 4GQ6 2E3K 4YY6 1BXL 4ESG 2FLU 1YCQ 3D9T 2XA0 3WN7 1Z92 3DAB 3TDU 1F47 1BKD\n",
        "    Done=['4QC3','1NW9','2E3K','4GQ6','3WN7','1YCQ','2XA0','1Z92', '1F47','4ESG','1BKD','4YY6','3D9T','3DAB','1BXL','2FLU','3TDU']#'3UVW','2B4J','2RNY','4AJY','1YCR',\n",
        "    if test[0][0] in Done:\n",
        "      continue\n",
        "    for epoch in range(100):\n",
        "      total_preds = torch.Tensor()\n",
        "      total_labels = torch.Tensor()\n",
        "      for n in range(len(train)):\n",
        "        complexname,Ligandname =train[n]\n",
        "        #if complexname=='3UVW':\n",
        "          #continue\n",
        "        #print(\"n\",n)\n",
        "        output=IPPI_Net(GNN_model,All_ProteinData_dict,complexname,Ligandname)\n",
        "        #print(\"output\",output)\n",
        "        if output!=0.0:\n",
        "          total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
        "          total_labels = torch.cat((total_labels, y_train[n].view(-1, 1).cpu()), 0)\n",
        "      #output_list=torch.FloatTensor(output_list,requires_grad=True).cuda\n",
        "      #target=torch.FloatTensor(target).cuda\n",
        "      loss = criterion(total_preds, total_labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      print(\"Epoch\",epoch,\"loss\",loss.cpu().data.numpy())\n",
        "      #############\n",
        "      Y_t,Y_score=[],[]\n",
        "      if epoch%3==0:\n",
        "        for nt in range(len(test)):\n",
        "          complexname,Ligandname =test[nt]\n",
        "          test_score=IPPI_Net(GNN_model,All_ProteinData_dict,complexname,Ligandname)\n",
        "          #print(\"nt\",nt,\"test_score\",test_score)\n",
        "          if test_score!=0.0:\n",
        "            Y_score.append(test_score.cpu().data.numpy())\n",
        "            Y_t.append(y_test[nt])\n",
        "        auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "        average_PR=average_precision_score(Y_t,Y_score)\n",
        "        print(\"test comp\",testPos[0][0],\"Epoch\",epoch,\"auc\",\"PR\",auc_roc.round(3),average_PR.round(3),\"loss\",loss.cpu().data.numpy().round(3))\n",
        "  #            print(\"Fauc_roc\",auc_roc)\n",
        "        AUC_list.append(auc_roc);PR_list.append(average_PR)\n",
        "        Y_t,Y_score=[],[]\n",
        "        LP.append(loss.cpu().data.numpy())\n",
        "    #if (loss.cpu().data.numpy()<0.22):\n",
        "      #break;\n",
        "    #Testing\n",
        "    for nt in range(len(test)):\n",
        "      complexname,Ligandname =test[nt]\n",
        "      #print(\"n\",n)\n",
        "      test_score=IPPI_Net(GNN_model,All_ProteinData_dict,complexname,Ligandname)\n",
        "      if test_score!=0.0:\n",
        "          Y_score.extend(test_score.cpu().data.numpy())\n",
        "          Y_t.append(y_test[nt])\n",
        "\n",
        "    print (complexname,\"test complex \",testPos[0][0],testPos[-1][0])\n",
        "    TestComplex=testPos[0][0]\n",
        "    Z=np.append(Z,Y_score);Yo=np.append(Yo,Y_t);\n",
        "    Auc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "    average_P_score=average_precision_score(Y_t,Y_score)\n",
        "    np.save(path+TestComplex+'_Loss.npy''_Epoch'+str (epoch),LP)\n",
        "    np.save (path+ TestComplex+'_AUC_list.npy''_Epoch'+str (epoch),AUC_list)\n",
        "    np.save (path+ TestComplex+'_PR_list.npy''_Epoch'+str (epoch),PR_list)\n",
        "    ####\n",
        "    file_model = path+'/IPPI_Net_Random_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(IPPI_Net.state_dict(), file_model)\n",
        "    file_model = path+'/GNN_model_Random_'+ TestComplex+'_Epoch'+str (epoch)\n",
        "    torch.save(GNN_model.state_dict(), file_model)\n",
        "    Y_t=np.array(Y_t)\n",
        "    #print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",complexname,\"Final AUC\",Auc,\"Final PR\",average_P_score)\n",
        "    print(TestComplex,\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",np.sum(Y_t[Y_t==1.0]),\"\\t\",np.abs(np.sum(Y_t[Y_t==-1.0])),\"\\t\",round (np.abs(np.sum(Y_t[Y_t==-1.0]))/np.sum(Y_t[Y_t==1.0]),1),\"\\n\")#round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "np.save(path+'Loss.npy',LP)\n",
        "np.save (path+ 'AUC_listEpoch5.npy',AUC_list)\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGgOYmALVz87"
      },
      "outputs": [],
      "source": [
        "Y_t=np.array(Y_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV16DB36e1b2"
      },
      "outputs": [],
      "source": [
        "LP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGsrZRE8Wgbc"
      },
      "outputs": [],
      "source": [
        " for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  print(\"nt\",nt,\"test_score\",test_score)\n",
        "  if test_score!=0.0:\n",
        "    Y_score.append(test_score.cpu().data.numpy())\n",
        "    Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqmWn8j5Qfz-"
      },
      "outputs": [],
      "source": [
        " for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  print(\"nt\",nt,\"test_score\",test_score)\n",
        "  if output!=0.0:\n",
        "    Y_score.append(test_score.cpu().data.numpy())\n",
        "    Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u11BxFkpPorl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiQb0RWpLRZl"
      },
      "outputs": [],
      "source": [
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  print(\"n\",n)\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  print(\"test_score\",test_score)\n",
        "  Y_score.append(test_score.cpu().data.numpy())\n",
        "  Y_t.append(y_test[nt])\n",
        "auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPUhwU8nMJiq"
      },
      "outputs": [],
      "source": [
        "test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4nGcD1FMNpf"
      },
      "outputs": [],
      "source": [
        "test_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2VPZxDcLbw7"
      },
      "outputs": [],
      "source": [
        "Y_t.append(y_test[nt])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpw-ZdGJGw6M"
      },
      "outputs": [],
      "source": [
        "for nt in range(len(test)):\n",
        "  complexname,Ligandname =test[nt]\n",
        "  #print(\"n\",n)\n",
        "  test_score=IPPI_Net(GNN_model,ProteinData_dict,complexname,Ligandname)\n",
        "  Y_score.extend(test_score.cpu().data.numpy())\n",
        "  Y_t.extend(y_test[nt].cpu().data.numpy())\n",
        "  auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "  print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "  AUC_list.append(auc_roc)\n",
        "  Y_t,Y_score=[],[]\n",
        "  LP.append(loss.cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rsg8kXhPFRzj"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rguvzuf2FUir"
      },
      "outputs": [],
      "source": [
        "total_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHJWOcomEVTn"
      },
      "outputs": [],
      "source": [
        "torch.cat(output_list,output,dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t3czIdXEbhV"
      },
      "outputs": [],
      "source": [
        "output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wljkK-gJ-VDS"
      },
      "outputs": [],
      "source": [
        "output_list=torch.FloatTensor(output_list).cuda\n",
        "target=torch.FloatTensor(target).cuda\n",
        "loss = criterion(output_list, y_train)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "params = list(IPPI_Net.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44sJjsQA-b3k"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mf424Jc8hCu"
      },
      "outputs": [],
      "source": [
        "output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW-UCyq98lK9"
      },
      "outputs": [],
      "source": [
        "output.cpu().detach().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8mAzmS_12n9"
      },
      "outputs": [],
      "source": [
        "trainPos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vduv6hc616mV"
      },
      "outputs": [],
      "source": [
        "getFP(Ligandname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ1017d21ZVe"
      },
      "outputs": [],
      "source": [
        "output_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5LelXbrBT17"
      },
      "outputs": [],
      "source": [
        "output_list=torch.FloatTensor(output_list).cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBpdCfrFxbap"
      },
      "outputs": [],
      "source": [
        "loss = criterion(output_list, y_train[0:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6mYOFxxxDVZ"
      },
      "outputs": [],
      "source": [
        " y_train=np.append(1.0*np.ones(len(trainPos)),-1.0*np.ones(len(trainNeg)))\n",
        " y_train=torch.FloatTensor( y_train).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw-jVFOxvC3m"
      },
      "outputs": [],
      "source": [
        " leny_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5rlhqTWyH0p"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwEglSceYhRu"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "\n",
        "    1/0\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      test_score.tolist()\n",
        "      y_test.tolist()\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEgZlfX3Tk5l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KVf0KqsTXsg"
      },
      "outputs": [],
      "source": [
        "testNeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3v-ZqVQ8dLy"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if loss<0.1:\n",
        "        break;\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      test_score.tolist()\n",
        "      y_test.tolist()\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faFfBkgmARO3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTk-3ha-ARny"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "    Max_auc_roc=0.0\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      #if loss<0.15:\n",
        "        #break;\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      #test_score.tolist()\n",
        "      #y_test.tolist()\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      #Max_auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      if (auc_roc-Max_auc_roc)>0:\n",
        "        Max_auc_roc=auc_roc\n",
        "        print(\"Max auc\",Max_auc_roc)\n",
        "      else:\n",
        "        break;\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "#            print(\"Fauc_roc\",auc_roc)\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VU1p0-iF7aW"
      },
      "outputs": [],
      "source": [
        "path='/content/PPI-Inhibitors/'\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];Pfeatures_dict={};target_list=[]\n",
        "print(\"Total samples\",len(ProteinData_dict))\n",
        "for p in ProteinData_dict:\n",
        "  #print(\"sample\",p)\n",
        "  result,PFeatures=model(ProteinData_dict[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  Pfeatures_dict[p]=PFeatures\n",
        "  #print(PFeatures.shape)\n",
        "###############\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import rbf_kernel as kernel\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import auc,precision_recall_curve\n",
        "CC=comp.keys()\n",
        "CC=list(CC);KK=[]\n",
        "[KK.append(k[0].split('_')[0]) for k in CC]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set (KK)))\n",
        "###########\n",
        "Loss=[];AUC_list=[];LP=[];Y_t=[];Y_score=[];Z=[];Yo=[];\n",
        "Complexnames=list (complex_ligand_dict.keys())\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    trainPos,trainNeg=GenerateNegativeBinderonly(trainindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "    train=np.vstack((trainPos,trainNeg))\n",
        "    #All examples passed for negative examples generation\n",
        "#        posexamples=testindex\n",
        "    testPos,testNeg=GenerateNegativeBinderonly(testindex,CC,complex_ligand_dict,New_binders_dict)\n",
        "#        AlltestExamples.extend(testPos);AlltestExamples.extend(testNeg);\n",
        "    ######\n",
        "    Ctr=[];Ptr=[];y_train=[]\n",
        "    for t in trainPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctr.append(getFP(Ldict[t[1]]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(1)\n",
        "    #####\n",
        "    for t in trainNeg:\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctr.append(getFP(t[1]));Ptr.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_train.append(-1);\n",
        "    #######\n",
        "    PC_train_Features=np.hstack((Ptr,Ctr))\n",
        "#        ######\n",
        "\n",
        "    Ctt=[];Ptt=[];y_test=[]\n",
        "    for t in testPos:\n",
        "        if getFP(Ldict[t[1]])is not None:\n",
        "            Ctt.append(getFP(Ldict[t[1]]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(1)\n",
        "    ####\n",
        "    for t in testNeg:\n",
        "#             if t[0]!='1TUE_A_2_B':\n",
        "        if getFP(t[1]) is not None:\n",
        "            Ctt.append(getFP(t[1]));Ptt.append( Pfeatures_dict[t[0]][0].cpu().data.numpy());y_test.append(-1);\n",
        "    #######\n",
        "    PC_test_Features=np.hstack((Ptt,Ctt))\n",
        "    X_train,X_test=PC_train_Features,PC_test_Features\n",
        "    ######################################3\n",
        "    Max_auc_roc=0.0\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    #print(IPPI_MLP_Net())\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(IPPI_Net.parameters(),lr=0.0001,weight_decay=0.001)#0.69 for 1mer single layer#, weight_decay=0.01, betas=(0.9, 0.999))\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "    X_train=torch.FloatTensor(X_train).cuda()\n",
        "    y_train=torch.FloatTensor( y_train).cuda()\n",
        "    X_test=torch.FloatTensor(X_test).cuda()\n",
        "    y_test=torch.FloatTensor( y_test).cuda()\n",
        "    for epoch in range(100):\n",
        "      output = IPPI_Net(X_train)\n",
        "      output=torch.squeeze(output, 1)\n",
        "      target =   y_train\n",
        "      loss = criterion(output, target)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      params = list(IPPI_Net.parameters())\n",
        "      test_score = IPPI_Net(X_test)\n",
        "      test_score=torch.squeeze(test_score, 1)\n",
        "      Y_score.extend(test_score.cpu().data.numpy())\n",
        "      Y_t.extend(y_test.cpu().data.numpy())\n",
        "      auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      #Max_auc_roc=roc_auc_score(np.array(Y_t), np.array(Y_score))\n",
        "      if (auc_roc-Max_auc_roc)>-0.01:\n",
        "        Max_auc_roc=auc_roc\n",
        "        print(\"Max auc\",Max_auc_roc)\n",
        "      else:\n",
        "        break;\n",
        "      print(\"Epoch\",epoch,\"auc_roc\",auc_roc,\"loss\",loss.cpu().data.numpy())\n",
        "      AUC_list.append(auc_roc)\n",
        "      Y_t,Y_score=[],[]\n",
        "      LP.append(loss.cpu().data.numpy())\n",
        "    #Testing\n",
        "    test_score = IPPI_Net(X_test)\n",
        "    test_score=torch.squeeze(test_score, 1)\n",
        "    test_score=test_score.cpu().data.numpy()\n",
        "    y_test=y_test.cpu().data.numpy()\n",
        "    Z=np.append(Z,test_score);Yo=np.append(Yo,y_test);\n",
        "    Auc=roc_auc_score(np.array(y_test), np.array(test_score))\n",
        "    print(\"len of Z,Yo\",len(Z),len(Yo),\"Test P\",t[0],\"Final auc_roc\",Auc)\n",
        "\n",
        "    average_P_score=average_precision_score(y_test,test_score)\n",
        "    print(testPos[0][0],\"\\t\",round (Auc,3),\"\\t\",round (average_P_score,3) ,\"\\t\",len(testPos),\"\\t\",len(testNeg),\"\\t\",round (len(testNeg )/len(testPos),1))\n",
        "    AUC_ROC_final.append(Auc);Avg_P_final.append(average_P_score);\n",
        "#####\n",
        "fpr, tpr, thresholds = roc_curve(Yo, Z)#(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Yo, Z)\n",
        "Auc=(Auc).round(2)\n",
        "# calculate precision-recall curve\n",
        "Z=np.array(Z);Yo=np.array(Yo);\n",
        "#Y_t=np.array(Y_t);Y_score=np.array(Y_score)\n",
        "precision, recall, thresholds = precision_recall_curve(Yo, Z)\n",
        "aucpr=average_precision_score (Yo, Z)\n",
        "  ######\n",
        "fig = plt.figure()\n",
        "plt.plot(recall,precision,color='m',marker=',',label='AUC-PR:{: .2f}'.format(aucpr))\n",
        "plt.title('AUC-PR');plt.xlabel('recall');plt.ylabel('precision');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"AUC-PR PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "aucpr=(aucpr).round(2)\n",
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")\n",
        "#    ######\n",
        "fig = plt.figure()\n",
        "plt.plot(fpr,tpr,color='k',marker='d',label='AUC:{: .2f}'.format(Auc))\n",
        "plt.title('AUCROC');plt.xlabel('FPR');plt.ylabel('TPR');plt.grid();plt.legend();plt.show();\n",
        "fig .savefig(path+\"PPI Inhibitors MLP.pdf\", bbox_inches='tight')\n",
        "###\n",
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))\n",
        "plt.figure()\n",
        "plt.plot(LP)\n",
        "plt.plot( AUC_list)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w8lhNNxI605"
      },
      "outputs": [],
      "source": [
        "PdBloc='/content/PPI-Inhibitors/Data/Pdb'\n",
        "pos_Pid,pos_smiles,pos_label=readFile('/content/PPI-Inhibitors/Data/2p2iInhibitorsSMILES.txt')\n",
        "import pandas as pd\n",
        "fields=['(Complexname)','Binders SMILES']\n",
        "df=pd.read_csv('/content/PPI-Inhibitors/Data/BindersWithComplexname.csv', skipinitialspace=True, usecols=fields)\n",
        "# See the keys\n",
        "#print(df.keys())\n",
        "neg_Pidname,neg_smiles=df[df.keys()[0]].values,df[df.keys()[1]].values\n",
        "neg_Pid=[n.split('_')[0] for n in neg_Pidname]\n",
        "####\n",
        "neg_label=-1.0*(np.ones(len(neg_smiles)))\n",
        "#####\n",
        "compound_iso_smiles=np.append(pos_smiles,neg_smiles)\n",
        "smile_graph = {}\n",
        "for smile in compound_iso_smiles:\n",
        "    g = smile_to_graph(smile)\n",
        "    smile_graph[smile] = g\n",
        "########\n",
        "smiles_all,Pid_all,labels_all=np.array(np.append(pos_smiles,neg_smiles)),np.array(np.append(pos_Pid,neg_Pid)),np.array(np.append(pos_label,neg_label))\n",
        "Processed_data=PrepairDataset.process(smiles_all,Pid_all,labels_all,smile_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KoeCGb8FoTi"
      },
      "outputs": [],
      "source": [
        "print(\"Final average over all folds,Leave one complex out\",np.average(AUC_ROC_final).round(4),'±',np.std( AUC_ROC_final).round(4),np.average(Avg_P_final).round(4),'±',np.std( Avg_P_final).round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFnPHOfVFdbO"
      },
      "outputs": [],
      "source": [
        "print(\"AucROC and aucpr\\n\",Auc,\"\\n\",aucpr,\"\\ntotal P:N ration 1:\",int (np.sum([Yo==-1.0])/np.sum([Yo==1.0])),\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtJr0-RjdALV"
      },
      "outputs": [],
      "source": [
        "#GNN_data,GCN_data,label=Processed_data[0]\n",
        "GNN_data_all,GCN_data_all,labels=[],[],[]\n",
        "for i in range(len(Processed_data)):\n",
        "  GNN_data,GCN_data,label=Processed_data[i]\n",
        "  GNN_data_all.append(GNN_data);GCN_data_all.append(GCN_data);labels.append(label);\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "result_list=[];target_list=[];mini_batch=[];target_list=[]\n",
        "print(\"Total samples\",GNN_data_all)\n",
        "for p in range(0,10):#len(GNN_data_all)):\n",
        "  print(\"sample\",p)\n",
        "  result,PFeatures=model(GNN_data_all[p])#[torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "  print(PFeatures.shape)\n",
        "###############\n",
        "modeling = GCN\n",
        "model_st = modeling.__name__\n",
        "cuda_name = \"cuda:0\"\n",
        "print('cuda_name:', cuda_name)\n",
        "dataset='PPI-inhibitor'\n",
        "#############################################\n",
        "CUDA_LAUNCH_BLOCKING = \"1\"\n",
        "# training the model\n",
        "device = torch.device(cuda_name if torch.cuda.is_available() else \"cpu\")\n",
        "model = modeling().to(device)\n",
        "####\n",
        "LR = 0.0005\n",
        "LOG_INTERVAL = 20\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "best_epoch = -1\n",
        "NUM_EPOCHS=10\n",
        "model_file_name = 'model_' + model_st + '_' + dataset +  '.model'\n",
        "result_file_name = 'result_' + model_st + '_' + dataset +  '.csv'\n",
        "TRAIN_BATCH_SIZE,TEST_BATCH_SIZE=10,10\n",
        "TotalData=len(GCN_data_all)\n",
        "train_data,test_data=GCN_data_all[0:int (TotalData/2)],GCN_data_all[int (TotalData/2):]\n",
        "###################33\n",
        "# make data PyTorch mini-batch processing ready\n",
        "train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=False)\n",
        "#############################################\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    train(model, device, train_loader, optimizer, epoch+1)\n",
        "    #1/0\n",
        "    ###\n",
        "    G,P,Sfeatures= predicting(model, device, test_loader,labels)\n",
        "    print(Sfeatures.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgyCP0gwrUOR"
      },
      "outputs": [],
      "source": [
        "G,P,Sfeatures= predicting(model, device, test_loader,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1KdE_6Cwxp_"
      },
      "outputs": [],
      "source": [
        "Sfeatures[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgVftpkLZLPA"
      },
      "outputs": [],
      "source": [
        "# make data PyTorch Geometric ready\n",
        "dataset='ppi_inhibitor'\n",
        "print('preparing ', dataset + '_train.pt in pytorch format!')\n",
        "train_data = PrepairDataset(root='data', dataset=dataset+'_train', xd=train_drugs, xt=train_prots, y=train_Y,smile_graph=smile_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRtfFzZ4WXzl"
      },
      "outputs": [],
      "source": [
        "Processed_data=PrepairDataset.process(smiles_all,Pid_all,labels_all,smile_graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JErYA7uLVA4a"
      },
      "outputs": [],
      "source": [
        "Processed_data=PrepairDataset.processProtein(Pid_all)#,\"/content/PPI-Inhibitors/Data/Pdb/*\")#labels_all,smile_graph,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtE98ci1UR08"
      },
      "outputs": [],
      "source": [
        "neg_Pid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTQKDaSq9Q56"
      },
      "outputs": [],
      "source": [
        "loc=glob.glob(\"/content/PPI-Inhibitors/Data/Pdb/*\")\n",
        "P1=loc[0]\n",
        "print(P1)\n",
        "parser = PDBParser()\n",
        "with warnings.catch_warnings(record=True) as w:\n",
        "  structure = parser.get_structure(\"\", P1)\n",
        "one_hot_atom=(atom1(structure))\n",
        "print(\"\\none_hot_atom.shape\",one_hot_atom.shape)\n",
        "#\n",
        "one_hot_res=(res1(structure))\n",
        "print(\"\\none_hot_atom.shape\",one_hot_res.shape)\n",
        "neigh_same_res,neigh_diff_res=(neigh1(structure))\n",
        "print(\"\\neigh_same_res.shape,neigh_diff_re.shape\",neigh_same_res.shape,\"\\n\",neigh_diff_res.shape)\n",
        "target=np.array(float (1.0))\n",
        "#############\n",
        "model=GNN()\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model=model.to(device)\n",
        "#model.load_state_dict(torch.load(saved_model))\n",
        "result_list=[];target_list=[];mini_batch=[];target_list=[]\n",
        "#for p in range(0,len(one_hot_atom),2):\n",
        "result,PFeatures=model([torch.tensor(one_hot_atom,dtype=torch.float32).to(device),torch.tensor(one_hot_res,dtype=torch.float32).to(device),torch.tensor(neigh_same_res).to(device).long(),torch.tensor(neigh_diff_res).to(device).long()])\n",
        "1/0\n",
        "#mini_batch.append(result)\n",
        "\n",
        "          # explain the flatten\n",
        "          #Result has a dimension of [N,1] and so we are trying to convert it to  a 1D tensor inorder to plot it\n",
        "          #So we are flattening the result and also the target\n",
        "result_list.append(torch.flatten(result.detach().cpu()).numpy())\n",
        "target_list.append(target)\n",
        "          #print(result_list)\n",
        "\n",
        "#result_list now consist of  a number o numpy arrays like [[<array 1>],[<array 2>], ..] but we need to flatten it\n",
        "# to plot it in a graph like [<array 1 contents>,<array 2 contents>,.... ]\n",
        "result_list=np.array(result_list).flatten()\n",
        "\n",
        "target_list=np.array(target_list).flatten()\n",
        "plt.plot(target_list,result_list,\".\")\n",
        "x = np.linspace(0,1,100)\n",
        "y=x\n",
        "plt.plot(x,y)\n",
        "plt.xlabel(\"Actual scores\")\n",
        "plt.ylabel(\"Predicted Scores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLxf_vjhR6gW"
      },
      "outputs": [],
      "source": [
        "PFeatures.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyCR01HmSJBa"
      },
      "outputs": [],
      "source": [
        "Features=torch.squeeze(Features,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTfSi40fSOyc"
      },
      "outputs": [],
      "source": [
        "Features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6lRbj7WSHb1"
      },
      "outputs": [],
      "source": [
        "same_neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBfRTNqZWzkM"
      },
      "outputs": [],
      "source": [
        "torch.sum(same_neigh > -1).type(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YsTVj98XRSU"
      },
      "outputs": [],
      "source": [
        "same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxQXEooLRKpD"
      },
      "outputs": [],
      "source": [
        "(same_neigh>-1).unsqueeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_s3_j_zRgDm"
      },
      "outputs": [],
      "source": [
        "same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UG6dkxslQpKD"
      },
      "outputs": [],
      "source": [
        "same_neigh,diff_neigh=torch.tensor(neigh_same_res[p]).to(device).long(),torch.tensor(neigh_diff_res[p]).to(device).long()\n",
        "unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(1)\n",
        "unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U_EJGyCZhTp"
      },
      "outputs": [],
      "source": [
        "unsqueezed_same_neigh_indicator.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVjlekYmMC_-"
      },
      "outputs": [],
      "source": [
        "result=model([torch.tensor(one_hot_atom[p],dtype=torch.float32).to(device),torch.tensor(one_hot_res[p],dtype=torch.float32).to(device),torch.tensor(neigh_same_res[p]).to(device).long(),torch.tensor(neigh_diff_res[p]).to(device).long()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVzUq4okNTXn"
      },
      "outputs": [],
      "source": [
        "Z=torch.tensor(one_hot_atom[p:p+2],dtype=torch.float32).to(device)#.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kesdH9WmOQau"
      },
      "outputs": [],
      "source": [
        "Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs4_NBaeNWfA"
      },
      "outputs": [],
      "source": [
        "Z.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJUxautSLGkV"
      },
      "outputs": [],
      "source": [
        "torch.tensor(one_hot_atom[p]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imRUpRMYlMt8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import sampler\n",
        "import glob\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "#from QA.data_import import get_dataloader,data1\n",
        "import matplotlib.pyplot as plt\n",
        "#from QA.train import train\n",
        "#from QA.test import test\n",
        "seed=3\n",
        "torch.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRVez8JZc3Ms"
      },
      "outputs": [],
      "source": [
        "import py3Dmol\n",
        "#taken from https://william-dawson.github.io/using-py3dmol.html\n",
        "print(\"Target\")\n",
        "with open(\"QA/T0759-D1.pdb\") as ifile:\n",
        "    file_info_target = \"\".join([x for x in ifile])\n",
        "view = py3Dmol.view(width=400, height=300)\n",
        "view.addModelsAsFrames(file_info_target)\n",
        "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'blue'}})\n",
        "view.zoomTo()\n",
        "view.show()\n",
        "print(\"Decoy\")\n",
        "with open(\"QA/T0759TS499_1-D1.pdb\") as ifile:\n",
        "    file_info_decoy = \"\".join([x for x in ifile])\n",
        "view = py3Dmol.view(width=400, height=300)\n",
        "view.addModelsAsFrames(file_info_decoy)\n",
        "view.setStyle({'model': -1}, {\"cartoon\": {'color': 'salmon'}})\n",
        "view.zoomTo()\n",
        "view.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxsFy6qkmgkP"
      },
      "source": [
        "#The Graph Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI8iz25ZnO21"
      },
      "source": [
        "### Input features\n",
        "\n",
        "1. One hot encoded atom information\n",
        "\n",
        "2. One hot encoded residue information\n",
        "\n",
        "### GCN update equations\n",
        "\n",
        "\n",
        "$$\n",
        "h_{i}^{(l)}=\\sigma\\left( W_{\\mathrm{center}}^{(l)} h_i^{(l-1)}  +\\frac{1}{|\\mathcal{N}_{\\mathrm{same}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{same}}(i)}{W_{\\mathrm{same}}^{(l)} h_j^{(l-1)}  } +\\frac{1}{|\\mathcal{N}_{\\mathrm{other}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{other}}(i)}{W_{\\mathrm{other}}^{(l)} h_j^{(l-1)} } \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "h_{i}^{(1)}=\\sigma\\left( W_{\\mathrm{center}}^{(0)} h_i^{(0)} + W_{\\mathrm{residue}} r_i +\\frac{1}{|\\mathcal{N}_{\\mathrm{same}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{same}}(i)}{W_{\\mathrm{same}}^{(0)} h_j^{(0)}  } +\\frac{1}{|\\mathcal{N}_{\\mathrm{other}}(i)|}\\sum_{j\\in \\mathcal{N}_{\\mathrm{other}}(i)}{W_{\\mathrm{other}}^{(0)} h_j^{(0)} } \\right)\n",
        "$$\n",
        "\n",
        "$h_i^{(0)}$: atom one-hot-encoding\n",
        "\n",
        "$r_i$ residue one-hot-encoding\n",
        "\n",
        "$\\mathcal{N}_{\\mathrm{same}}(i)$: the set of neighbouring atoms of atom $i$ that are within the same residue\n",
        "\n",
        "\n",
        "$\\mathcal{N}_{\\mathrm{other}}(i)$: the set of neighbouring atoms of atom $i$ that are from different residues\n",
        "\n",
        "$\\sigma$: the activation function\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKWpBoJ2_Ni_"
      },
      "source": [
        "### Steps for computing GCN output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5xoap9pBIB1"
      },
      "source": [
        "In the following example we assume four different atom types: Carbon, Oxygen, Nitrogen, and Hydrogen.  Their one-hot-encoding is  [1,0,0,0] ,[0,1,0,0],[0,0,1,0] and [0,0,0,1], respectively.\n",
        "\n",
        "#### Computing the node signals, $W_{\\mathrm{center}}^{(0)} h^{(0)}$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ__Lem1_WWl"
      },
      "outputs": [],
      "source": [
        "one_hot_encoded_atom=torch.tensor([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],dtype=torch.float32)\n",
        "Wv = torch.randn(4, 10)\n",
        "node_signals=one_hot_encoded_atom @ Wv\n",
        "node_signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8MjRCdDYfP3"
      },
      "source": [
        "#### Computing $\\frac{1}{|\\mathcal{N}_{i}|}\\sum_{j\\in \\mathcal{N}_{i}} W_{\\mathrm{neighbors}}^{(l)} h_j^{(l-1)} $\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBJglgIuBfmO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# neigh_info:\n",
        "# the indices of the closest neighbours of the atoms of a protein.\n",
        "# In this example we have 4 atoms with 3 neighbours per atom.\n",
        "# If no neighbour is present, the index is -1.\n",
        "neigh_info=torch.tensor(\n",
        "                         [\n",
        "                          [1,3,-1],   #neighbours of first atom\n",
        "                          [0,2,-1],   #neighbours of second atom\n",
        "                          [1,3,-1],   #neighbours of third atom\n",
        "                          [2,0,-1]    #neighbours of fourth atom\n",
        "                         ]\n",
        "                        )\n",
        "atom_feat=torch.randn((4,6))\n",
        "# atom_feat:\n",
        "# the features of all the atoms in a protein\n",
        "# Its dimensionality is [4,6] where  4 is the number of atoms and 6 is dimensionality\n",
        "# of the embedding space of each atom\n",
        "\n",
        "neigh_info.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tRgvftTCdzD"
      },
      "outputs": [],
      "source": [
        "print(f'Atom features \\n{atom_feat}')\n",
        "print(f'Neighbour information \\n{neigh_info}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EafQTIwKhNiO"
      },
      "source": [
        "Computing the neighbour signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yesrVFtyCjvF"
      },
      "outputs": [],
      "source": [
        "W_n = torch.randn((6,10),dtype=torch.float32)\n",
        "neigh_signals=atom_feat@W_n\n",
        "neigh_signals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xz_eoAChp3V"
      },
      "source": [
        "Boolean matrix that indicates presence of a neighboring atom:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xP-TwMuy-eOd"
      },
      "outputs": [],
      "source": [
        "neigh_indicator=(neigh_info>-1)\n",
        "neigh_indicator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE6U5eMpl3Oa"
      },
      "source": [
        "Reshaping it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBBtRU-QmqRp"
      },
      "outputs": [],
      "source": [
        "unsqueezed_neigh_indicator=neigh_indicator.unsqueeze(2)\n",
        "unsqueezed_neigh_indicator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyC5MBmR4e0K"
      },
      "source": [
        "Now its shape matches the shape of the neighbor signals:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUKfchWz3xOL"
      },
      "outputs": [],
      "source": [
        "unsqueezed_neigh_indicator.shape, neigh_signals[neigh_info].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrIEdeAqvIi"
      },
      "source": [
        "Next, we compute the features of neighbouring atoms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7jc41l1lCCY"
      },
      "outputs": [],
      "source": [
        "neigh_features=neigh_signals[neigh_info]*unsqueezed_neigh_indicator\n",
        "print (neigh_features.shape)\n",
        "neigh_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYQ11eXSu1O2"
      },
      "source": [
        "Next step:  $\\sum_{j\\in \\mathcal{N}_{i}} W_{\\mathrm{neighbors}}^{(l)} h_j^{(l-1)} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-OV-4Iss_OC"
      },
      "outputs": [],
      "source": [
        "sum_neigh=torch.sum(neigh_features, 1)\n",
        "print(sum_neigh.shape)\n",
        "sum_neigh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwXwWjBlrkVs"
      },
      "source": [
        "We still need to divide by the number of neighbors.  First, find the total number of  neighbouring atoms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GiaOaVWnF_3"
      },
      "outputs": [],
      "source": [
        "num_neigh=torch.sum(neigh_info>-1,1)\n",
        "num_neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPBvwNRxy0MA"
      },
      "outputs": [],
      "source": [
        "num_neigh = num_neigh.unsqueeze(1)\n",
        "# To prevent divide by 0 error\n",
        "num_neigh[num_neigh==0]=1\n",
        "print(num_neigh.shape)\n",
        "num_neigh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ5ULPSt8K4w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFnbxxRp8UM_"
      },
      "source": [
        "Now its shape matches the shape of `sum_neigh`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw45KuWG79dE"
      },
      "outputs": [],
      "source": [
        "sum_neigh.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJal8PbtvPiy"
      },
      "source": [
        "Finally, we can compute $\\frac{1}{|\\mathcal{N}_{i}|}\\sum_{j\\in \\mathcal{N}_{i}} W_{\\mathrm{neighbors}}^{(l)} h_j^{(l-1)} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIRL7Du7r43C"
      },
      "outputs": [],
      "source": [
        "final_neigh_features=(sum_neigh/num_neigh)\n",
        "final_neigh_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13e7lii98CI1"
      },
      "source": [
        "\n",
        "## GNN code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqI1QAF1l9cM"
      },
      "outputs": [],
      "source": [
        "class GNN_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device=device\n",
        "        self.Wsv = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr = nn.Parameter( torch.randn(self.v_feats, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "    def forward(self, x):\n",
        "        Z,same_neigh,diff_neigh = x\n",
        "        print(\"input\",x)\n",
        "        print(Z.shape)\n",
        "        node_signals = Z@self.Wsv\n",
        "        neigh_signals_same=Z@self.Wsr\n",
        "        neigh_signals_diff=Z@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "        final_res = torch.relu(node_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res,same_neigh,diff_neigh\n",
        "\n",
        "class GNN_First_Layer(nn.Module):\n",
        "\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "\n",
        "        self.trainable = trainable\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "        self.Wv = nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wr = nn.Parameter( torch.randn(21,self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wsr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.Wdr= nn.Parameter( torch.randn(13, self.filters, device=self.cuda_device,requires_grad=True))\n",
        "        self.neighbours=10\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        atoms, residues,same_neigh,diff_neigh = x\n",
        "        node_signals = atoms@self.Wv\n",
        "        residue_signals = residues@self.Wr\n",
        "        neigh_signals_same=atoms@self.Wsr\n",
        "        neigh_signals_diff=atoms@self.Wdr\n",
        "        unsqueezed_same_neigh_indicator=(same_neigh>-1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator=(diff_neigh>-1).unsqueeze(2)\n",
        "        same_neigh_features=neigh_signals_same[same_neigh]*unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features=neigh_signals_diff[diff_neigh]*unsqueezed_diff_neigh_indicator\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        # To prevent divide by zero error\n",
        "        same_norm[same_norm==0]=1\n",
        "        diff_norm[diff_norm==0]=1\n",
        "        neigh_same_atoms_signal=(torch.sum(same_neigh_features, axis=1))/same_norm\n",
        "        neigh_diff_atoms_signal=(torch.sum(diff_neigh_features, axis=1))/diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals+residue_signals +neigh_same_atoms_signal+neigh_diff_atoms_signal)\n",
        "        return final_res, same_neigh,diff_neigh\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter( torch.randn(self.in_dims, self.out_dims, device=self.cuda_device,requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "\n",
        "        return Z\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GNN_First_Layer(filters=128)\n",
        "        self.conv2 = GNN_Layer(v_feats=128, filters=256)\n",
        "        self.conv3 = GNN_Layer(v_feats=256, filters=512)\n",
        "        self.dense = Dense(in_dims=512, out_dims=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1=self.conv1(x)\n",
        "        x2=self.conv2(x1)\n",
        "        x3=self.conv3(x2)\n",
        "        x=x3[0]\n",
        "        x=torch.sum(x,axis=0).view(1,-1)\n",
        "        x = F.normalize(x)\n",
        "        x5=self.dense(x)\n",
        "        x6=torch.squeeze(x5,1)\n",
        "\n",
        "        return x6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwoSSfAPDOCo"
      },
      "source": [
        "## GNN Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kMKuQYXWwfX"
      },
      "outputs": [],
      "source": [
        "data_loc=(\"Data/train_data/*/*\")\n",
        "label_loc=(\"Data/train_label/\")\n",
        "workers=0\n",
        "batch_size=10\n",
        "loader_info=data1(data_loc,label_loc,batch_size,workers)\n",
        "\n",
        "train(GNN(),get_dataloader(loader_info))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOjfQlabDQuJ"
      },
      "source": [
        "## GNN Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qotMtxQCZQ7p"
      },
      "outputs": [],
      "source": [
        "data_loc=(\"Data/test_data/*/*\")\n",
        "label_loc=(\"Data/test_label/\")\n",
        "workers=0\n",
        "batch_size=10\n",
        "temp=data1(data_loc,label_loc,batch_size,workers)\n",
        "test(GNN(),get_dataloader(temp),'modelGCNL2_Global_Basic1.ckpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcpU9kAt9bJl"
      },
      "source": [
        "### GNN performance on the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1qcV5zutVmI"
      },
      "outputs": [],
      "source": [
        "data_loc=(\"Data/train_data/*/*\")\n",
        "label_loc=(\"Data/train_label/\")\n",
        "workers=0\n",
        "batch_size=10\n",
        "temp=data1(data_loc,label_loc,batch_size,workers)\n",
        "test(GNN(),get_dataloader(temp),'modelGCNL2_Global_Basic1.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaWVorauHHDN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HxsFy6qkmgkP",
        "OI8iz25ZnO21",
        "UKWpBoJ2_Ni_",
        "f5xoap9pBIB1",
        "E8MjRCdDYfP3",
        "13e7lii98CI1",
        "VwoSSfAPDOCo",
        "HOjfQlabDQuJ",
        "AcpU9kAt9bJl"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}