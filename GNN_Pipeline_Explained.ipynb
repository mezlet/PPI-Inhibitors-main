{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN-Based Pipeline for Predicting Small Molecule Inhibition of Protein Complexes - Detailed Explanation\n",
    "\n",
    "This notebook provides a comprehensive explanation of the Graph Neural Network (GNN) based pipeline for predicting small molecule inhibition of protein-protein interaction (PPI) complexes.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Setup and Installation](#setup)\n",
    "3. [Data Balancing Utilities](#data-balancing)\n",
    "4. [Protein Structure Processing](#protein-processing)\n",
    "5. [GNN Model Architecture](#gnn-architecture)\n",
    "6. [Training Pipeline](#training)\n",
    "7. [Evaluation and Results](#evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview {#overview}\n",
    "\n",
    "### What This Pipeline Does\n",
    "\n",
    "This pipeline predicts whether a small molecule can inhibit protein-protein interactions (PPIs). The key components are:\n",
    "\n",
    "- **Input**: 3D protein structures (PDB files) and small molecule SMILES strings\n",
    "- **Process**: Convert proteins to graphs, extract features using GNN, combine with compound fingerprints\n",
    "- **Output**: Binary prediction (inhibitor or not) with confidence scores\n",
    "\n",
    "### Key Technical Approach\n",
    "\n",
    "1. **Graph Representation**: Proteins are represented as graphs where:\n",
    "   - Nodes = atoms\n",
    "   - Edges = spatial proximity (within 6Å)\n",
    "   - Two types of edges: same-residue and different-residue neighbors\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - Atom type (C, N, O, etc.) - one-hot encoded\n",
    "   - Residue type (ALA, ARG, etc.) - one-hot encoded\n",
    "   - Neighborhood information (up to 10 neighbors per atom)\n",
    "\n",
    "3. **Model Architecture**:\n",
    "   - GNN for protein structure encoding\n",
    "   - MLP for final prediction combining protein features, interface features, and compound fingerprints\n",
    "\n",
    "4. **Training Strategy**:\n",
    "   - Leave-one-complex-out cross-validation (LOCO-CV)\n",
    "   - Balanced batch sampling to handle class imbalance\n",
    "   - Class-weighted loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation {#setup}\n",
    "\n",
    "### Environment Configuration\n",
    "\n",
    "The pipeline requires GPU acceleration for efficient training. When running on Google Colab:\n",
    "1. Go to **Runtime → Change Runtime Type**\n",
    "2. Set **Hardware accelerator** to **GPU**\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "- **BioPython**: For parsing PDB files and extracting protein structures\n",
    "- **RDKit**: For processing molecular structures and generating fingerprints\n",
    "- **PyTorch**: Deep learning framework for GNN implementation\n",
    "- **scikit-learn**: For data preprocessing and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install biopython  # For protein structure parsing\n",
    "!pip install rdkit      # For molecular fingerprints\n",
    "\n",
    "# Clone the repository containing data and features\n",
    "!git clone https://github.com/adibayaseen/PPI-Inhibitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Balancing Utilities {#data-balancing}\n",
    "\n",
    "### The Imbalanced Data Problem\n",
    "\n",
    "In drug discovery, we typically have many more non-inhibitors (negatives) than inhibitors (positives). This creates challenges:\n",
    "- Models can achieve high accuracy by simply predicting everything as negative\n",
    "- True positive examples get \"drowned out\" during training\n",
    "- The model fails to learn the distinguishing features of inhibitors\n",
    "\n",
    "### Solution: Balanced Sampling\n",
    "\n",
    "This pipeline implements two strategies for handling imbalanced data:\n",
    "\n",
    "1. **BalancedDataset + WeightedRandomSampler**: Assigns higher sampling probability to minority class\n",
    "2. **BinaryBalancedSampler**: Ensures each batch has exactly 50% positive and 50% negative examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 1: BalancedDataset\n",
    "\n",
    "**Purpose**: Creates a dataset with sample weights inversely proportional to class frequency.\n",
    "\n",
    "**How It Works**:\n",
    "1. Counts examples in each class using `np.bincount()`\n",
    "2. Calculates weights as `1 / class_count`\n",
    "3. Assigns each sample a weight based on its class\n",
    "\n",
    "**Example**: \n",
    "- If you have 100 negatives and 10 positives:\n",
    "  - Negative weight = 1/100 = 0.01\n",
    "  - Positive weight = 1/10 = 0.1\n",
    "  - Positives are 10x more likely to be sampled\n",
    "\n",
    "**Note**: Uses stochastic sampling, so some examples might never be selected in an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "class BalancedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class that creates a balanced dataset from imbalanced data.\n",
    "    This dataset calculates sample weights inversely proportional to class frequencies,\n",
    "    which can be used with a WeightedRandomSampler to achieve balanced batches.\n",
    "\n",
    "    NOTE: As it involves stochastic sampling, there is a chance that a few training \n",
    "    examples are actually never selected.\n",
    "\n",
    "    Attributes:\n",
    "        data: The input data (list, NumPy array, or PyTorch tensor)\n",
    "        labels: The labels corresponding to the data (1D array-like object)\n",
    "        sample_weights: Weights for each sample, inversely proportional to class frequencies\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "        # Count the number of examples in each class\n",
    "        class_counts = np.bincount(self.labels)\n",
    "        \n",
    "        # Assign weight inversely proportional to class frequency\n",
    "        # Rare classes get higher weights\n",
    "        weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "        \n",
    "        # Create a weight list for each sample based on its class\n",
    "        self.sample_weights = weights[labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: create_balanced_loader\n",
    "\n",
    "**Purpose**: Convenience function to create a DataLoader with automatic balancing.\n",
    "\n",
    "**Parameters**:\n",
    "- `data`: Input features\n",
    "- `labels`: Binary labels (0 or 1)\n",
    "- `batch_size`: Number of samples per batch (default: 32)\n",
    "\n",
    "**Returns**: A DataLoader that yields balanced batches\n",
    "\n",
    "**Usage**: Ideal for quick prototyping when you want automatic balancing without manual setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_balanced_loader(data, labels, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader with balanced batches for a given dataset.\n",
    "    This function is useful for training models on imbalanced datasets.\n",
    "\n",
    "    Args:\n",
    "        data: The input data (list, NumPy array, or PyTorch tensor)\n",
    "        labels: The labels corresponding to the data (1D array-like object)\n",
    "        batch_size: The size of each batch (default: 32)\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A PyTorch DataLoader that yields balanced batches\n",
    "\n",
    "    Usage Example:\n",
    "        >>> data = [features1, features2, ...]  # Your data features\n",
    "        >>> labels = [label1, label2, ...]     # Your data labels\n",
    "        >>> balanced_loader = create_balanced_loader(data, labels, batch_size=32)\n",
    "        >>> for batch_data, batch_labels in balanced_loader:\n",
    "        >>>     # Train your model using the balanced batches\n",
    "    \"\"\"\n",
    "    dataset = BalancedDataset(data, labels)\n",
    "    \n",
    "    # WeightedRandomSampler will take care of the balancing\n",
    "    # replacement=True allows samples to be selected multiple times\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=dataset.sample_weights, \n",
    "        num_samples=len(dataset.sample_weights), \n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 2: BinaryBalancedSampler\n",
    "\n",
    "**Purpose**: Ensures each batch has exactly equal numbers of positive and negative examples.\n",
    "\n",
    "**How It Works**:\n",
    "1. Identifies majority and minority classes\n",
    "2. Oversamples minority class to match majority class size\n",
    "3. Uses StratifiedKFold to split into batches while maintaining 50-50 ratio\n",
    "\n",
    "**Key Features**:\n",
    "- Guarantees perfect balance in every batch\n",
    "- Uses stratified splitting to maintain ratio\n",
    "- May result in more examples per epoch than in dataset (due to oversampling)\n",
    "\n",
    "**Equivalent Epochs**: The number of times minority class is seen in one DataLoader iteration.\n",
    "- If you have 100 positives and 1000 negatives, equivalent_epochs ≈ 10\n",
    "- This means minority examples are seen 10x per \"epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class BinaryBalancedSampler(Sampler):\n",
    "    \"\"\"\n",
    "    A PyTorch Sampler that returns batches with an equal number of positive \n",
    "    and negative examples. The sampler oversamples from the minority class to \n",
    "    balance the majority class, ensuring that each batch contains 50% positive \n",
    "    and 50% negative examples.\n",
    "\n",
    "    NOTE: It leads to more examples in single iteration through the data loader \n",
    "    than in one epoch\n",
    "\n",
    "    Attributes:\n",
    "        class_vector: List or numpy array of class labels\n",
    "        batch_size: The size of each batch\n",
    "        n_splits: The number of batches/splits in the dataset\n",
    "        equivalent_epochs: The number of times the sampler goes over the minority \n",
    "                          class in one complete iteration of the DataLoader\n",
    "    \"\"\"\n",
    "    def __init__(self, class_vector, batch_size=10):\n",
    "        self.batch_size = batch_size\n",
    "        self.class_vector = class_vector\n",
    "        \n",
    "        YY = np.array(self.class_vector)\n",
    "        \n",
    "        # Find majority class (class with most examples)\n",
    "        U, C = np.unique(YY, return_counts=True)\n",
    "        M = U[np.argmax(C)]  # Majority class label\n",
    "        \n",
    "        # Get indices of majority and minority classes\n",
    "        Midx = np.nonzero(YY == M)[0]  # Majority indices\n",
    "        midx = np.nonzero(YY != M)[0]  # Minority indices\n",
    "        \n",
    "        # Oversample minority indices to match majority size\n",
    "        midx_ = np.random.choice(midx, size=len(Midx))\n",
    "        \n",
    "        # Combine majority and oversampled minority\n",
    "        self.YY = np.array(list(YY[Midx]) + list(YY[midx_]))\n",
    "        self.idx = np.array(list(Midx) + list(midx_))\n",
    "        \n",
    "        # Calculate number of batches\n",
    "        self.n_splits = int(np.ceil(len(self.idx) / self.batch_size))\n",
    "        \n",
    "        # Calculate equivalent epochs\n",
    "        self.equivalent_epochs = len(self.idx) / len(self.class_vector)\n",
    "        print(f'Equivalent epochs in one iteration of data loader: {self.equivalent_epochs}')\n",
    "\n",
    "    def gen_sample_array(self):\n",
    "        \"\"\"\n",
    "        Generates batch indices using StratifiedKFold to maintain class balance.\n",
    "        Yields indices for each batch.\n",
    "        \"\"\"\n",
    "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True)\n",
    "        for tridx, ttidx in skf.split(self.idx, self.YY):\n",
    "            yield np.array(self.idx[ttidx])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_sample_array())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 3: CustomDataset\n",
    "\n",
    "**Purpose**: Basic PyTorch Dataset wrapper for data and labels.\n",
    "\n",
    "**Usage**: Simple container for pairing features with labels. Used with BinaryBalancedSampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset wrapper that pairs data with labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Protein Structure Processing {#protein-processing}\n",
    "\n",
    "### Overview\n",
    "\n",
    "To use proteins in a GNN, we need to convert 3D structures (PDB files) into graph representations. This involves:\n",
    "\n",
    "1. **Node Features**: Information about each atom\n",
    "2. **Edge Information**: Which atoms are connected (neighbors)\n",
    "\n",
    "### The Three Core Functions\n",
    "\n",
    "1. **atom1()**: Encodes atom types\n",
    "2. **res1()**: Encodes residue types\n",
    "3. **neigh1()**: Computes neighborhood structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 1: atom1(structure)\n",
    "\n",
    "**Purpose**: One-hot encode atom types in the protein structure.\n",
    "\n",
    "**How It Works**:\n",
    "1. Defines vocabulary of atom types: C, CA, CB, CG, CH2, N, NH2, OG, OH, O1, O2, SE\n",
    "2. Unknown atoms are mapped to \"1\" (catch-all category)\n",
    "3. Uses sklearn's OneHotEncoder for encoding\n",
    "\n",
    "**Input**: BioPython Structure object\n",
    "\n",
    "**Output**: NumPy array of shape (N_atoms, 13) where:\n",
    "- N_atoms = total number of atoms in structure\n",
    "- 13 = number of atom types (12 known + 1 unknown)\n",
    "\n",
    "**Example**: For a Cα (alpha carbon) atom, output might be [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import warnings\n",
    "\n",
    "def atom1(structure):\n",
    "    \"\"\"\n",
    "    One-hot encodes atom types in a protein structure.\n",
    "    \n",
    "    Args:\n",
    "        structure: BioPython Structure object from PDB file\n",
    "    \n",
    "    Returns:\n",
    "        atoms_onehot: NumPy array of shape (N_atoms, 13) with one-hot encoded atom types\n",
    "    \"\"\"\n",
    "    # Define vocabulary of atom types\n",
    "    atomslist = np.array(sorted(np.array([\n",
    "        'C', 'CA', 'CB', 'CG', 'CH2', 'N', 'NH2', 'OG', 'OH', 'O1', 'O2', 'SE', '1'\n",
    "    ]))).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize and fit encoder\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc.fit(atomslist)\n",
    "    \n",
    "    # Extract atom names from structure\n",
    "    atom_list = []\n",
    "    for atom in structure.get_atoms():\n",
    "        if atom.get_name() in atomslist:\n",
    "            atom_list.append(atom.get_name())\n",
    "        else:\n",
    "            # Unknown atoms mapped to '1'\n",
    "            atom_list.append(\"1\")\n",
    "    \n",
    "    # One-hot encode\n",
    "    atoms_onehot = enc.transform(np.array(atom_list).reshape(-1, 1)).toarray()\n",
    "    \n",
    "    return atoms_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 2: res1(structure)\n",
    "\n",
    "**Purpose**: One-hot encode residue types for each atom in the protein structure.\n",
    "\n",
    "**How It Works**:\n",
    "1. Defines vocabulary of 20 standard amino acids plus unknown\n",
    "2. For each atom, gets its parent residue\n",
    "3. Encodes the residue type\n",
    "\n",
    "**Input**: BioPython Structure object\n",
    "\n",
    "**Output**: NumPy array of shape (N_atoms, 21) where:\n",
    "- N_atoms = total number of atoms\n",
    "- 21 = number of residue types (20 amino acids + 1 unknown)\n",
    "\n",
    "**Why Encode at Atom Level?**: \n",
    "- The GNN operates on atoms as nodes\n",
    "- Each atom needs to know what residue it belongs to\n",
    "- This provides chemical context for the atom\n",
    "\n",
    "**Example**: All atoms in an alanine residue will have the same residue encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res1(structure):\n",
    "    \"\"\"\n",
    "    One-hot encodes residue information for each atom.\n",
    "    Each atom is labeled with its parent residue type.\n",
    "    \n",
    "    Args:\n",
    "        structure: BioPython Structure object from PDB file\n",
    "    \n",
    "    Returns:\n",
    "        res_onehot: NumPy array of shape (N_atoms, 21) with one-hot encoded residue types\n",
    "    \"\"\"\n",
    "    # Define vocabulary of residue types (20 amino acids + unknown)\n",
    "    residuelist = np.array(sorted(np.array([\n",
    "        'ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS', \n",
    "        'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS', '1'\n",
    "    ]))).reshape(-1, 1)\n",
    "    \n",
    "    # Initialize and fit encoder\n",
    "    encr = OneHotEncoder(handle_unknown='ignore')\n",
    "    encr.fit(residuelist)\n",
    "    \n",
    "    # Extract residue name for each atom\n",
    "    residue_list = []\n",
    "    for atom in structure.get_atoms():\n",
    "        if atom.get_parent().get_resname() in residuelist:\n",
    "            residue_list.append((atom.get_parent()).get_resname())\n",
    "        else:\n",
    "            # Unknown residues mapped to '1'\n",
    "            residue_list.append(\"1\")\n",
    "\n",
    "    # One-hot encode\n",
    "    res_onehot = encr.transform(np.array(residue_list).reshape(-1, 1)).toarray()\n",
    "\n",
    "    return res_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function 3: neigh1(structure)\n",
    "\n",
    "**Purpose**: Compute neighborhood structure for the protein graph.\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Spatial Neighbors**: Atoms within 6Å distance are considered neighbors\n",
    "- **Two Types of Edges**:\n",
    "  1. **Same-residue neighbors**: Atoms in the same amino acid (covalent bonds)\n",
    "  2. **Different-residue neighbors**: Atoms in different amino acids (non-covalent interactions)\n",
    "- **Limited to 10 neighbors**: Each type is capped at 10 to control graph density\n",
    "\n",
    "**Why Distinguish Edge Types?**\n",
    "- Same-residue edges represent chemical bonds (strong, local)\n",
    "- Different-residue edges represent interactions (weaker, global)\n",
    "- The GNN learns different weight matrices for each type\n",
    "\n",
    "**Algorithm Steps**:\n",
    "1. Use BioPython's NeighborSearch to find all atom pairs within 6Å\n",
    "2. Sort neighbors by distance (closest first)\n",
    "3. For each atom, collect up to 10 same-residue neighbors\n",
    "4. For each atom, collect up to 10 different-residue neighbors\n",
    "5. Use -1 to indicate \"no neighbor\" if atom has < 10 neighbors\n",
    "\n",
    "**Output**: Two NumPy arrays of shape (N_atoms, 10):\n",
    "- `neigh_same_res`: Indices of same-residue neighbors\n",
    "- `neigh_diff_res`: Indices of different-residue neighbors\n",
    "\n",
    "**Example**: \n",
    "```\n",
    "neigh_same_res[42] = [41, 43, 40, 44, -1, -1, -1, -1, -1, -1]\n",
    "# Atom 42 has 4 same-residue neighbors (atoms 41, 43, 40, 44)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB.NeighborSearch import NeighborSearch\n",
    "\n",
    "def neigh1(structure):\n",
    "    \"\"\"\n",
    "    Calculates the neighbors of each atom with a 6Å distance cutoff.\n",
    "    Distinguishes between same-residue and different-residue neighbors.\n",
    "    Limits each atom to 10 neighbors of each type.\n",
    "    \n",
    "    Args:\n",
    "        structure: BioPython Structure object from PDB file\n",
    "    \n",
    "    Returns:\n",
    "        neigh_same_res: NumPy array of shape (N_atoms, 10) with indices of same-residue neighbors\n",
    "        neigh_diff_res: NumPy array of shape (N_atoms, 10) with indices of different-residue neighbors\n",
    "        \n",
    "    Note: -1 indicates no neighbor in that position\n",
    "    \"\"\"\n",
    "    # Get all atoms as numpy array\n",
    "    atom_list = np.array([atom for atom in structure.get_atoms()])\n",
    "\n",
    "    # Find all atom pairs within 6Å\n",
    "    p4 = NeighborSearch(atom_list)\n",
    "    neighbour_list = p4.search_all(6, level=\"A\")\n",
    "    neighbour_list = np.array(neighbour_list)\n",
    "\n",
    "    # Calculate distances between neighbors\n",
    "    dist = np.array([atom1 - atom2 for atom1, atom2 in neighbour_list])\n",
    "    \n",
    "    # Sort neighbors by distance (ascending)\n",
    "    place = np.argsort(dist)\n",
    "    sorted_neighbour_list = neighbour_list[place]\n",
    "\n",
    "    # Extract atom objects\n",
    "    source_vertex_list_atom_object = np.array(sorted_neighbour_list[:, 0])\n",
    "    len_source_vertex = len(source_vertex_list_atom_object)\n",
    "    neighbour_vertex_with_respect_each_source_atom_object = np.array(sorted_neighbour_list[:, 1])\n",
    "    \n",
    "    # Store original atom serial numbers and residue numbers\n",
    "    old_atom_number = []\n",
    "    old_residue_number = []\n",
    "    for i in atom_list:\n",
    "        old_atom_number.append(i.get_serial_number())\n",
    "        old_residue_number.append(i.get_parent().get_id()[1])\n",
    "    old_atom_number = np.array(old_atom_number)\n",
    "    old_residue_number = np.array(old_residue_number)\n",
    "    \n",
    "    total_atoms = len(atom_list)\n",
    "    \n",
    "    # Initialize neighbor arrays with -1 (no neighbor)\n",
    "    neigh_same_res = np.array([[-1]*10 for i in range(total_atoms)])\n",
    "    neigh_diff_res = np.array([[-1]*10 for i in range(total_atoms)])\n",
    "    \n",
    "    # Counters for number of neighbors added\n",
    "    same_flag = [0] * total_atoms\n",
    "    diff_flag = [0] * total_atoms\n",
    "    \n",
    "    # Iterate through all neighbor pairs\n",
    "    for i in range(len_source_vertex):\n",
    "        source_atom_id = source_vertex_list_atom_object[i].get_serial_number()\n",
    "        neigh_atom_id = neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
    "        source_atom_res = source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
    "        neigh_atom_res = neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
    "        \n",
    "        # Find indices in original atom array\n",
    "        temp_index1 = np.where(source_atom_id == old_atom_number)[0]\n",
    "        temp_index2 = np.where(neigh_atom_id == old_atom_number)[0]\n",
    "        \n",
    "        for i1 in temp_index1:\n",
    "            if old_residue_number[i1] == source_atom_res:\n",
    "                source_index = i1\n",
    "                break\n",
    "        for i1 in temp_index2:\n",
    "            if old_residue_number[i1] == neigh_atom_res:\n",
    "                neigh_index = i1\n",
    "                break\n",
    "        \n",
    "        # If both atoms in same residue\n",
    "        if source_atom_res == neigh_atom_res:\n",
    "            # Add to same-residue neighbors (limit to 10)\n",
    "            if int(same_flag[source_index]) < 10:\n",
    "                neigh_same_res[source_index][same_flag[source_index]] = neigh_index\n",
    "                same_flag[source_index] += 1\n",
    "\n",
    "            if int(same_flag[neigh_index]) < 10:\n",
    "                neigh_same_res[neigh_index][same_flag[neigh_index]] = source_index\n",
    "                same_flag[neigh_index] += 1\n",
    "\n",
    "        # If atoms in different residues\n",
    "        elif source_atom_res != neigh_atom_res:\n",
    "            # Add to different-residue neighbors (limit to 10)\n",
    "            if int(diff_flag[source_index]) < 10:\n",
    "                neigh_diff_res[source_index][diff_flag[source_index]] = neigh_index\n",
    "                diff_flag[source_index] += 1\n",
    "\n",
    "            if int(diff_flag[neigh_index]) < 10:\n",
    "                neigh_diff_res[neigh_index][diff_flag[neigh_index]] = source_index\n",
    "                diff_flag[neigh_index] += 1\n",
    "\n",
    "    return neigh_same_res, neigh_diff_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GNN Model Architecture {#gnn-architecture}\n",
    "\n",
    "### Overview\n",
    "\n",
    "The Graph Neural Network consists of:\n",
    "1. **GNN_First_Layer**: Initial layer that processes raw atom and residue features\n",
    "2. **GNN_Layer**: Subsequent layers that process and aggregate information\n",
    "3. **Dense**: Final layer for dimensionality reduction\n",
    "4. **GNN**: Complete model that chains these components\n",
    "\n",
    "### Key Concept: Message Passing\n",
    "\n",
    "Each GNN layer performs **message passing**:\n",
    "1. Each node (atom) creates a \"message\" for its neighbors\n",
    "2. Each node receives messages from its neighbors\n",
    "3. Node updates its representation by combining its own features with neighbor messages\n",
    "4. This allows information to flow through the graph\n",
    "\n",
    "**Why 3 Layers?**\n",
    "- Layer 1: Sees immediate neighbors (1-hop)\n",
    "- Layer 2: Sees neighbors of neighbors (2-hop)\n",
    "- Layer 3: Sees even further (3-hop)\n",
    "- Each layer increases the \"receptive field\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN_First_Layer\n",
    "\n",
    "**Purpose**: Initial layer that processes raw features and creates first hidden representation.\n",
    "\n",
    "**Inputs**:\n",
    "- `atoms`: One-hot encoded atom types (N × 13)\n",
    "- `residues`: One-hot encoded residue types (N × 21)\n",
    "- `same_neigh`: Same-residue neighbor indices (N × 10)\n",
    "- `diff_neigh`: Different-residue neighbor indices (N × 10)\n",
    "\n",
    "**Learnable Parameters**:\n",
    "- `Wv`: Weight matrix for atom features (13 → 512)\n",
    "- `Wr`: Weight matrix for residue features (21 → 512)\n",
    "- `Wsr`: Weight matrix for same-residue neighbor messages (13 → 512)\n",
    "- `Wdr`: Weight matrix for different-residue neighbor messages (13 → 512)\n",
    "\n",
    "**Message Passing Formula**:\n",
    "```\n",
    "h_i = ReLU(atoms_i @ Wv + residues_i @ Wr + \n",
    "           mean(atoms_j @ Wsr for j in same_neighbors) +\n",
    "           mean(atoms_k @ Wdr for k in diff_neighbors))\n",
    "```\n",
    "\n",
    "**Output**: Hidden representation (N × 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN_First_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    First layer of the GNN that processes raw atom and residue features.\n",
    "    \n",
    "    This layer:\n",
    "    1. Transforms atom features through Wv\n",
    "    2. Transforms residue features through Wr\n",
    "    3. Aggregates same-residue neighbor information through Wsr\n",
    "    4. Aggregates different-residue neighbor information through Wdr\n",
    "    5. Combines all signals with ReLU activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, trainable=True, **kwargs):\n",
    "        super(GNN_First_Layer, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        # Set device (GPU if available)\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        self.cuda_device = device\n",
    "        \n",
    "        # Initialize learnable weight matrices\n",
    "        self.Wv = nn.Parameter(torch.randn(13, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        self.Wr = nn.Parameter(torch.randn(21, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        self.Wsr = nn.Parameter(torch.randn(13, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        self.Wdr = nn.Parameter(torch.randn(13, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        \n",
    "        self.neighbours = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        atoms, residues, same_neigh, diff_neigh = x\n",
    "        \n",
    "        # Transform node's own features\n",
    "        node_signals = atoms @ self.Wv\n",
    "        residue_signals = residues @ self.Wr\n",
    "        \n",
    "        # Transform atom features for neighbor aggregation\n",
    "        neigh_signals_same = atoms @ self.Wsr\n",
    "        neigh_signals_diff = atoms @ self.Wdr\n",
    "        \n",
    "        # Create masks for valid neighbors (indices > -1)\n",
    "        unsqueezed_same_neigh_indicator = (same_neigh > -1).unsqueeze(2)\n",
    "        unsqueezed_diff_neigh_indicator = (diff_neigh > -1).unsqueeze(2)\n",
    "        \n",
    "        # Gather neighbor features and mask out invalid neighbors\n",
    "        same_neigh_features = neigh_signals_same[same_neigh] * unsqueezed_same_neigh_indicator\n",
    "        diff_neigh_features = neigh_signals_diff[diff_neigh] * unsqueezed_diff_neigh_indicator\n",
    "        \n",
    "        # Calculate normalization factors (number of valid neighbors)\n",
    "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
    "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
    "        \n",
    "        # Prevent division by zero\n",
    "        same_norm[same_norm == 0] = 1\n",
    "        diff_norm[diff_norm == 0] = 1\n",
    "        \n",
    "        # Aggregate neighbor signals (mean pooling)\n",
    "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1)) / same_norm\n",
    "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1)) / diff_norm\n",
    "\n",
    "        # Combine all signals with ReLU activation\n",
    "        final_res = torch.relu(\n",
    "            node_signals + residue_signals + \n",
    "            neigh_same_atoms_signal + neigh_diff_atoms_signal\n",
    "        )\n",
    "\n",
    "        return final_res, same_neigh, diff_neigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN_Layer\n",
    "\n",
    "**Purpose**: Subsequent GNN layers that continue message passing on hidden representations.\n",
    "\n",
    "**Inputs**:\n",
    "- `Z`: Hidden representation from previous layer (N × v_feats)\n",
    "- `same_neigh`: Same-residue neighbor indices (N × 10)\n",
    "- `diff_neigh`: Different-residue neighbor indices (N × 10)\n",
    "\n",
    "**Learnable Parameters**:\n",
    "- `Wsv`: Weight matrix for self features (v_feats → filters)\n",
    "- `Wsr`: Weight matrix for same-residue neighbors (v_feats → filters)\n",
    "- `Wdr`: Weight matrix for different-residue neighbors (v_feats → filters)\n",
    "\n",
    "**Difference from First Layer**:\n",
    "- No separate residue features (already incorporated)\n",
    "- Operates on learned representations instead of raw features\n",
    "\n",
    "**Message Passing Formula**:\n",
    "```\n",
    "h_i^(l+1) = ReLU(h_i^l @ Wsv + \n",
    "                 mean(h_j^l @ Wsr for j in same_neighbors) +\n",
    "                 mean(h_k^l @ Wdr for k in diff_neighbors))\n",
    "```\n",
    "\n",
    "**Output**: Updated hidden representation (N × filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Subsequent GNN layer that processes hidden representations.\n",
    "    \n",
    "    This layer:\n",
    "    1. Transforms node's hidden features through Wsv\n",
    "    2. Aggregates same-residue neighbor information through Wsr\n",
    "    3. Aggregates different-residue neighbor information through Wdr\n",
    "    4. Combines signals with ReLU activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
    "        super(GNN_Layer, self).__init__()\n",
    "        self.v_feats = v_feats  # Input feature dimension\n",
    "        self.filters = filters  # Output feature dimension\n",
    "        self.trainable = trainable\n",
    "        \n",
    "        # Set device\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        self.cuda_device = device\n",
    "        \n",
    "        # Initialize learnable weight matrices\n",
    "        self.Wsv = nn.Parameter(torch.randn(self.v_feats, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        self.Wdr = nn.Parameter(torch.randn(self.v_feats, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        self.Wsr = nn.Parameter(torch.randn(self.v_feats, self.filters, device=self.cuda_device, requires_grad=True))\n",
    "        \n",
    "        self.neighbours = 10\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z, same_neigh, diff_neigh = x\n",
    "        \n",
    "        # Transform node's own features\n",
    "        node_signals = Z @ self.Wsv\n",
    "        \n",
    "        # Transform features for neighbor aggregation\n",
    "        neigh_signals_same = Z @ self.Wsr\n",
    "        neigh_signals_diff = Z @ self.Wdr\n",
    "        \n",
    "        # Create masks for valid neighbors\n",
    "        unsqueezed_same_neigh_indicator = (same_neigh > -1).unsqueeze(2)\n",
    "        unsqueezed_diff_neigh_indicator = (diff_neigh > -1).unsqueeze(2)\n",
    "        \n",
    "        # Gather and mask neighbor features\n",
    "        same_neigh_features = neigh_signals_same[same_neigh] * unsqueezed_same_neigh_indicator\n",
    "        diff_neigh_features = neigh_signals_diff[diff_neigh] * unsqueezed_diff_neigh_indicator\n",
    "        \n",
    "        # Calculate normalization factors\n",
    "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
    "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
    "\n",
    "        # Prevent division by zero\n",
    "        same_norm[same_norm == 0] = 1\n",
    "        diff_norm[diff_norm == 0] = 1\n",
    "        \n",
    "        # Aggregate neighbor signals (mean pooling)\n",
    "        neigh_same_atoms_signal = (torch.sum(same_neigh_features, axis=1)) / same_norm\n",
    "        neigh_diff_atoms_signal = (torch.sum(diff_neigh_features, axis=1)) / diff_norm\n",
    "        \n",
    "        # Combine all signals with ReLU activation\n",
    "        final_res = torch.relu(\n",
    "            node_signals + neigh_same_atoms_signal + neigh_diff_atoms_signal\n",
    "        )\n",
    "\n",
    "        return final_res, same_neigh, diff_neigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer\n",
    "\n",
    "**Purpose**: Simple fully-connected layer with sigmoid activation.\n",
    "\n",
    "**Usage**: Reduces dimensionality from 512 to 1 (final prediction layer in some contexts).\n",
    "\n",
    "**Formula**: `y = sigmoid(x @ W)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple dense (fully-connected) layer with sigmoid activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
    "        super(Dense, self).__init__()\n",
    "        self.in_dims = in_dims\n",
    "        self.out_dims = out_dims\n",
    "        \n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        self.cuda_device = device\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(self.in_dims, self.out_dims, device=self.cuda_device, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete GNN Model\n",
    "\n",
    "**Purpose**: Chains all GNN components into a complete protein encoder.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Input: (atoms, residues, same_neigh, diff_neigh)\n",
    "  ↓\n",
    "GNN_First_Layer: (13+21) → 512\n",
    "  ↓\n",
    "GNN_Layer: 512 → 1024\n",
    "  ↓\n",
    "GNN_Layer: 1024 → 512\n",
    "  ↓\n",
    "Dense: 512 → 1 (not used in final pipeline)\n",
    "  ↓\n",
    "Global Sum Pooling: Aggregate all atom representations\n",
    "  ↓\n",
    "L2 Normalization: Normalize to unit vector\n",
    "  ↓\n",
    "Output: Protein embedding (1 × 512)\n",
    "```\n",
    "\n",
    "**Key Operations**:\n",
    "1. **Global Sum Pooling**: `sum(all atom representations)` - creates protein-level representation\n",
    "2. **L2 Normalization**: Ensures embeddings have unit norm for better comparison\n",
    "\n",
    "**Output**: Fixed-size vector (512-dim) representing the entire protein structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Complete GNN model for encoding protein structures.\n",
    "    \n",
    "    Architecture:\n",
    "    - Layer 1: Processes raw features (13+21) → 512\n",
    "    - Layer 2: 512 → 1024 (expansion)\n",
    "    - Layer 3: 1024 → 512 (compression)\n",
    "    - Global sum pooling + L2 normalization\n",
    "    \n",
    "    Output: 512-dimensional protein embedding\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GNN_First_Layer(filters=512)\n",
    "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
    "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
    "        self.dense = Dense(in_dims=512, out_dims=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass through three GNN layers\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        \n",
    "        # Extract features (ignore neighbor indices)\n",
    "        x = x3[0]\n",
    "        \n",
    "        # Global sum pooling: aggregate all atom representations\n",
    "        x = torch.sum(x, axis=0).view(1, -1)\n",
    "        \n",
    "        # L2 normalization: normalize to unit vector\n",
    "        x = F.normalize(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def processProtein(UniqueProtein, PdBloc):\n",
    "        \"\"\"\n",
    "        Processes PDB files into graph representations.\n",
    "        \n",
    "        Args:\n",
    "            UniqueProtein: List of protein names\n",
    "            PdBloc: Path to directory containing PDB files\n",
    "        \n",
    "        Returns:\n",
    "            PData_dict: Dictionary mapping protein names to graph data\n",
    "                       [one_hot_atom, one_hot_res, neigh_same_res, neigh_diff_res]\n",
    "        \"\"\"\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        PData_dict = {}\n",
    "        \n",
    "        for i in range(len(UniqueProtein)):\n",
    "            UniqueProtein[i] = UniqueProtein[i].split('.pdb')[0]\n",
    "            P1 = PdBloc + UniqueProtein[i] + '.pdb'\n",
    "            \n",
    "            # Parse PDB file\n",
    "            parser = PDBParser()\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                structure = parser.get_structure(\"\", P1)\n",
    "            \n",
    "            # Extract features\n",
    "            one_hot_atom = atom1(structure)\n",
    "            one_hot_res = res1(structure)\n",
    "            neigh_same_res, neigh_diff_res = neigh1(structure)\n",
    "            \n",
    "            # Convert to tensors and move to device\n",
    "            one_hot_atom = torch.tensor(one_hot_atom, dtype=torch.float32).to(device)\n",
    "            one_hot_res = torch.tensor(one_hot_res, dtype=torch.float32).to(device)\n",
    "            neigh_same_res = torch.tensor(neigh_same_res).to(device).long()\n",
    "            neigh_diff_res = torch.tensor(neigh_diff_res).to(device).long()\n",
    "            \n",
    "            # Store as list\n",
    "            GNNData = [one_hot_atom, one_hot_res, neigh_same_res, neigh_diff_res]\n",
    "            PData_dict[UniqueProtein[i]] = GNNData\n",
    "            \n",
    "        return PData_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPPI_MLP_Net\n",
    "\n",
    "**Purpose**: Multi-layer perceptron that combines all features for final prediction.\n",
    "\n",
    "**Inputs**:\n",
    "- `PFeatures`: GNN embedding of protein (512-dim)\n",
    "- `ProteinInterfaceF`: Hand-crafted interface features (varies)\n",
    "- `LigandFeatures`: Compound fingerprints (2048-dim typically)\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Concatenate: [GNN_features | Interface_features | Compound_features] → 2840\n",
    "  ↓\n",
    "FC1: 2840 → 1024 (tanh)\n",
    "  ↓\n",
    "FC2: 1024 → 512 (tanh)\n",
    "  ↓\n",
    "FC3: 512 → 100 (relu)\n",
    "  ↓\n",
    "FC4: 100 → 1 (linear, for BCE loss)\n",
    "  ↓\n",
    "Output: Logit score (inhibitor probability after sigmoid)\n",
    "```\n",
    "\n",
    "**Why Concatenate?**\n",
    "- GNN features: Learned structural patterns\n",
    "- Interface features: Domain knowledge about binding sites\n",
    "- Compound features: Chemical properties\n",
    "- MLP learns how to combine these complementary information sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPPI_MLP_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer perceptron for final inhibition prediction.\n",
    "    Combines GNN protein features, interface features, and compound features.\n",
    "    \n",
    "    Input: Concatenated features (total 2840-dim)\n",
    "    Output: Logit score for binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(IPPI_MLP_Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2840, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 100)\n",
    "        self.fc6 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, PFeatures, LigandFeatures, ProteinInterfaceF):\n",
    "        # Concatenate all features\n",
    "        P_all_Features = torch.hstack((PFeatures, ProteinInterfaceF))\n",
    "        PC_Features = torch.hstack((P_all_Features, LigandFeatures))\n",
    "        \n",
    "        # Pass through MLP\n",
    "        x = torch.tanh(self.fc1(PC_Features))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc6(x)  # Output logits\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Pipeline {#training}\n",
    "\n",
    "### Training Strategy: Leave-One-Complex-Out Cross-Validation (LOCO-CV)\n",
    "\n",
    "**Why LOCO-CV?**\n",
    "- Standard CV would leak information (compounds tested on same protein)\n",
    "- Real use case: predict for new protein complexes\n",
    "- LOCO-CV tests generalization to unseen protein structures\n",
    "\n",
    "**Process**:\n",
    "1. Group all examples by protein complex\n",
    "2. For each complex:\n",
    "   - Train on all other complexes\n",
    "   - Test on this complex\n",
    "3. Aggregate results across all folds\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Data Loading**:\n",
    "   - Protein structures (pre-processed as graphs)\n",
    "   - Interface features (pre-computed)\n",
    "   - Compound fingerprints (pre-computed)\n",
    "   - Labels (inhibitor: 1, non-inhibitor: 0)\n",
    "\n",
    "2. **Balanced Sampling**:\n",
    "   - Uses BinaryBalancedSampler\n",
    "   - Each batch has 50% positives, 50% negatives\n",
    "\n",
    "3. **Class Weighting**:\n",
    "   - Additional weighting in loss function\n",
    "   - Accounts for different imbalance ratios across complexes\n",
    "\n",
    "4. **Optimization**:\n",
    "   - Adam optimizer (learning rate: 0.001)\n",
    "   - BCE with Logits Loss\n",
    "   - 5 epochs per fold\n",
    "\n",
    "5. **Model Selection**:\n",
    "   - Tracks best AUCROC on test set\n",
    "   - Saves best model per fold\n",
    "   - Early stopping if no improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Structure\n",
    "\n",
    "```python\n",
    "for each protein complex:\n",
    "    # Split data\n",
    "    train_data = all complexes except this one\n",
    "    test_data = this complex\n",
    "    \n",
    "    # Standardize features\n",
    "    fit scalers on train_data\n",
    "    transform train and test data\n",
    "    \n",
    "    # Initialize models\n",
    "    GNN_model = GNN()\n",
    "    IPPI_Net = IPPI_MLP_Net()\n",
    "    \n",
    "    # Create balanced data loader\n",
    "    loader = BinaryBalancedSampler(train_labels)\n",
    "    \n",
    "    best_aucroc = 0\n",
    "    for epoch in range(5):\n",
    "        for batch in loader:\n",
    "            # Training step\n",
    "            1. Pass unique proteins through GNN (avoid redundant computation)\n",
    "            2. Gather features for batch examples\n",
    "            3. Pass through IPPI_Net\n",
    "            4. Calculate loss\n",
    "            5. Backpropagate\n",
    "            \n",
    "            # Validation step (every iteration)\n",
    "            1. Evaluate on test set\n",
    "            2. Calculate AUCROC\n",
    "            3. If best so far, save model\n",
    "    \n",
    "    # Load best model and save predictions\n",
    "    save final scores and labels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Results {#evaluation}\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Two primary metrics are used:\n",
    "\n",
    "1. **AUCROC (Area Under Receiver Operating Characteristic)**\n",
    "   - Measures ability to rank inhibitors higher than non-inhibitors\n",
    "   - Range: 0.5 (random) to 1.0 (perfect)\n",
    "   - Good for overall classification performance\n",
    "\n",
    "2. **AUCPR (Area Under Precision-Recall Curve)**\n",
    "   - More informative for imbalanced datasets\n",
    "   - Focuses on performance on positive class\n",
    "   - Better reflects real-world utility\n",
    "\n",
    "### Reported Performance\n",
    "\n",
    "The pipeline achieves competitive performance across multiple protein complexes:\n",
    "\n",
    "| Complex | AUCROC | Notes |\n",
    "|---------|--------|-------|\n",
    "| 2XA0 | 0.59 | Challenging case |\n",
    "| 3WN7 | 0.92 | Strong performance |\n",
    "| 3UVW | 0.85 | Good performance |\n",
    "| 1YCR | 0.81 | Good performance |\n",
    "| 4ESG | 0.93 | Strong performance |\n",
    "| 3D9T | 0.88 | Strong performance |\n",
    "| 2FLU | 0.95 | Excellent performance |\n",
    "| 4QC3 | 0.93 | Strong performance |\n",
    "\n",
    "**Average AUCROC**: 0.86 ± 0.09\n",
    "**Average AUCPR**: 0.44 ± 0.20\n",
    "\n",
    "### Comparison with Baselines\n",
    "\n",
    "The notebook includes comparison code for:\n",
    "- **SVM baseline**: Uses hand-crafted features only\n",
    "- **GearNet**: Alternative protein structure encoder\n",
    "- **GNN pipeline** (this model): Combines GNN with interface features\n",
    "\n",
    "The GNN-based pipeline shows improved performance over SVM baseline, demonstrating the value of learned structural representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Problem**: Predict if small molecules can inhibit protein-protein interactions\n",
    "\n",
    "2. **Solution**: Graph Neural Network on 3D protein structures\n",
    "   - Proteins → graphs (atoms as nodes, spatial proximity as edges)\n",
    "   - GNN learns structural patterns predictive of inhibition\n",
    "   - Combines with compound fingerprints via MLP\n",
    "\n",
    "3. **Technical Innovations**:\n",
    "   - Two edge types (same/different residue) with separate weights\n",
    "   - Balanced batch sampling for imbalanced data\n",
    "   - LOCO-CV for realistic evaluation\n",
    "\n",
    "4. **Performance**: \n",
    "   - AUCROC ~0.86 across diverse protein complexes\n",
    "   - Outperforms traditional ML baseline\n",
    "\n",
    "### Potential Improvements\n",
    "\n",
    "1. **Architecture**:\n",
    "   - Attention mechanisms for neighbor aggregation\n",
    "   - Residual connections for deeper networks\n",
    "   - Edge features (distance, bond types)\n",
    "\n",
    "2. **Features**:\n",
    "   - Physicochemical properties (charge, hydrophobicity)\n",
    "   - Secondary structure information\n",
    "   - Evolutionary conservation scores\n",
    "\n",
    "3. **Training**:\n",
    "   - Data augmentation (rotations, perturbations)\n",
    "   - Transfer learning from large protein datasets\n",
    "   - Ensemble methods\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - External test set (temporally split)\n",
    "   - Analysis of failure cases\n",
    "   - Interpretability studies (which atoms matter?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Original Repository**: https://github.com/adibayaseen/PPI-Inhibitors\n",
    "\n",
    "**Key Libraries**:\n",
    "- BioPython: Cock et al., Bioinformatics 2009\n",
    "- RDKit: https://www.rdkit.org/\n",
    "- PyTorch: Paszke et al., NeurIPS 2019\n",
    "\n",
    "**Related Work on GNNs for Proteins**:\n",
    "- Graph Neural Networks for protein structure (various papers)\n",
    "- Message passing neural networks\n",
    "- Geometric deep learning on molecular graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
