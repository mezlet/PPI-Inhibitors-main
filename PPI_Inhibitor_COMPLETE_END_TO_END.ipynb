{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mezlet/PPI-Inhibitors-main/blob/main/PPI_Inhibitor_COMPLETE_END_TO_END.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS8ykcg8KYBV"
      },
      "source": [
        "# Complete PPI Inhibitor Prediction - Single Notebook\n",
        "## ALL Components Included\n",
        "\n",
        "**Paper:** Yaseen et al. \"Predicting small-molecule inhibition of protein complexes\"\n",
        "\n",
        "**GitHub:** https://github.com/adibayaseen/PPI-Inhibitors\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… This notebook includes EVERYTHING:\n",
        "- All corrected functions (atom1, res1, neigh1)\n",
        "- Complete GNN architecture  \n",
        "- Dense class\n",
        "- BalancedDataset & create_balanced_loader\n",
        "- BinaryBalancedSampler\n",
        "- readFile & processProtein utilities\n",
        "- Full LOCO cross-validation\n",
        "- Visualization\n",
        "\n",
        "### ğŸ¯ Expected Results:\n",
        "- **Mean AUC-ROC:** 0.86 Â± 0.09\n",
        "- **Mean AUC-PR:** 0.39 Â± 0.24\n",
        "\n",
        "### â±ï¸ Runtime:\n",
        "- Setup: 5-10 minutes\n",
        "- Full training (23 complexes): 12-24 hours on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAxeS3V5KYBX",
        "outputId": "05f99f3d-7c36-4914-b348-04d862afe045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Running in Google Colab\n",
            "âš ï¸  Make sure Runtime -> Change runtime type -> GPU is enabled\n"
          ]
        }
      ],
      "source": [
        "# Environment check\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"âœ“ Running in Google Colab\")\n",
        "    print(\"âš ï¸  Make sure Runtime -> Change runtime type -> GPU is enabled\")\n",
        "else:\n",
        "    print(\"âœ“ Running locally\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hS242rJKKYBX",
        "outputId": "23910f13-9686-4d25-8ee3-06349ae51b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "328e89a0682a4fc6a2b86e4a27d3f5b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting biopython==1.81\n",
            "  Downloading biopython-1.81-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision==0.17.2\n",
            "  Downloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio==2.2.2\n",
            "  Downloading torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting torch-geometric==2.5.3\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "Collecting tqdm==4.66.2\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting pandas==2.1.4\n",
            "  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scikit-learn==1.3.2\n",
            "  Downloading scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.8.3\n",
            "  Downloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting networkx==3.2.1\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.2) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.2) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (1.16.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (2.32.4)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (3.2.5)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric==2.5.3) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.3) (25.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.6.85)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.1.4) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric==2.5.3) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric==2.5.3) (3.11)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.2) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric==2.5.3) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading biopython-1.81-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.2.2-cp312-cp312-manylinux1_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m186.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading rdkit-2025.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (36.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tqdm, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, rdkit, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, biopython, torch, scikit-learn, matplotlib, torchvision, torchaudio, torch-geometric\n",
            "\u001b[2K  Attempting uninstall: tqdm\n",
            "\u001b[2K    Found existing installation: tqdm 4.67.1\n",
            "\u001b[2K    Uninstalling tqdm-4.67.1:\n",
            "\u001b[2K      Successfully uninstalled tqdm-4.67.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K  Attempting uninstall: networkx\n",
            "\u001b[2K    Found existing installation: networkx 3.5\n",
            "\u001b[2K    Uninstalling networkx-3.5:\n",
            "\u001b[2K      Successfully uninstalled networkx-3.5\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torch-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[2K  Attempting uninstall: scikit-learn\n",
            "\u001b[2K    Found existing installation: scikit-learn 1.6.1\n",
            "\u001b[2K    Uninstalling scikit-learn-1.6.1:\n",
            "\u001b[2K      Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.23.0+cu126\n",
            "\u001b[2K    Uninstalling torchvision-0.23.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[2K  Attempting uninstall: torchaudio\n",
            "\u001b[2K    Found existing installation: torchaudio 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torchaudio-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/23\u001b[0m [torch-geometric]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "imbalanced-learn 0.14.0 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "datasets 4.0.0 requires tqdm>=4.66.3, but you have tqdm 4.66.2 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
            "xarray 2025.10.1 requires pandas>=2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.2 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed biopython-1.81 matplotlib-3.8.3 networkx-3.2.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 pandas-2.1.4 rdkit-2025.9.1 scikit-learn-1.3.2 torch-2.2.2 torch-geometric-2.5.3 torchaudio-2.2.2 torchvision-0.17.2 tqdm-4.66.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "ec60ed1281d04b659028609e9f03db70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Packages installed\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# !pip install -q --upgrade pip setuptools wheel\n",
        "# !pip install -q rdkit biopython==1.81 torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 \\\n",
        "#               torch-geometric==2.5.3 tqdm==4.66.2 pandas==2.1.4 numpy==1.26.4 \\\n",
        "#               scikit-learn==1.3.2 matplotlib==3.8.3 seaborn==0.13.2 networkx==3.2.1 gdown\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install rdkit biopython==1.81 torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 torch-geometric==2.5.3 tqdm==4.66.2 pandas==2.1.4 numpy==1.26.4 scikit-learn==1.3.2 matplotlib==3.8.3 seaborn==0.13.2 networkx==3.2.1 gdown\n",
        "\n",
        "print(\"\\nâœ“ Packages installed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov4Kk4mJKYBY",
        "outputId": "d6f7b06d-25cc-435e-9a9a-1a560634f469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PPI-Inhibitors'...\n",
            "remote: Enumerating objects: 1341, done.\u001b[K\n",
            "remote: Counting objects: 100% (536/536), done.\u001b[K\n",
            "remote: Compressing objects: 100% (404/404), done.\u001b[K\n",
            "remote: Total 1341 (delta 223), reused 395 (delta 129), pack-reused 805 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1341/1341), 2.59 GiB | 35.08 MiB/s, done.\n",
            "Resolving deltas: 100% (405/405), done.\n",
            "Updating files: 100% (605/605), done.\n",
            "/content/PPI-Inhibitors\n",
            "/content/PPI-Inhibitors/Data\n",
            "/content/PPI-Inhibitors\n",
            "\n",
            "âœ“ Repository cloned and datasets downloaded\n"
          ]
        }
      ],
      "source": [
        "# Clone repository and download data\n",
        "!rm -rf PPI-Inhibitors\n",
        "!git clone https://github.com/adibayaseen/PPI-Inhibitors\n",
        "%cd PPI-Inhibitors\n",
        "\n",
        "# Download datasets\n",
        "!mkdir -p Data\n",
        "%cd Data\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iComplexPairs.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/01ad4975fb9133825b1bf9e71b64fcdaaa5e4d8b/Data/2p2iInhibitorsSMILES.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/2d6bd03422602ec19147870c487e64018b52660f/Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt\n",
        "!wget -q https://github.com/adibayaseen/PPI-Inhibitors/raw/b1e45884f61f792399abad2e4492f48083ab1093/Data/BindersWithComplexname.csv\n",
        "%cd ..\n",
        "\n",
        "print(\"\\nâœ“ Repository cloned and datasets downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WyuCJh5KYBY",
        "outputId": "91e1277b-afba-485c-a37a-bf161ef8ae28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "Downloading pre-computed protein features (this may take a few minutes)...\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW\n",
            "To: /content/drive/MyDrive/GNN-PPI-Inhibitor/ProteinData_dict.pickle\n",
            "100% 34.6M/34.6M [00:00<00:00, 35.2MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE\n",
            "From (redirected): https://drive.google.com/uc?id=1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE&confirm=t&uuid=ea965873-28b7-4e50-b32a-1439f88c132c\n",
            "To: /content/drive/MyDrive/GNN-PPI-Inhibitor/DBD5_ProteinData_dict.pickle\n",
            "100% 336M/336M [00:02<00:00, 143MB/s]\n",
            "\n",
            "âœ“ Features downloaded\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and download pre-computed features\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd '/content/drive/MyDrive'\n",
        "    !mkdir -p GNN-PPI-Inhibitor\n",
        "\n",
        "    print(\"Downloading pre-computed protein features (this may take a few minutes)...\")\n",
        "    !gdown --id 1goeDiPZSKT1Xx3j00eNG9xlqYkLLv1gW -O GNN-PPI-Inhibitor/ProteinData_dict.pickle\n",
        "    !gdown --id 1GOYEKLQCoGea9QQ72kujy0rdJKbUSYAE -O GNN-PPI-Inhibitor/DBD5_ProteinData_dict.pickle\n",
        "\n",
        "    print(\"\\nâœ“ Features downloaded\")\n",
        "else:\n",
        "    print(\"âš ï¸  Running locally - make sure pre-computed features are available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDmyEeZMKYBY"
      },
      "source": [
        "---\n",
        "## 2. Imports and CUDA Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99yse1MtKYBY",
        "outputId": "b7a30b0e-e738-4f5f-a5ea-1ab7f69424bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import warnings\n",
        "from Bio.PDB import *\n",
        "from Bio.PDB.NeighborSearch import NeighborSearch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from torch.utils.data.sampler import WeightedRandomSampler\n",
        "from torch.autograd import Variable\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score, precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import DataStructs\n",
        "from scipy import spatial\n",
        "from Bio import SeqIO\n",
        "from Bio.SeqIO import FastaIO\n",
        "from itertools import product\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "from sklearn.preprocessing import normalize\n",
        "import math\n",
        "from Bio.Data import IUPACData\n",
        "from Bio.PDB.Polypeptide import *\n",
        "\n",
        "# CUDA setup\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")\n",
        "\n",
        "def cuda(v):\n",
        "    \"\"\"Move tensor to CUDA if available\"\"\"\n",
        "    if USE_CUDA:\n",
        "        return v.cuda()\n",
        "    return v\n",
        "\n",
        "def toTensor(v, dtype=torch.float, requires_grad=False):\n",
        "    \"\"\"Convert to tensor and move to device\"\"\"\n",
        "    return cuda(Variable(torch.tensor(v)).type(dtype).requires_grad_(requires_grad))\n",
        "\n",
        "def toNumpy(v):\n",
        "    \"\"\"Convert tensor to numpy\"\"\"\n",
        "    if USE_CUDA:\n",
        "        return v.detach().cpu().numpy()\n",
        "    return v.detach().numpy()\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "if USE_CUDA:\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected. Training will be very slow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "732Vez3gKYBY"
      },
      "source": [
        "---\n",
        "## 3. Feature Extraction Functions\n",
        "\n",
        "### CORRECTED implementations from repository:\n",
        "- `atom1()`: 13-dim encoding\n",
        "- `res1()`: 21-dim encoding\n",
        "- `neigh1()`: **THE KEY** - heterogeneous neighbor lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO7xwAwvKYBY",
        "outputId": "250867c5-c179-4185-9ce0-ef273b74e197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ atom1() defined (13-dim)\n"
          ]
        }
      ],
      "source": [
        "def atom1(structure):\n",
        "    \"\"\"\n",
        "    One-hot encode atoms - 13 dimensions.\n",
        "\n",
        "    Categories: ['C', 'CA', 'CB', 'CG', 'CH2', 'N', 'NH2', 'OG', 'OH', 'O1', 'O2', 'SE', '1']\n",
        "    '1' = unknown atom type\n",
        "    \"\"\"\n",
        "    atomslist = np.array(sorted(np.array(['C', 'CA', 'CB', 'CG', 'CH2', 'N','NH2',  'OG','OH', 'O1', 'O2', 'SE','1']))).reshape(-1,1)\n",
        "    enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    enc.fit(atomslist)\n",
        "\n",
        "    atom_list = []\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_name() in atomslist:\n",
        "            atom_list.append(atom.get_name())\n",
        "        else:\n",
        "            atom_list.append(\"1\")\n",
        "\n",
        "    atoms_onehot = enc.transform(np.array(atom_list).reshape(-1,1)).toarray()\n",
        "    return atoms_onehot\n",
        "\n",
        "print(\"âœ“ atom1() defined (13-dim)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkoVHLokKYBY",
        "outputId": "898ae61d-55d5-4b9b-fae2-5114fc37d27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ res1() defined (21-dim)\n"
          ]
        }
      ],
      "source": [
        "def res1(structure):\n",
        "    \"\"\"\n",
        "    One-hot encode residues - 21 dimensions.\n",
        "\n",
        "    20 amino acids + '1' for unknown\n",
        "    \"\"\"\n",
        "    residuelist = np.array(sorted(np.array(['ALA', 'ARG', 'ASN', 'ASP', 'GLN', 'GLU', 'GLY', 'ILE', 'LEU', 'LYS',\n",
        "                                            'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'CYS', 'HIS','1']))).reshape(-1,1)\n",
        "    encr = OneHotEncoder(handle_unknown='ignore')\n",
        "    encr.fit(residuelist)\n",
        "\n",
        "    residue_list = []\n",
        "    for atom in structure.get_atoms():\n",
        "        if atom.get_parent().get_resname() in residuelist:\n",
        "            residue_list.append((atom.get_parent()).get_resname())\n",
        "        else:\n",
        "            residue_list.append(\"1\")\n",
        "\n",
        "    res_onehot = encr.transform(np.array(residue_list).reshape(-1,1)).toarray()\n",
        "    return res_onehot\n",
        "\n",
        "print(\"âœ“ res1() defined (21-dim)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaGk5-fiKYBZ",
        "outputId": "2772c1c2-54eb-4715-cc0a-00ca1242ed88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ neigh1() defined - Creates heterogeneous neighbor lists\n",
            "  This is THE KEY innovation of the paper!\n"
          ]
        }
      ],
      "source": [
        "def neigh1(structure):\n",
        "    \"\"\"\n",
        "    Calculate heterogeneous neighbors - THE KEY INNOVATION!\n",
        "\n",
        "    Returns TWO separate neighbor lists:\n",
        "    - neigh_same_res: 10 neighbors within SAME residue\n",
        "    - neigh_diff_res: 10 neighbors in DIFFERENT residues\n",
        "\n",
        "    This is what makes the GNN \"heterogeneous\".\n",
        "    \"\"\"\n",
        "    atom_list = np.array([atom for atom in structure.get_atoms()])\n",
        "\n",
        "    # Find neighbors within 6 Angstroms\n",
        "    p4 = NeighborSearch(atom_list)\n",
        "    neighbour_list = p4.search_all(6, level=\"A\")\n",
        "    neighbour_list = np.array(neighbour_list)\n",
        "\n",
        "    # Sort by distance\n",
        "    dist = np.array([np.linalg.norm(neighbour_list[i,0].get_coord() - neighbour_list[i,1].get_coord())\n",
        "                     for i in range(len(neighbour_list))])\n",
        "    place = np.argsort(dist)\n",
        "    sorted_neighbour_list = neighbour_list[place]\n",
        "\n",
        "    # Get atom and residue IDs\n",
        "    source_vertex_list_atom_object = np.array(sorted_neighbour_list[:,0])\n",
        "    len_source_vertex = len(source_vertex_list_atom_object)\n",
        "    neighbour_vertex_with_respect_each_source_atom_object = np.array(sorted_neighbour_list[:,1])\n",
        "\n",
        "    old_atom_number = []\n",
        "    old_residue_number = []\n",
        "    for i in atom_list:\n",
        "        old_atom_number.append(i.get_serial_number())\n",
        "        old_residue_number.append(i.get_parent().get_id()[1])\n",
        "\n",
        "    old_atom_number = np.array(old_atom_number)\n",
        "    old_residue_number = np.array(old_residue_number)\n",
        "\n",
        "    total_atoms = len(atom_list)\n",
        "\n",
        "    # Initialize neighbor arrays (-1 = no neighbor)\n",
        "    neigh_same_res = np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    neigh_diff_res = np.array([[-1]*10 for i in range(total_atoms)])\n",
        "    same_flag = [0]*total_atoms\n",
        "    diff_flag = [0]*total_atoms\n",
        "\n",
        "    # Fill neighbor arrays\n",
        "    for i in range(len_source_vertex):\n",
        "        source_atom_id = source_vertex_list_atom_object[i].get_serial_number()\n",
        "        neigh_atom_id = neighbour_vertex_with_respect_each_source_atom_object[i].get_serial_number()\n",
        "        source_atom_res = source_vertex_list_atom_object[i].get_parent().get_id()[1]\n",
        "        neigh_atom_res = neighbour_vertex_with_respect_each_source_atom_object[i].get_parent().get_id()[1]\n",
        "\n",
        "        # Find indices\n",
        "        temp_index1 = np.where(source_atom_id == old_atom_number)[0]\n",
        "        temp_index2 = np.where(neigh_atom_id == old_atom_number)[0]\n",
        "\n",
        "        for i1 in temp_index1:\n",
        "            if old_residue_number[i1] == source_atom_res:\n",
        "                source_index = i1\n",
        "                break\n",
        "        for i1 in temp_index2:\n",
        "            if old_residue_number[i1] == neigh_atom_res:\n",
        "                neigh_index = i1\n",
        "                break\n",
        "\n",
        "        # Same residue neighbors\n",
        "        if source_atom_res == neigh_atom_res:\n",
        "            if int(same_flag[source_index]) < 10:\n",
        "                neigh_same_res[source_index][same_flag[source_index]] = neigh_index\n",
        "                same_flag[source_index] += 1\n",
        "\n",
        "            if int(same_flag[neigh_index]) < 10:\n",
        "                neigh_same_res[neigh_index][same_flag[neigh_index]] = source_index\n",
        "                same_flag[neigh_index] += 1\n",
        "\n",
        "        # Different residue neighbors\n",
        "        elif source_atom_res != neigh_atom_res:\n",
        "            if int(diff_flag[source_index]) < 10:\n",
        "                neigh_diff_res[source_index][diff_flag[source_index]] = neigh_index\n",
        "                diff_flag[source_index] += 1\n",
        "\n",
        "            if int(diff_flag[neigh_index]) < 10:\n",
        "                neigh_diff_res[neigh_index][diff_flag[neigh_index]] = source_index\n",
        "                diff_flag[neigh_index] += 1\n",
        "\n",
        "    return neigh_same_res, neigh_diff_res\n",
        "\n",
        "print(\"âœ“ neigh1() defined - Creates heterogeneous neighbor lists\")\n",
        "print(\"  This is THE KEY innovation of the paper!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEQA7YhZKYBZ"
      },
      "source": [
        "### Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoTDtA1PKYBZ",
        "outputId": "12be34ce-9add-48ba-f7cf-cd608c4db3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Utility functions defined\n"
          ]
        }
      ],
      "source": [
        "def readFile(filename):\n",
        "    \"\"\"\n",
        "    Read inhibitor data file.\n",
        "    Format: name pdbid complex ligandid smiles label\n",
        "    \"\"\"\n",
        "    with open(filename) as f:\n",
        "        D = f.readlines()\n",
        "\n",
        "    Name, PdbId, Ligandnames, SMILES, labels = [], [], [], [], []\n",
        "\n",
        "    for d in tqdm(D, desc=\"Reading file\"):\n",
        "        if len(d.split()) == 6:\n",
        "            name, inhibtedc, Pdbid, Ligandid, smiles, y = d.split()\n",
        "            Name.append(name)\n",
        "            PdbId.append(Pdbid)\n",
        "            Ligandnames.append(Ligandid)\n",
        "            SMILES.append(smiles)\n",
        "            labels.append(float(y))\n",
        "\n",
        "    return PdbId, Ligandnames, SMILES, labels\n",
        "\n",
        "\n",
        "def processProtein(UniqueProtein, PdBloc):\n",
        "    \"\"\"\n",
        "    Process PDB files to graph representation.\n",
        "    Only needed if processing NEW proteins (not using pre-computed features).\n",
        "    \"\"\"\n",
        "    PData_dict = {}\n",
        "\n",
        "    for i in range(len(UniqueProtein)):\n",
        "        print(f'Converting PDB to Graph: {i+1}/{len(UniqueProtein)}')\n",
        "        UniqueProtein[i] = UniqueProtein[i].split('.pdb')[0]\n",
        "        P1 = PdBloc + UniqueProtein[i] + '.pdb'\n",
        "\n",
        "        parser = PDBParser()\n",
        "        with warnings.catch_warnings(record=True):\n",
        "            structure = parser.get_structure(\"\", P1)\n",
        "\n",
        "        one_hot_atom = atom1(structure)\n",
        "        one_hot_res = res1(structure)\n",
        "        neigh_same_res, neigh_diff_res = neigh1(structure)\n",
        "\n",
        "        one_hot_atom = torch.tensor(one_hot_atom, dtype=torch.float32).to(device)\n",
        "        one_hot_res = torch.tensor(one_hot_res, dtype=torch.float32).to(device)\n",
        "        neigh_same_res = torch.tensor(neigh_same_res).to(device).long()\n",
        "        neigh_diff_res = torch.tensor(neigh_diff_res).to(device).long()\n",
        "\n",
        "        GNNData = [one_hot_atom, one_hot_res, neigh_same_res, neigh_diff_res]\n",
        "        PData_dict[UniqueProtein[i]] = GNNData\n",
        "\n",
        "    return PData_dict\n",
        "\n",
        "print(\"âœ“ Utility functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msahN7tOKYBZ"
      },
      "source": [
        "---\n",
        "## 4. Model Architecture\n",
        "\n",
        "All classes from repository included."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed2myfXjKYBZ",
        "outputId": "d29a1bcc-9973-40c2-e51a-409008dd2d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ GNN_First_Layer defined\n"
          ]
        }
      ],
      "source": [
        "class GNN_First_Layer(nn.Module):\n",
        "    \"\"\"\n",
        "    First GNN layer - processes atom and residue features.\n",
        "    Weight matrices: Wv (atoms), Wr (residues), Wsr (same-res), Wdr (diff-res)\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, trainable=True, **kwargs):\n",
        "        super(GNN_First_Layer, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.trainable = trainable\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.Wv = nn.Parameter(torch.randn(13, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.Wr = nn.Parameter(torch.randn(21, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.Wsr = nn.Parameter(torch.randn(13, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.Wdr = nn.Parameter(torch.randn(13, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.neighbours = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        atoms, residues, same_neigh, diff_neigh = x\n",
        "\n",
        "        node_signals = atoms @ self.Wv\n",
        "        residue_signals = residues @ self.Wr\n",
        "        neigh_signals_same = atoms @ self.Wsr\n",
        "        neigh_signals_diff = atoms @ self.Wdr\n",
        "\n",
        "        unsqueezed_same_neigh_indicator = (same_neigh > -1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator = (diff_neigh > -1).unsqueeze(2)\n",
        "\n",
        "        same_neigh_features = neigh_signals_same[same_neigh] * unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features = neigh_signals_diff[diff_neigh] * unsqueezed_diff_neigh_indicator\n",
        "\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        same_norm[same_norm == 0] = 1\n",
        "        diff_norm[diff_norm == 0] = 1\n",
        "\n",
        "        neigh_same_atoms_signal = torch.sum(same_neigh_features, axis=1) / same_norm\n",
        "        neigh_diff_atoms_signal = torch.sum(diff_neigh_features, axis=1) / diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals + residue_signals + neigh_same_atoms_signal + neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res, same_neigh, diff_neigh\n",
        "\n",
        "print(\"âœ“ GNN_First_Layer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDerdOEsKYBa",
        "outputId": "4512be7d-9259-48b7-df7e-97fe05c1e496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ GNN_Layer defined\n"
          ]
        }
      ],
      "source": [
        "class GNN_Layer(nn.Module):\n",
        "    \"\"\"\n",
        "    Subsequent GNN layers.\n",
        "    Weight matrices: Wsv (center), Wsr (same-res), Wdr (diff-res)\n",
        "    \"\"\"\n",
        "    def __init__(self, filters, v_feats, trainable=True, **kwargs):\n",
        "        super(GNN_Layer, self).__init__()\n",
        "        self.v_feats = v_feats\n",
        "        self.filters = filters\n",
        "        self.trainable = trainable\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.Wsv = nn.Parameter(torch.randn(self.v_feats, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.Wdr = nn.Parameter(torch.randn(self.v_feats, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.Wsr = nn.Parameter(torch.randn(self.v_feats, self.filters, device=self.cuda_device, requires_grad=True))\n",
        "        self.neighbours = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        Z, same_neigh, diff_neigh = x\n",
        "\n",
        "        node_signals = Z @ self.Wsv\n",
        "        neigh_signals_same = Z @ self.Wsr\n",
        "        neigh_signals_diff = Z @ self.Wdr\n",
        "\n",
        "        unsqueezed_same_neigh_indicator = (same_neigh > -1).unsqueeze(2)\n",
        "        unsqueezed_diff_neigh_indicator = (diff_neigh > -1).unsqueeze(2)\n",
        "\n",
        "        same_neigh_features = neigh_signals_same[same_neigh] * unsqueezed_same_neigh_indicator\n",
        "        diff_neigh_features = neigh_signals_diff[diff_neigh] * unsqueezed_diff_neigh_indicator\n",
        "\n",
        "        same_norm = torch.sum(same_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "        diff_norm = torch.sum(diff_neigh > -1, 1).unsqueeze(1).type(torch.float)\n",
        "\n",
        "        same_norm[same_norm == 0] = 1\n",
        "        diff_norm[diff_norm == 0] = 1\n",
        "\n",
        "        neigh_same_atoms_signal = torch.sum(same_neigh_features, axis=1) / same_norm\n",
        "        neigh_diff_atoms_signal = torch.sum(diff_neigh_features, axis=1) / diff_norm\n",
        "\n",
        "        final_res = torch.relu(node_signals + neigh_same_atoms_signal + neigh_diff_atoms_signal)\n",
        "\n",
        "        return final_res, same_neigh, diff_neigh\n",
        "\n",
        "print(\"âœ“ GNN_Layer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVoC4-_aKYBa",
        "outputId": "105f0bb5-b967-40f6-cb5c-6f106b673790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Dense class defined\n"
          ]
        }
      ],
      "source": [
        "class Dense(nn.Module):\n",
        "    \"\"\"\n",
        "    Dense layer with sigmoid activation.\n",
        "    (Defined in repository but not used in final model - included for completeness)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dims, out_dims, trainable=True, **kwargs):\n",
        "        super(Dense, self).__init__()\n",
        "        self.in_dims = in_dims\n",
        "        self.out_dims = out_dims\n",
        "        self.cuda_device = device\n",
        "\n",
        "        self.W = nn.Parameter(torch.randn(self.in_dims, self.out_dims,\n",
        "                                         device=self.cuda_device, requires_grad=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        Z = torch.sigmoid(torch.matmul(x, self.W))\n",
        "        return Z\n",
        "\n",
        "print(\"âœ“ Dense class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdE7CRjxKYBa",
        "outputId": "fab26fa8-9fa4-4f75-e8d8-108bcf918cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ GNN model defined\n"
          ]
        }
      ],
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Complete 3-layer GNN.\n",
        "    Architecture: 13+21 â†’ 512 â†’ 1024 â†’ 512 â†’ global mean pool\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GNN_First_Layer(filters=512)\n",
        "        self.conv2 = GNN_Layer(v_feats=512, filters=1024)\n",
        "        self.conv3 = GNN_Layer(v_feats=1024, filters=512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(x1)\n",
        "        x3 = self.conv3(x2)\n",
        "\n",
        "        # Global mean pooling\n",
        "        x = x3[0]\n",
        "        x = torch.sum(x, axis=0).view(1, -1)\n",
        "        x = F.normalize(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"âœ“ GNN model defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjUAr5BcKYBa",
        "outputId": "e7daf021-e4af-440f-fa95-c799e5cb491f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ IPPI_MLP_Net defined\n"
          ]
        }
      ],
      "source": [
        "class IPPI_MLP_Net(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP for final inhibition prediction.\n",
        "    Input: 2840 = 512 (GNN) + 280 (interface+seq) + 2048 (compound)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(IPPI_MLP_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2840, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 100)\n",
        "        self.fc6 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, PFeatures, LigandFeatures, ProteinInterfaceF):\n",
        "        P_all_Features = torch.hstack((PFeatures, ProteinInterfaceF))\n",
        "        PC_Features = torch.hstack((P_all_Features, LigandFeatures))\n",
        "\n",
        "        x = torch.tanh(self.fc1(PC_Features))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc6(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "print(\"âœ“ IPPI_MLP_Net defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rOqbTIyKYBa"
      },
      "source": [
        "---\n",
        "## 5. Dataset & Sampling Classes\n",
        "\n",
        "All sampling methods from repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzuZTW9jKYBa",
        "outputId": "fc7aa8a0-e6ba-4b85-d78f-7ea0ad195382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ CustomDataset defined\n"
          ]
        }
      ],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"Simple dataset wrapper\"\"\"\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "print(\"âœ“ CustomDataset defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f05S9PDhKYBa",
        "outputId": "7c12463a-f96a-4b4f-ad67-5085e16b173d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ BinaryBalancedSampler defined\n"
          ]
        }
      ],
      "source": [
        "class BinaryBalancedSampler(Sampler):\n",
        "    \"\"\"\n",
        "    Returns batches with 50% positive and 50% negative examples.\n",
        "    This is the main sampler used in training.\n",
        "    \"\"\"\n",
        "    def __init__(self, class_vector, batch_size=10):\n",
        "        self.batch_size = batch_size\n",
        "        self.class_vector = class_vector\n",
        "\n",
        "        YY = np.array(self.class_vector)\n",
        "        U, C = np.unique(YY, return_counts=True)\n",
        "        M = U[np.argmax(C)]  # Majority class\n",
        "\n",
        "        Midx = np.nonzero(YY == M)[0]  # Majority indices\n",
        "        midx = np.nonzero(YY != M)[0]  # Minority indices\n",
        "\n",
        "        # Oversample minority to match majority\n",
        "        midx_ = np.random.choice(midx, size=len(Midx))\n",
        "\n",
        "        self.YY = np.array(list(YY[Midx]) + list(YY[midx_]))\n",
        "        self.idx = np.array(list(Midx) + list(midx_))\n",
        "\n",
        "        self.n_splits = int(np.ceil(len(self.idx) / self.batch_size))\n",
        "        self.equivalent_epochs = len(self.idx) / len(self.class_vector)\n",
        "\n",
        "        print(f'Equivalent epochs in one iteration: {self.equivalent_epochs:.2f}')\n",
        "\n",
        "    def gen_sample_array(self):\n",
        "        skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True)\n",
        "        for tridx, ttidx in skf.split(self.idx, self.YY):\n",
        "            yield np.array(self.idx[ttidx])\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.gen_sample_array())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_splits\n",
        "\n",
        "print(\"âœ“ BinaryBalancedSampler defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTVdkjD3KYBb",
        "outputId": "c2f14df8-244b-4ed5-e5a1-4e54b43fa111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ BalancedDataset & create_balanced_loader defined (alternatives)\n"
          ]
        }
      ],
      "source": [
        "class BalancedDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Alternative balanced dataset using sample weights.\n",
        "    (BinaryBalancedSampler is used in main training - this is an alternative)\n",
        "    \"\"\"\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "        class_counts = np.bincount(self.labels)\n",
        "        weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "        self.sample_weights = weights[labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "def create_balanced_loader(data, labels, batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates DataLoader with balanced batches using WeightedRandomSampler.\n",
        "    Alternative to BinaryBalancedSampler.\n",
        "    \"\"\"\n",
        "    dataset = BalancedDataset(data, labels)\n",
        "\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=dataset.sample_weights,\n",
        "        num_samples=len(dataset.sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "    return loader\n",
        "\n",
        "print(\"âœ“ BalancedDataset & create_balanced_loader defined (alternatives)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY8xn-rgKYBb"
      },
      "source": [
        "---\n",
        "## 6. Load Pre-computed Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m8JhTZBKYBb",
        "outputId": "c223d5c2-4a13-4851-8729-a08397e1b758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data path: /content/drive/MyDrive/GNN-PPI-Inhibitor/\n",
            "GitHub path: /content/PPI-Inhibitors/\n"
          ]
        }
      ],
      "source": [
        "# Set paths\n",
        "if IN_COLAB:\n",
        "    path = '/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    githubpath = '/content/PPI-Inhibitors/'\n",
        "else:\n",
        "    path = 'GNN-PPI-Inhibitor/'\n",
        "    githubpath = 'PPI-Inhibitors/'\n",
        "\n",
        "print(f\"Data path: {path}\")\n",
        "print(f\"GitHub path: {githubpath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvUXpdD4KYBb",
        "outputId": "482ea3c8-587f-4a34-aecc-a58bd0b5df08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading interface/sequence features...\n",
            "âœ“ Loaded features for 290 complexes\n",
            "âœ“ Loaded fingerprints for 8868 compounds\n"
          ]
        }
      ],
      "source": [
        "# Load interface and sequence features (280-dim)\n",
        "print(\"Loading interface/sequence features...\")\n",
        "\n",
        "Ubench5InterfaceandSeq_dict = pickle.load(open(githubpath + 'Features/NewUbench5InterfaceandSeq_dict.npy', \"rb\"))\n",
        "Pos_seqandInterfaceF_dict = pickle.load(open(githubpath + 'Features/Pos_seqandInterfaceF_dict.npy', \"rb\"))\n",
        "\n",
        "Complex_AllFeatures_dict = dict(list(Pos_seqandInterfaceF_dict.items()) + list(Ubench5InterfaceandSeq_dict.items()))\n",
        "\n",
        "ComplexInterfaceFeatures = {}\n",
        "for key in Complex_AllFeatures_dict:\n",
        "    if len(key.split('_')) > 1:\n",
        "        compname = key.split('_')[0]\n",
        "        ComplexInterfaceFeatures[compname] = Complex_AllFeatures_dict[key]\n",
        "    else:\n",
        "        ComplexInterfaceFeatures[key] = Complex_AllFeatures_dict[key]\n",
        "\n",
        "print(f\"âœ“ Loaded features for {len(ComplexInterfaceFeatures)} complexes\")\n",
        "\n",
        "# Load compound fingerprints (2048-dim)\n",
        "CompoundFingerprintFeaturesDict = pickle.load(open(githubpath + 'Features/Compound_Fingerprint_Features_Dict.npy', \"rb\"))\n",
        "print(f\"âœ“ Loaded fingerprints for {len(CompoundFingerprintFeaturesDict)} compounds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15WJCdSRKYBb",
        "outputId": "df6b0cad-0d98-443c-be76-7685fbd58893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading protein structures...\n",
            "âœ“ Loaded 290 protein structures\n"
          ]
        }
      ],
      "source": [
        "# Load protein GNN data\n",
        "print(\"Loading protein structures...\")\n",
        "\n",
        "ProteinDataGNN_dict = pickle.load(open(path + 'ProteinData_dict.pickle', \"rb\"))\n",
        "DBD5_ProteinDataGNN_dict = pickle.load(open(path + 'DBD5_ProteinData_dict.pickle', \"rb\"))\n",
        "\n",
        "All_ProteinData_dict = dict(list(ProteinDataGNN_dict.items()) + list(DBD5_ProteinDataGNN_dict.items()))\n",
        "\n",
        "# Move to CUDA\n",
        "for d in All_ProteinData_dict:\n",
        "    data = All_ProteinData_dict[d]\n",
        "    All_ProteinData_dict[d] = [data[0].cuda(), data[1].cuda(), data[2].cuda(), data[3].cuda()]\n",
        "\n",
        "print(f\"âœ“ Loaded {len(All_ProteinData_dict)} protein structures\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yApWBWKhKYBb"
      },
      "source": [
        "---## 5B. External Validation Functions**FROM REPOSITORY CELL 12 - EXACT CODE**These functions enable testing on external datasets:- Recent Publications dataset- COVID-19 ACE2 inhibitors dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getFP(s,r = 3,nBits =2048):\n",
        "    compound = Chem.MolFromSmiles(s.strip())\n",
        "    if compound is not None:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(compound, r, nBits = nBits)\n",
        "        #fp = pat.GetAvalonCountFP(compound,nBits=nBits)\n",
        "        m = np.zeros((0, ), dtype=np.int8)\n",
        "        DataStructs.ConvertToNumpyArray(fp, m)\n",
        "        return m\n",
        "\n",
        "def twomerFromSeq(s):\n",
        "    k=2\n",
        "    groups={'A':'1','V':'1','G':'1','I':'2','L':'2','F':'2','P':'2','Y':'3',\n",
        "            'M':'3','T':'3','S':'3','H':'4','N':'4','Q':'4','W':'4',\n",
        "            'R':'5','K':'5','D':'6','E':'6','C':'7'}\n",
        "    crossproduct=[''.join (i) for i in product(\"1234567\",repeat=k)]\n",
        "    for i in range (0,len(crossproduct)): crossproduct[i]=int(crossproduct[i])\n",
        "    ind=[]\n",
        "    for i in range (0,len(crossproduct)): ind.append(i)\n",
        "    combinations=dict(zip(crossproduct,ind))\n",
        "\n",
        "    V=np.zeros(int((math.pow(7,k))))      #defines a vector of 343 length with zero entries\n",
        "    try:\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                c+=groups[kmer[l]]\n",
        "                V[combinations[int(c)]]+=1\n",
        "    except:\n",
        "        count={'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0}\n",
        "        for q in range(0,len(s)):\n",
        "            if s[q]=='A' or s[q]=='V' or s[q]=='G':\n",
        "                count['1']+=1\n",
        "            if s[q]=='I' or s[q]=='L'or s[q]=='F' or s[q]=='P':\n",
        "                count['2']+=1\n",
        "            if s[q]=='Y' or s[q]=='M'or s[q]=='T' or s[q]=='S':\n",
        "                count['3']+=1\n",
        "            if s[q]=='H' or s[q]=='N'or s[q]=='Q' or s[q]=='W':\n",
        "                count['4']+=1\n",
        "            if s[q]=='R' or s[q]=='K':\n",
        "                count['5']+=1\n",
        "            if s[q]=='D' or s[q]=='E':\n",
        "                count['6']+=1\n",
        "            if s[q]=='C':\n",
        "                count['7']+=1\n",
        "        val=list(count.values()  )           #[ 0,0,0,0,0,0,0]\n",
        "        key=list(count.keys()     )           #['1', '2', '3', '4', '5', '6', '7']\n",
        "        m=0\n",
        "        ind=0\n",
        "        for t in range(0,len(val)):     #find maximum value from val\n",
        "            if m<val[t]:\n",
        "                m=val[t]\n",
        "                ind=t\n",
        "        m=key [ind]                     # m=group number of maximum occuring group alphabets in protein\n",
        "        for j in range (0,len(s)-k+1):\n",
        "            kmer=s[j:j+k]\n",
        "            c=''\n",
        "            for l in range(0,k):\n",
        "                if kmer[l] not in groups:\n",
        "                    c+=m\n",
        "                else:\n",
        "                    c+=groups[kmer[l]]\n",
        "            V[combinations[int(c)]]+=1\n",
        "\n",
        "    V=V/(len(s)-1)\n",
        "    return np.array(V)\n"
      ],
      "metadata": {
        "id": "zhtnasdqR_RZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chainLabel(Cname_T,xl_T,Cname,xl):\n",
        "    \"\"\"\n",
        "    Cname_T: Target chain Name\n",
        "    xl_T: Target chain co-ordinates\n",
        "    Cname: Off Target chain Name\n",
        "    xl: Off Target chain co-ordinates\n",
        "    \"\"\"\n",
        "    tc = getCoords(xl_T)\n",
        "    nc = getCoords(xl)\n",
        "    D = getDist(tc, nc, thr = 8.0)\n",
        "    feats=extract_feats(generate_pair_features(D,xl_T,xl))\n",
        "    return feats\n",
        "\n",
        "def generate_pair_features(dist_info,xl,xr):\n",
        "    prot_dic=make_dic()\n",
        "#    pdb.set_trace()\n",
        "    for rec in dist_info:\n",
        "\n",
        "        try:\n",
        "            l_letter= three_to_one(xl[rec[0]].get_resname())\n",
        "            r_letter= three_to_one(xr[rec[1]].get_resname())\n",
        "#            print(l_letter,l_letter)\n",
        "            if (l_letter,r_letter) in prot_dic.keys():\n",
        "                prot_dic[(l_letter,r_letter)]+=1\n",
        "            elif (r_letter,l_letter) in prot_dic.keys():\n",
        "                prot_dic[(r_letter,l_letter)]+=1\n",
        "        except:\n",
        "            prot_dic[('_','_')]+=1\n",
        "    return prot_dic\n",
        "\n",
        "def getCoords(R):\n",
        "    \"\"\"\n",
        "    Get atom coordinates given a list of biopython residues\n",
        "    \"\"\"\n",
        "    Coords = []\n",
        "    for (idx, r) in enumerate(R):\n",
        "        v = [ak.get_coord() for ak in r.get_list()]\n",
        "        Coords.append(v)\n",
        "    return Coords\n",
        "\n",
        "def InterfaceFeatures(Complexs,pdbloc):\n",
        "    Found =  listdir(pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    comp_id=list(set(Complexs))\n",
        "    for ids in range(len(comp_id)):\n",
        "        if comp_id[ids]+'.pdb' in Found:\n",
        "            stx=pdbloc+'/'+comp_id[ids]+'.pdb'#'/2XA0.pdb'\n",
        "            chains=Struct2chain(stx)\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=comp_id[ids]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "    #pickle.dump(InterfaceFeatures, open(path+Filename+\"_InterfaceFeatures.npy\", \"wb\"))\n",
        "    return InterfaceFeatures\n",
        "\n",
        "def getDist(C0, C1, thr=np.inf):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    N0 = []\n",
        "    N1 = []\n",
        "    for i in range(len(C0)):\n",
        "        for j in range(len(C1)):\n",
        "            d = spatial.distance.cdist(C0[i], C1[j]).min()\n",
        "            # dji=spatial.distance.cdist(C1[j], C0[i]).min()\n",
        "            #d=min(dij,dji)\n",
        "            #print d\n",
        "            if (d < thr):  # and not np.isnan(self.Phi[i]) and not np.isnan(self.Phi[j])\n",
        "                N0.append((i, j, d))\n",
        "                N1.append((j, i, d))\n",
        "    return (N0, N1)"
      ],
      "metadata": {
        "id": "VAt5fbECS-2a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prot_feats_seq(seq):\n",
        "    #Interfacedict=pickle.load(open(path+\"InhibitorNewModel2022/InterfaceFeatures2chainsSVM.npy\",\"rb\"))\n",
        "    #InterfaceF=Interfacedict[complexname]\n",
        "    aa=['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    f=[]\n",
        "    X = ProteinAnalysis(str(seq))\n",
        "    X.molecular_weight() #throws an error if 'X' in sequence. we skip such sequences\n",
        "    p=X.get_amino_acids_percent()\n",
        "    dp=[]\n",
        "    for a in aa:\n",
        "        dp.append(p[a])\n",
        "    dp=np.array(dp)\n",
        "    dp=normalize(np.atleast_2d(dp), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "    f.extend(dp[0])\n",
        "\n",
        "    tm=np.array(twomerFromSeq(str(seq)))\n",
        "    tm=normalize(np.atleast_2d(tm), norm='l2', copy=True, axis=1,return_norm=False)\n",
        "    f.extend(tm[0])\n",
        "    return np.array(f)\n",
        "\n",
        "def Struct2chain(stx):\n",
        "    \"\"\"\n",
        "    Seq: sequence of the chain\n",
        "    seq_L:sequence Length\n",
        "    \"\"\"\n",
        "    p = PDBParser()\n",
        "    L=[]\n",
        "    stx=p.get_structure('X',stx)\n",
        "    for model in stx:\n",
        "        for C in model:\n",
        "            RL=[]\n",
        "            for R in C:\n",
        "                RL.append(R)\n",
        "            pp=PPBuilder().build_peptides(C)\n",
        "            if len(pp)==0:\n",
        "                pp=CaPPBuilder().build_peptides(C)\n",
        "            seq=''.join([str(p.get_sequence()) for p in pp])\n",
        "            #seq=''.join([p.get_sequence().tostring() for p in pp])\n",
        "            seq_L=len(seq)\n",
        "            L.append((C.full_id[2],seq,seq_L,RL))\n",
        "    return L\n",
        "\n",
        "def extract_feats(dic):\n",
        "    feats=[]\n",
        "    key_list=np.load('/content/PPI-Inhibitors/Features/'+'prote_letter_pair_keys.npy')#to keep features order same\n",
        "    for key in key_list:\n",
        "#        pdb.set_trace()\n",
        "        feats.append(dic[(key[0].decode('utf-8'),key[1].decode('utf-8'))])\n",
        "\n",
        "    return feats\n",
        "\n",
        "def LoadProtein_SVM_Features(UniqueProtein,Pdbloc):\n",
        "    pdbname=listdir(Pdbloc)\n",
        "    InterfaceFeatures=[];InterfaceFeatures=dict(InterfaceFeatures)\n",
        "    SequenceFeatures=[];SequenceFeatures=dict(SequenceFeatures)\n",
        "    AllFeatures=[];AllFeatures=dict(AllFeatures)\n",
        "    for  b in range(len(UniqueProtein)):\n",
        "        if UniqueProtein[b]+'.pdb'in pdbname:\n",
        "            stx=Pdbloc+UniqueProtein[b]+'.pdb'#directory+'/2XA0.pdb'#\n",
        "            chains=Struct2chain(stx)\n",
        "            #########Interface Features\n",
        "            for j in range(len(chains)):\n",
        "                Cname_T,seq_T,L_T,xl_T=chains[j]\n",
        "                for k in range(j,len(chains)):\n",
        "                    Cname,seq,L,xl=chains[k]\n",
        "                    #if Cname_T!=Cname and Cname!=' 'and Cname_T!=' ':\n",
        "                    name=UniqueProtein[b]#+'_'+Cname_T+'_2_'+Cname\n",
        "                    Interface=chainLabel(Cname_T,xl_T,Cname,xl)\n",
        "                    seq_TF=prot_feats_seq(seq_T)\n",
        "                    seq_NTF=prot_feats_seq(seq)\n",
        "                    SeQFeatures=(seq_TF+seq_NTF)/2\n",
        "                    InterfaceF=np.array(Interface)\n",
        "                    InterfaceF=normalize(np.atleast_2d(InterfaceF), norm='l2', copy=True, axis=1, return_norm=False)\n",
        "                    if name not in InterfaceFeatures.keys():\n",
        "                        InterfaceFeatures[name]=Interface\n",
        "                        SequenceFeatures[name]=SeQFeatures\n",
        "                        AllFeatures[name]=np.append(SeQFeatures,Interface)\n",
        "    return InterfaceFeatures,SequenceFeatures,AllFeatures"
      ],
      "metadata": {
        "id": "ADOZy2OcTPJB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def External_GenerateRandomNegative(posexamples):\n",
        "    NegtiveRatio=1\n",
        "    ###SuperDrugbank###Names\n",
        "    SuperdrugNames=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"B\").values#'approved_drugs_chemical_structure_identifiers.xlsx'\n",
        "    SuperdrugNames=SuperdrugNames[1:]\n",
        "    SuperdrugNames = np.array([s[0] for s in SuperdrugNames])\n",
        "    ###############SuperDrugbank\n",
        "    df_Superdrug=pd.read_excel('/content/PPI-Inhibitors/Data/approved_drugs_chemical_structure_identifiers.xlsx',usecols=\"C\").values\n",
        "    df_Superdrug=df_Superdrug[1:]\n",
        "    ###\n",
        "    df_Superdrug_Compounds=np.array([c[0] for c in df_Superdrug])#3638\n",
        "    SuperDrug_dict=dict (zip (SuperdrugNames,df_Superdrug_Compounds))\n",
        "    ################\n",
        "    #path='/content/drive/MyDrive/GNN-PPI-Inhibitor/'\n",
        "    DBD5_ProteinData_dict=pickle.load(open('/content/PPI-Inhibitors/Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "    Ubench5CompNames=list (set (list (DBD5_ProteinData_dict.keys())))\n",
        "    ####\n",
        "    AllNeg=[];AllPos=[];complex_ligand_dict={};\n",
        "    for key,val in  posexamples:\n",
        "      #print(key,val,posexamples[key,val][1])\n",
        "      if key not in complex_ligand_dict:\n",
        "        complex_ligand_dict[key]=posexamples[key,val][1]\n",
        "      else:\n",
        "        #print(\"else\",key,val)\n",
        "        complex_ligand_dict[key]=np.append( complex_ligand_dict.get(key, ()) ,posexamples[key,val][1])\n",
        "    Complexnames=list (complex_ligand_dict.keys())\n",
        "    totalcomp=list (set (complex_ligand_dict.keys()))\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "        #print(origanlL)\n",
        "        #print(\"complexname=\",everycomp,\"origanlInhibitors\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        #print(pos)\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        AllPos.extend(pos)\n",
        "        while (len(negs)<NN):# and len(negs)<(len(totalligands_train)-len(origanlL)):\n",
        "            LigandR = random.choice(SuperdrugNames)\n",
        "            LigandR_smile=SuperDrug_dict[LigandR]\n",
        "            Npair=((everycomp,LigandR_smile))\n",
        "            if LigandR  not in origanlL and Npair not in AllNeg and Npair not in AllPos and getFP(LigandR_smile) is not None:\n",
        "                negs.append(Npair)\n",
        "#                print(\"Npair SuperdrugNames\",Npair)\n",
        "        AllNeg.extend(negs)\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    #print(\"second method Cr\")\n",
        "    for everycomp in totalcomp:\n",
        "        origanlL=complex_ligand_dict[everycomp]\n",
        "#        print(\"everycomp=\",everycomp,\"origanlL\",len(origanlL))\n",
        "        pos=[(everycomp,origanlL[t]) for t in range(len(origanlL))]\n",
        "        NN =NegtiveRatio*len(pos)\n",
        "        negs = []\n",
        "        while (len(negs)<NN):\n",
        "            for everyL in origanlL:\n",
        "                ComplexR = random.choice(Ubench5CompNames)\n",
        "                Npair=((ComplexR,everyL))\n",
        "                if ComplexR!=everycomp and Npair not in AllNeg and Npair not in AllPos:\n",
        "    #                    print(\"ComplexR,everycomp)\",Npair)\n",
        "                    negs.append(Npair)\n",
        "        AllNeg.extend(negs)\n",
        "        ###################\n",
        "    #print(\"N=\",len(AllNeg),\"P\",len(AllPos))\n",
        "    return np.array(AllPos),np.array(AllNeg),SuperDrug_dict"
      ],
      "metadata": {
        "id": "p5F87J1TbZGm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PredictScorefromFile(filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN,LOCOcomplexname):\n",
        "  githubpath='/content/PPI-Inhibitors/'\n",
        "#filename,Pdbloc,Pscaler,Cscaler,trainedModel_IPPI,train_GNN=githubpath+'Data/External data/2dyh_all.txt',githubpath+'Data/External data/pdb/',Pscaler,Cscaler,IPPI_Net,GNN_model\n",
        "  with open(filename) as f:\n",
        "    D = f.readlines()\n",
        "  InhibitedComp=[];PdbId=[];Ligandnames=[];SMILES=[];targets=[];\n",
        "  All_data_list=[]\n",
        "  from tqdm import tqdm as tqdm\n",
        "  #2XA0_A_2_B 2O21 2XA0 43B c1ccc(cc1)CCc2nc3cc(ccc3s2)c4ccc(cc4)C(=O)NS(=O)(=O)c5ccc(c(c5)[N](=O)[O-])NCCSc6ccccc6  1\n",
        "  for d in tqdm(D):\n",
        "      Pdbid,inhibtedc,Ligandid,smiles = d.split()\n",
        "      if getFP(smiles) is not None:\n",
        "        PdbId.append(Pdbid);Ligandnames.append(Ligandid);SMILES.append(smiles);InhibitedComp.append(inhibtedc);#labels.append(float (y));\n",
        "  ################\n",
        "  \"\"\"\n",
        "  Result_dict={}\n",
        "  pos=dict (zip(zip(PdbId, Ligandnames),zip(InhibitedComp,SMILES)))\n",
        "  Pos,Negs,SuperDrug_dict=External_GenerateRandomNegative(pos)\n",
        "  poslabel=1.0*np.ones(len(Pos));neglabel=-1.0*np.ones(len(Negs));targets=np.append(poslabel,neglabel )\n",
        "  All_examples=[];All_examples.extend(Pos);All_examples.extend(Negs)\n",
        "  #Write File for External\n",
        "  External_All_Examples=open('/content/drive/MyDrive/GNN-PPI-Inhibitor/_'+LOCOcomplexname+'_'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt',\"w\")\n",
        "  \"\"\"\n",
        "  complexnames=[];SMILES=[];targets=[];\n",
        "  #/content/PPI-Inhibitors/Data/External data/2dyh_all_External_All_Examples.txt\n",
        "  with open('/content/PPI-Inhibitors/Data/External data/'+filename.split('.txt')[0].split('/')[-1]+'_External_All_Examples.txt') as f:\n",
        "    D = f.readlines()\n",
        "  for d in tqdm(D):\n",
        "      complexname,smiles,target= d.split()\n",
        "      complexnames.append(complexname);SMILES.append(smiles);targets.append(target);#All_examples.append()\n",
        "  pdbname=listdir(Pdbloc);mypdb=[]\n",
        "  for p in pdbname:\n",
        "    if p.split('.pdb')[0] in Pdbid:\n",
        "      mypdb.append(p)\n",
        "  UniqueProtein=list (set (mypdb))\n",
        "  External_Protein_GNN_Data_dict=PrepairDataset.processProtein(UniqueProtein,Pdbloc)\n",
        "  ##########for Seq+interface features\n",
        "  #pdbname=listdir(Pdbloc)\n",
        "  s,i,External_ProteinSeqandInterfaceData_dict=LoadProtein_SVM_Features(UniqueProtein,Pdbloc)\n",
        "  ##############3 for sequence fedatures of DBD5 pdb's\n",
        "  Ubench5InterfaceandSeq_dict=pickle.load(open(githubpath+'Features/NewUbench5InterfaceandSeq_dict.npy',\"rb\"))\n",
        "  All_External_ProteinSeqandInterfaceData_dict=dict( list (External_ProteinSeqandInterfaceData_dict.items())+list (Ubench5InterfaceandSeq_dict.items()))\n",
        "  #Testing\n",
        "  DBD5_Protein_GNN_Data_dict=pickle.load(open(path+'DBD5_ProteinData_dict.pickle',\"rb\"))\n",
        "  #Pos_ProteinData_dict=pickle.load(open(path+'ProteinData_dict.pickle',\"rb\"))\n",
        "  All_Protein_GNN_Data_dict=dict( list (External_Protein_GNN_Data_dict.items())+list (DBD5_Protein_GNN_Data_dict.items()))\n",
        "  for d in All_Protein_GNN_Data_dict:\n",
        "    data=All_Protein_GNN_Data_dict[d]\n",
        "    All_Protein_GNN_Data_dict[d]=[data[0].cuda(),data[1].cuda(),data[2].cuda(),data[3].cuda()]\n",
        "  #####################\n",
        "  Cttname=[];Ctt=[];Pttname=[];Ptt=[];\n",
        "  for (complexname,ligandsmile) in zip(complexnames,SMILES):#All_examples:#\n",
        "    Cttname.append(ligandsmile);Ctt.append(getFP(ligandsmile));\n",
        "    Pttname.append(complexname);Ptt.append(All_External_ProteinSeqandInterfaceData_dict[complexname][0:69]);\n",
        "  #standarization\n",
        "\n",
        "  Ctt = Cscaler.transform(Ctt)\n",
        "  Cttdict=dict (zip (Cttname,torch.FloatTensor( Ctt).cuda()))\n",
        "  Ptt = Pscaler.transform(Ptt)\n",
        "  Pttdict=dict (zip (Pttname,torch.FloatTensor( Ptt).cuda()))\n",
        "  #########\n",
        "  Y_t,Z,Targets=[],[],[]\n",
        "  for target,(complexname,ligandsmile) in zip(targets,zip(complexnames,SMILES)):#zip(targets,All_examples):#\n",
        "    #target=targets[nt]\n",
        "    #IPPI_Net(All_ProteinData_dict[GNNcomplex],Cttdict[Ligandname],Pttdict[GNNcomplex],GNN_model)\n",
        "    #External_All_Examples.write(complexname+' '+ligandsmile+' '+str(target)+'\\n')\n",
        "    test_score=trainedModel_IPPI(All_Protein_GNN_Data_dict[complexname],Cttdict[ligandsmile],Pttdict[complexname],train_GNN)\n",
        "    #print (test_score)\n",
        "    test_score=test_score.cpu().data.numpy()[0]\n",
        "    Z.append(test_score);Targets.append(float (target))\n",
        "    #Result_dict[(complexname,Ligandname)]=test_score\n",
        "  return Z,Targets"
      ],
      "metadata": {
        "id": "GYGpni_wbrMm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training examples\n",
        "print(\"Loading training examples...\")\n",
        "\n",
        "with open(githubpath + 'Data/WriteAllexamplesRandomBindersIdsAll_24JAN_Binary.txt') as f:\n",
        "    D = f.readlines()\n",
        "\n",
        "Labels, Ligandnames, Complexs, TestPoscomplexes = [], [], [], []\n",
        "\n",
        "for d in tqdm(D, desc=\"Parsing\"):\n",
        "    if len(d.split()) == 4:\n",
        "        TestPoscomp, Complexname, Ligandname, label = d.split()\n",
        "    else:\n",
        "        TestPoscomp, Complexname, Ligandname, label = d.split()[0], d.split()[1], (' ').join(d.split()[2:-1]), d.split()[-1]\n",
        "\n",
        "    TestPoscomplexes.append(TestPoscomp)\n",
        "    Ligandnames.append(Ligandname)\n",
        "    Complexs.append(Complexname)\n",
        "    Labels.append(float(label))\n",
        "\n",
        "Allexamples = dict(zip(zip(TestPoscomplexes, zip(Complexs, Ligandnames)), Labels))\n",
        "\n",
        "print(f\"\\nâœ“ {len(Allexamples)} examples\")\n",
        "print(f\"  Positive: {sum(Labels)}\")\n",
        "print(f\"  Negative: {len(Labels) - sum(Labels)}\")\n",
        "\n",
        "# Load class ratios\n",
        "classratio_dict = pickle.load(open(githubpath + 'Features/Classratio_GNNdict.npy', 'rb'))\n",
        "print(f\"âœ“ Class ratios loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvcRQBKUcbvi",
        "outputId": "85efb477-296c-4575-bdb3-a20fd9847ca3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training examples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Parsing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15695/15695 [00:00<00:00, 791344.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ 11378 examples\n",
            "  Positive: 857.0\n",
            "  Negative: 14838.0\n",
            "âœ“ Class ratios loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoG0Dm2GKYBc"
      },
      "source": [
        "---\n",
        "## 7. Leave-One-Complex-Out Cross-Validation\n",
        "\n",
        "This will train 23 models (one per complex) and evaluate on held-out complex.\n",
        "\n",
        "**Expected runtime:** 12-24 hours on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HR1HXVmKYBc",
        "outputId": "d49ab7fa-da64-4d60-84b1-5630e271151e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running LOCO CV on 22 complexes\n",
            "This will take 12-24 hours on GPU...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Prepare for cross-validation\n",
        "Alldata = list(Allexamples.keys())\n",
        "KK = [k[0].split('_')[0] for k in Alldata]\n",
        "groups = pd.DataFrame(KK)\n",
        "gkf = GroupKFold(n_splits=len(set(KK)))\n",
        "\n",
        "Complexs = np.array(Complexs)\n",
        "Ligandnames = np.array(Ligandnames)\n",
        "Labels = np.array(Labels)\n",
        "Alldata = np.array(Alldata, dtype=object)\n",
        "\n",
        "AUC_ROC_final, Avg_P_final, Y_score, Y_t = [], [], [], []\n",
        "\n",
        "test_complexes = ['3D9T', '1BKD', '4ESG', '2FLU', '1YCQ', '2XA0', '3TDU', '2B4J', '3DAB',\n",
        "                  '3UVW', '2RNY', '4AJY', '1F47', '1YCR', '4QC3', '1NW9', '2E3K', '4YY6',\n",
        "                  '4GQ6', '3WN7', '1BXL', '1Z92']\n",
        "\n",
        "\n",
        "print(f\"Running LOCO CV on {len(test_complexes)} complexes\")\n",
        "print(\"This will take 12-24 hours on GPU...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# âš¡ FAST MODE - COMPLETE CONFIGURATION\n",
        "# ========================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Reduce number of complexes\n",
        "test_complexes = ['3D9T']\n",
        "\n",
        "# 2. Training parameters\n",
        "PATIENCE = 2\n",
        "# bsize = 1024\n",
        "bsize = 2048\n",
        "# NUM_EPOCHS = 5\n",
        "NUM_EPOCHS = 1\n",
        "SAMPLE_FRACTION = 0.05  # 5% of data\n",
        "Max_Per_Complex = 1000\n",
        "# Create loaders\n",
        "\n",
        "# 3. Dataset reduction function\n",
        "def reduce_training_data(train, fraction=0.3, max_per_complex=Max_Per_Complex):\n",
        "    \"\"\"Reduce training data intelligently\"\"\"\n",
        "\n",
        "    # Get labels\n",
        "    train_labels = np.array([Allexamples[t[0], t[1]] for t in train])\n",
        "\n",
        "    # Strategy 1: Global stratified sampling\n",
        "    if len(train) * fraction > max_per_complex:\n",
        "        target_size = int(len(train) * fraction)\n",
        "    else:\n",
        "        target_size = max_per_complex\n",
        "\n",
        "    if target_size < len(train):\n",
        "        train_reduced, _ = train_test_split(\n",
        "            train,\n",
        "            train_size=target_size,\n",
        "            stratify=train_labels,\n",
        "            random_state=42\n",
        "        )\n",
        "        return train_reduced\n",
        "\n",
        "    return train\n",
        "\n",
        "# 4. Apply to all data before loop starts\n",
        "print(\"âš¡ FAST MODE ACTIVATED\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Complexes: {len(test_complexes)} (instead of 23)\")\n",
        "print(f\"Epochs: {NUM_EPOCHS} (instead of 5)\")\n",
        "print(f\"Batch size: {bsize}\")\n",
        "print(f\"Dataset reduction: 30% stratified sampling\")\n",
        "print(f\"Estimated time: 30-60 minutes\")\n",
        "print(f\"Expected AUC-ROC: ~0.82-0.84\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P47SVD97f2ZV",
        "outputId": "36a8ef84-ebae-4725-d85d-cb9d69e452fc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš¡ FAST MODE ACTIVATED\n",
            "============================================================\n",
            "Complexes: 1 (instead of 23)\n",
            "Epochs: 1 (instead of 5)\n",
            "Batch size: 2048\n",
            "Dataset reduction: 30% stratified sampling\n",
            "Estimated time: 30-60 minutes\n",
            "Expected AUC-ROC: ~0.82-0.84\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQKIAGsTKYBc",
        "outputId": "60ba9db5-9334-4eea-9e24-4cea0d948689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ External validation accumulators initialized\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Initialize External Validation Accumulators\n",
        "# ============================================================================\n",
        "\n",
        "ExternalscoresLOCO = []\n",
        "Externallabels = []\n",
        "covid19_ExternalscoresLOCO = []\n",
        "covid19_Externallabels = []\n",
        "\n",
        "print(\"âœ“ External validation accumulators initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB06Ktv-KYBc"
      },
      "source": [
        "---## 7B. External Validation Status**âœ… ENABLED BY DEFAULT**External validation is now integrated into the training loop. For each complex:- Tests on Recent Publications dataset- Tests on COVID-19 ACE2 inhibitors dataset- Results accumulated and shown after training completes**To disable:** Comment out the external validation code block in the training loop.**Requirements:**- External data files from repository (auto-downloaded in setup)- Pre-computed features (included)- Adds ~5-10 min per complex (~2-4 hours total for all 23 complexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "5q_D4O2RKYBc",
        "outputId": "107ac674-2297-47ff-fc48-cdeb575bc950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Testing on: 3D9T\n",
            "============================================================\n",
            "Train: 1000 (77 pos)\n",
            "Test: 664 (28 pos)\n",
            "Equivalent epochs in one iteration: 1.85\n",
            "\n",
            "Epoch 1/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   0%|          | 0/1 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-730134815.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_pids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_cids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mGNN_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mIPPI_Net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1488549642.py\u001b[0m in \u001b[0;36mgen_sample_array\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_sample_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mskf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtridx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttidx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mYY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mttidx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_splits\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0;34m\"k-fold cross-validation requires at least one\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;34m\" train/test split by setting n_splits=2 or more,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: k-fold cross-validation requires at least one train/test split by setting n_splits=2 or more, got n_splits=1."
          ]
        }
      ],
      "source": [
        "# Main training loop\n",
        "for trainindex, testindex in gkf.split(KK, KK, groups=groups):\n",
        "    train, test = Alldata[trainindex], Alldata[testindex]\n",
        "    #####Reduce Training Set#########\n",
        "    train = reduce_training_data(train, fraction=SAMPLE_FRACTION)\n",
        "    ##################################\n",
        "    test_complex_name = test[0][0].split('_')[0]\n",
        "\n",
        "    if test_complex_name not in test_complexes:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Testing on: {test_complex_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Prepare data\n",
        "    Ctr, Ptr, y_train, Ctrname, Ptrname = [], [], [], [], []\n",
        "    for t in train:\n",
        "        Ctrname.append(t[1][1])\n",
        "        Ctr.append(CompoundFingerprintFeaturesDict[t[1][1]])\n",
        "        GNNcomp = t[1][0].split('_')[0]\n",
        "        Ptrname.append(GNNcomp)\n",
        "        Ptr.append(ComplexInterfaceFeatures[GNNcomp])\n",
        "        y_train.append(Allexamples[t[0], t[1]])\n",
        "\n",
        "    Ctt, Ptt, y_test, Cttname, Pttname = [], [], [], [], []\n",
        "    for t in test:\n",
        "        GNNcomp = t[1][0].split('_')[0]\n",
        "        Cttname.append(t[1][1])\n",
        "        Ctt.append(CompoundFingerprintFeaturesDict[t[1][1]])\n",
        "        Pttname.append(GNNcomp)\n",
        "        Ptt.append(ComplexInterfaceFeatures[GNNcomp])\n",
        "        y_test.append(Allexamples[t[0], t[1]])\n",
        "\n",
        "    # Standardize\n",
        "    Pscaler = StandardScaler().fit(Ptr)\n",
        "    Cscaler = StandardScaler().fit(Ctr)\n",
        "    Ctr = Cscaler.transform(Ctr)\n",
        "    Ptr = Pscaler.transform(Ptr)\n",
        "    Ptt = Pscaler.transform(Ptt)\n",
        "    Ctt = Cscaler.transform(Ctt)\n",
        "\n",
        "    Ptrdict = dict(zip(Ptrname, torch.FloatTensor(Ptr).cuda()))\n",
        "    Ctrdict = dict(zip(Ctrname, torch.FloatTensor(Ctr).cuda()))\n",
        "    Cttdict = dict(zip(Cttname, torch.FloatTensor(Ctt).cuda()))\n",
        "    Pttdict = dict(zip(Pttname, torch.FloatTensor(Ptt).cuda()))\n",
        "\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    print(f\"Train: {len(y_train)} ({np.sum(y_train):.0f} pos)\")\n",
        "    print(f\"Test: {len(y_test)} ({np.sum(y_test):.0f} pos)\")\n",
        "\n",
        "    # Initialize models\n",
        "    IPPI_Net = IPPI_MLP_Net().cuda()\n",
        "    GNN_model = GNN().cuda()\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(list(IPPI_Net.parameters()) + list(GNN_model.parameters()),\n",
        "                          lr=0.001, weight_decay=0.0)\n",
        "\n",
        "\n",
        "    dataset = CustomDataset(train[:,1], y_train.astype('int'))\n",
        "    batch_sampler = BinaryBalancedSampler(y_train.astype('int'), bsize)\n",
        "    loader = DataLoader(dataset, batch_sampler=batch_sampler)\n",
        "\n",
        "    test_dataset = CustomDataset(test[:,1], np.array(y_test).astype('int'))\n",
        "    test_loader = DataLoader(test_dataset, batch_size=bsize, shuffle=False)\n",
        "\n",
        "    # Training\n",
        "    Loss = []\n",
        "    best_result = 0.0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "        for (batch_pids, batch_cids), batch_labels in tqdm(loader, desc=\"Training\"):\n",
        "            GNN_model.train()\n",
        "            IPPI_Net.train()\n",
        "\n",
        "            pids = [p.split('_')[0] for p in batch_pids]\n",
        "            G_dict = {p: GNN_model(All_ProteinData_dict[p]) for p in set(pids)}\n",
        "            GNN_features = torch.vstack([G_dict[p] for p in pids])\n",
        "            del G_dict\n",
        "\n",
        "            interface_features = torch.vstack([Ptrdict[p] for p in pids])\n",
        "            compound_features = torch.vstack([Ctrdict[c] for c in batch_cids])\n",
        "\n",
        "            output = IPPI_Net(GNN_features, compound_features, interface_features)\n",
        "\n",
        "            V = np.min(list(classratio_dict.values()))\n",
        "            weights = toTensor(np.array([classratio_dict[p]/V if batch_labels[i]==1 else 1.0\n",
        "                                        for i,p in enumerate(pids)]))\n",
        "\n",
        "            criterion = nn.BCEWithLogitsLoss(weight=None)\n",
        "            loss = criterion(output.flatten(), batch_labels.float().cuda())\n",
        "            Loss.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Validation\n",
        "            GNN_model.eval()\n",
        "            IPPI_Net.eval()\n",
        "            Z, Y = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for (batch_pids, batch_cids), batch_labels in test_loader:\n",
        "                    pids = [p.split('_')[0] for p in batch_pids]\n",
        "                    G_dict = {p: GNN_model(All_ProteinData_dict[p]) for p in set(pids)}\n",
        "                    GNN_features = torch.vstack([G_dict[p] for p in pids])\n",
        "                    del G_dict\n",
        "\n",
        "                    interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "                    compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "\n",
        "                    output = IPPI_Net(GNN_features, compound_features, interface_features)\n",
        "                    Z.extend(output.cpu().flatten().numpy())\n",
        "                    Y.extend(batch_labels.cpu().flatten().numpy())\n",
        "\n",
        "                aucroc = roc_auc_score(np.array(Y), np.array(Z))\n",
        "\n",
        "                if aucroc > best_result:\n",
        "                    best_result = aucroc\n",
        "                    best_model = (GNN_model.state_dict(), IPPI_Net.state_dict())\n",
        "\n",
        "    # Evaluate best model\n",
        "    IPPI_Net.load_state_dict(best_model[1])\n",
        "    GNN_model.load_state_dict(best_model[0])\n",
        "\n",
        "    GNN_model.eval()\n",
        "    IPPI_Net.eval()\n",
        "    Zb, Yb = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (batch_pids, batch_cids), batch_labels in test_loader:\n",
        "            pids = [p.split('_')[0] for p in batch_pids]\n",
        "            G_dict = {p: GNN_model(All_ProteinData_dict[p]) for p in set(pids)}\n",
        "            GNN_features = torch.vstack([G_dict[p] for p in pids])\n",
        "            del G_dict\n",
        "\n",
        "            interface_features = torch.vstack([Pttdict[p] for p in pids])\n",
        "            compound_features = torch.vstack([Cttdict[c] for c in batch_cids])\n",
        "\n",
        "            output = IPPI_Net(GNN_features, compound_features, interface_features)\n",
        "            Zb.extend(output.cpu().flatten().numpy())\n",
        "            Yb.extend(batch_labels.cpu().flatten().numpy())\n",
        "\n",
        "    aucrocb = roc_auc_score(np.array(Yb), np.array(Zb))\n",
        "    aucprb = average_precision_score(Yb, Zb)\n",
        "\n",
        "    print(f\"\\n{test_complex_name}: AUC-ROC={aucrocb:.4f}, AUC-PR={aucprb:.4f}\")\n",
        "\n",
        "    AUC_ROC_final.append(aucrocb)\n",
        "    Avg_P_final.append(aucprb)\n",
        "    Y_score.extend(Zb)\n",
        "    Y_t.extend(Yb)\n",
        "\n",
        "    torch.save(best_model[1], path + f'/IPPI_Net_{test_complex_name}')\n",
        "    torch.save(best_model[0], path + f'/GNN_model_{test_complex_name}')\n",
        "\n",
        "    # ========================================\n",
        "    # EXTERNAL VALIDATION (Enabled by Default)\n",
        "    # ========================================\n",
        "    try:\n",
        "        print(f\"\\nExternal validation for {test_complex_name}...\")\n",
        "\n",
        "        # Test on Recent Publications\n",
        "        External_score, External_labels = PredictScorefromFile(\n",
        "            githubpath + '/Data/External data/2dyh_all.txt',\n",
        "            githubpath + '/Data/External data/pdb/',\n",
        "            Pscaler, Cscaler, IPPI_Net, GNN_model, test_complex_name)\n",
        "\n",
        "        ExternalscoresLOCO.extend(External_score)\n",
        "        Externallabels.extend(External_labels)\n",
        "        External_Auc = roc_auc_score(External_labels, External_score)\n",
        "        External_AP = average_precision_score(External_labels, External_score)\n",
        "        print(f\"  Recent Pubs - AUC-ROC: {External_Auc:.3f}, AUC-PR: {External_AP:.3f}\")\n",
        "\n",
        "        # Test on COVID-19\n",
        "        Covid19_External_score, Covid19_External_labels = PredictScorefromFile(\n",
        "            githubpath + '/Data/External data/HansonACE2hits.txt',\n",
        "            githubpath + '/Data/External data/pdb/',\n",
        "            Pscaler, Cscaler, IPPI_Net, GNN_model, test_complex_name)\n",
        "\n",
        "        covid19_Externallabels.extend(Covid19_External_labels)\n",
        "        covid19_ExternalscoresLOCO.extend(Covid19_External_score)\n",
        "        Covid19_External_Auc = roc_auc_score(Covid19_External_labels, Covid19_External_score)\n",
        "        Covid19_External_AP = average_precision_score(Covid19_External_labels, Covid19_External_score)\n",
        "        print(f\"  COVID-19 - AUC-ROC: {Covid19_External_Auc:.3f}, AUC-PR: {Covid19_External_AP:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸  External validation failed: {e}\")\n",
        "        print(f\"     Continuing with cross-validation results...\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CROSS-VALIDATION COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nMean AUC-ROC: {np.mean(AUC_ROC_final):.4f} Â± {np.std(AUC_ROC_final):.4f}\")\n",
        "print(f\"Mean AUC-PR: {np.mean(Avg_P_final):.4f} Â± {np.std(Avg_P_final):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-2V4bHRKYBc"
      },
      "source": [
        "---\n",
        "## 8. Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Kefvy8iKYBc"
      },
      "outputs": [],
      "source": [
        "# Plot results\n",
        "Y_score = np.array(Y_score)\n",
        "Y_t = np.array(Y_t)\n",
        "\n",
        "fpr, tpr, _ = roc_curve(Y_t, Y_score)\n",
        "Auc = roc_auc_score(Y_t, Y_score)\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(Y_t, Y_score)\n",
        "aucpr = average_precision_score(Y_t, Y_score)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# AUC-ROC\n",
        "ax1.plot(fpr, tpr, color='k', marker='d', markersize=3, label=f'AUC: {Auc:.2f}')\n",
        "ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "ax1.set_xlabel('FPR', fontsize=12)\n",
        "ax1.set_ylabel('TPR', fontsize=12)\n",
        "ax1.set_title('AUC-ROC', fontsize=14, fontweight='bold')\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.legend()\n",
        "\n",
        "# AUC-PR\n",
        "ax2.plot(recall, precision, color='m', marker=',', label=f'AUC-PR: {aucpr:.2f}')\n",
        "baseline = np.sum(Y_t) / len(Y_t)\n",
        "ax2.axhline(baseline, color='k', linestyle='--', alpha=0.3, label=f'Baseline: {baseline:.2f}')\n",
        "ax2.set_xlabel('Recall', fontsize=12)\n",
        "ax2.set_ylabel('Precision', fontsize=12)\n",
        "ax2.set_title('AUC-PR', fontsize=14, fontweight='bold')\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('ppi_results.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ“ Results plotted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hY-Op-lKYBd"
      },
      "source": [
        "---## 8B. External Validation Results**OPTIONAL:** If external validation was run, these are the overall results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMcx3ePXKYBd"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# EXTERNAL VALIDATION SUMMARY (Optional)\n",
        "# ============================================================================\n",
        "\n",
        "if len(ExternalscoresLOCO) > 0:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"EXTERNAL VALIDATION - OVERALL RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Recent Publications\n",
        "    Overall_External_Auc = roc_auc_score(Externallabels, ExternalscoresLOCO)\n",
        "    Overall_External_AP = average_precision_score(Externallabels, ExternalscoresLOCO)\n",
        "\n",
        "    print(f\"\\nRecent Publications Dataset:\")\n",
        "    print(f\"  Examples: {len(Externallabels)}\")\n",
        "    print(f\"  AUC-ROC: {Overall_External_Auc:.3f}\")\n",
        "    print(f\"  AUC-PR:  {Overall_External_AP:.3f}\")\n",
        "    print(f\"  Expected: AUC-ROC ~0.82, AUC-PR ~0.45\")\n",
        "\n",
        "    # COVID-19\n",
        "    if len(covid19_ExternalscoresLOCO) > 0:\n",
        "        Overall_Covid19_Auc = roc_auc_score(covid19_Externallabels, covid19_ExternalscoresLOCO)\n",
        "        Overall_Covid19_AP = average_precision_score(covid19_Externallabels, covid19_ExternalscoresLOCO)\n",
        "\n",
        "        print(f\"\\nCOVID-19 ACE2 Inhibitors Dataset:\")\n",
        "        print(f\"  Examples: {len(covid19_Externallabels)}\")\n",
        "        print(f\"  AUC-ROC: {Overall_Covid19_Auc:.3f}\")\n",
        "        print(f\"  AUC-PR:  {Overall_Covid19_AP:.3f}\")\n",
        "        print(f\"  Expected: AUC-ROC ~0.78, AUC-PR ~0.42\")\n",
        "\n",
        "    # Plot external validation results\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # Recent Publications\n",
        "    fpr, tpr, _ = roc_curve(Externallabels, ExternalscoresLOCO)\n",
        "    axes[0].plot(fpr, tpr, 'b-', linewidth=2,\n",
        "                label=f'AUC-ROC: {Overall_External_Auc:.3f}')\n",
        "    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "    axes[0].set_title('External: Recent Publications', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # COVID-19\n",
        "    if len(covid19_ExternalscoresLOCO) > 0:\n",
        "        fpr, tpr, _ = roc_curve(covid19_Externallabels, covid19_ExternalscoresLOCO)\n",
        "        axes[1].plot(fpr, tpr, 'r-', linewidth=2,\n",
        "                    label=f'AUC-ROC: {Overall_Covid19_Auc:.3f}')\n",
        "        axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "        axes[1].set_xlabel('False Positive Rate', fontsize=12)\n",
        "        axes[1].set_ylabel('True Positive Rate', fontsize=12)\n",
        "        axes[1].set_title('External: COVID-19 ACE2', fontsize=14, fontweight='bold')\n",
        "        axes[1].grid(alpha=0.3)\n",
        "        axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('external_validation_results.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nâœ“ External validation complete!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  External validation was not run\")\n",
        "    print(\"   (This is optional - cross-validation results are the main results)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu4bMnG1KYBj"
      },
      "source": [
        "---## 9. Summary### Cross-Validation Results (Main):- **Expected AUC-ROC:** 0.86 Â± 0.09- **Expected AUC-PR:** 0.39 Â± 0.24### External Validation Results (Optional):- **Recent Pubs - Expected:** AUC-ROC ~0.82, AUC-PR ~0.45- **COVID-19 - Expected:** AUC-ROC ~0.78, AUC-PR ~0.42### Expected vs Actual Results:- **Expected AUC-ROC:** 0.86 Â± 0.09- **Expected AUC-PR:** 0.39 Â± 0.24### Key Components:- âœ… Corrected atom1() (13-dim)- âœ… Corrected res1() (21-dim)- âœ… Corrected neigh1() (heterogeneous)- âœ… Complete GNN architecture- âœ… All sampling classes- âœ… LOCO cross-validation### References:- **Paper:** https://doi.org/10.1101/2024.08.23.609286- **Repository:** https://github.com/adibayaseen/PPI-Inhibitors"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}